{
  "hash": "8f010ac975437a0482b897d55c7f482b",
  "result": {
    "engine": "knitr",
    "markdown": "# GLM-II: Sums of Squares {#sec-glm2}\n\n\n\n\n\n\n\n\n\n\n\n\n\nLast week we discussed how linear regression represents the relationship between a predictor variable (X) and an outcome variable (Y) as a diagonal line. This line will have some prediction error for each individual data point. The regression line, by definition, is the line with the smallest possible overall prediction error (across all participants). Today, we explore this concept of \"smallest possible overall prediction error\" in more detail.\n\nThe sum of prediction errors across all participants is always zero because the regression line passes through the \"middle\" of the data. So there's always an equal amount of negative prediction errors and positive ones, which cancel each other out. To calculate the \"total prediction error\", we must square the prediction errors, which eliminates the negative values and ensures that we can add them up to a positive number. We call the sum of squared prediction errors the the \"sum of squared errors\" (SSE). When we estimate a regression model in statistical software, we ask it to find the regression line that minimizes the SSE and give us the line with the smallest prediction errors. For bivariate linear regression, we can calculate this line using matrix algebra (outside the scope of this course); we call this the \"ordinary least squares\" method.\n\nNow that we know the total amount of prediction error (SSE), we also have a basic measure of goodness of fit for the regression line. However, SSE is not readily interpretable because it lacks a meaningful scale. To assess the goodness of fit relative to a baseline, we compare the SSE of the regression line to the sum of squares we would obtain if we did not use the predictor variable - that is, if we just predicted the mean value for each individual. A model with only the mean and no predictor variables is called the null model. The sum of squared distances between the mean and individual observations is referred to as the Total Sum of Squares (TSS), which represents the average squared distance of individual observations from the mean of Y.\n\nTo understand how much of the TSS is explained by the regression line, we calculate the Regression Sum of Squares (RSS). This is the difference between the TSS and the SSE: the reduction in TSS achieved by using the regression line to predict observations instead of just the mean. It indicates how well the regression line explains the variance in the dependent variable.\n\nWe can standardize this RSS by dividing it by the SSE, which gives us the \"explained variance\" $R^2$, which ranges from 0 to 1. A higher R^2 value indicates that a larger portion of the total variance in the dependent variable is accounted for by the predictor variable. Explained variance is the proportion of the total sum of squares (TSS) that is explained by the regression line (RSS).\n\nUnderstanding these sums of squares gives us a good foundation for understanding another statistic: the correlation coefficient $r$. The correlation coefficient describes the strength and direction of the linear relationship between two variables. It differs from regression because regression describes one of these variables as an outcome of the other: an asymmetrical relationship. Correlation instead just describes how strongly these two variables are associated without labeling one as the predictor and the other as the outcome: a symmetrical relationship. The correlation coefficient is a standardized measure of the strength of this association that ranges from -1 (perfectly negatively associated) to 1 (perfectly positively associated). A correlation coefficient of 0 means that there is no association between X and Y. Correlation and regression are very closely related, as the squared correlation coefficient ($r$, squared) is the same as the measure of explained variance from simple linear regression, $R^2$, and is also the same as the standardized regression coefficient.\n\n# Lecture\n\n\n\n\n\n\n{{< video https://www.youtube.com/embed/okrRJXb0YT4 >}}\n\n\n\n\n\n\n\n\n\n\n\n# Formative Test\n\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\n\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nWhat does SSE stand for in linear regression? ^[Sum of Squared Errors]\n\n* (A) Sum of Squares for the Expectation  \n* (B) Sum of Squared Errors  \n* (C) Sum of Squared Explained Variance  \n* (D) Sum of Standard Errors  \n\n\n\n**Question 2**\n\nWhich of the following describes the concept of Ordinary Least Squares? ^[Minimizing the sum of squared errors]\n\n* (A) Minimizing the total sum of squares  \n* (B) Minimizing the sum of squared errors  \n* (C) Minimizing the correlation  \n* (D) Minimizing the regression sum of squares  \n\n\n\n**Question 3**\n\nWhat does the Regression Sum of Squares (RSS) represent? ^[The reduction in sum of squares by using the regression line as a predictor, rather than the mean]\n\n* (A) The sum of squared prediction errors  \n* (B) The explained variance by the regression line  \n* (C) The reduction in sum of squares by using the regression line as a predictor, rather than the mean  \n* (D) The total sum of squares minus the predicted values  \n\n\n\n**Question 4**\n\nWhat is the purpose of the Total Sum of Squares (TSS) in linear regression? ^[It measures the total variability of the dependent variable.]\n\n* (A) It measures the total variability of the dependent variable.  \n* (B) It measures the residual error in the model.  \n* (C) It measures the standard deviation of the errors.  \n* (D) It measures the explained variance of the predictor.  \n\n\n\n**Question 5**\n\nWhat does the term 'explained variance' refer to in linear regression? ^[The portion of the total variance explained by the regression line.]\n\n* (A) The portion of the regression sum of squares explained by the predictor.  \n* (B) The portion of the standard deviation explained by the predictor.  \n* (C) The portion of the total variance explained by the null model.  \n* (D) The portion of the total variance explained by the regression line.  \n\n\n\n**Question 6**\n\nHow is correlation different from regression? ^[Correlation measures the strength of linear association, while regression predicts one variable from another.]\n\n* (A) Correlation measures the strength of linear association, while regression predicts one variable from another.  \n* (B) Correlation focuses on categorical variables, while regression focuses on continuous variables.  \n* (C) Correlation measures the total variance, while regression measures the explained variance.  \n* (D) Correlation predicts one variable from another, while regression measures the strength of linear association.  \n\n\n\n**Question 7**\n\nWhat is the range of the correlation coefficient (r) between two variables? ^[[-1, 1]]\n\n* (A) [-∞, ∞]  \n* (B) [-1, 1]  \n* (C) [-1, 0]  \n* (D) [0, 1]  \n\n\n\n**Question 8**\n\nWhich statistic is equivalent to the standardized regression coefficient in bivariate regression? ^[Correlation coefficient (r)]\n\n* (A) Sum of Squared Errors (SSE)  \n* (B) Standard deviation of the predictor  \n* (C) Correlation coefficient (r)  \n* (D) Coefficient of determination (R²)  \n\n\n\n**Question 9**\n\nWhat is the relationship between the Total Sum of Squares (TSS) and the Regression Sum of Squares (RSS)? ^[TSS - SSE = RSS]\n\n* (A) SSR- SSE = SST  \n* (B) TSS + SSE = SSR  \n* (C) TSS - SSE = RSS  \n* (D) TSS + RSS = SSE  \n\n\n\n**Question 10**\n\nHow is the proportion of explained variance (R²) related to the correlation coefficient (r)? ^[R² = r²]\n\n* (A) R² = 1 - r²  \n* (B) R² = 1 / r  \n* (C) R² = r²  \n* (D) R² = 2 * r  \n\n\n\n:::\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Show explanations]\n\n```\n**Question 1**\n\n SSE stands for Sum of Squared Errors, which represents the sum of the squared differences between actual and predicted values.\n\n**Question 2**\n\n Ordinary Least Squares aims to minimize the total prediction error, which is achieved by finding the regression line.\n\n**Question 3**\n\n RSS is the reduction in sum of squares that occurs when using the regression line to predict observations instead of just the mean.\n\n**Question 4**\n\n TSS measures the total variability of the dependent variable around its mean.\n\n**Question 5**\n\n Explained variance refers to the proportion of the total variance in the dependent variable that is explained by the predictor.\n\n**Question 6**\n\n Correlation quantifies the strength and direction of the linear relationship between two variables, while regression aims to predict one variable from another using the regression line.\n\n**Question 7**\n\n The correlation coefficient (r) ranges from -1 to 1, where -1 represents a perfect negative association, 0 represents no association, and 1 represents a perfect positive association.\n\n**Question 8**\n\n In bivariate linear regression, the standardized regression coefficient is equivalent to the correlation coefficient (r) between the predictor and the outcome.\n\n**Question 9**\n\n The relationship between TSS and RSS is given by TSS - RSS = SSE, indicating that the difference between TSS and RSS accounts for the error sum of squares.\n\n**Question 10**\n\n The proportion of explained variance (R²) is equal to the square of the correlation coefficient (r), meaning R² = r².\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n:::\n\n\n\n\n\n\n# In SPSS\n\n## Correlation Analysis\n\n\n\n\n\n\n{{< video https://www.youtube.com/watch?v=VOI5IlHfZVE >}}\n\n\n\n\n\n\n\n\n\n\n# Tutorial\n\n## Bivariate Regression\n\nSocial science students were asked about their opinion towards Tilburg’s nightlife, number of Facebook friends, and some other characteristics. The data are in the [`SocScSurvey.sav`](data/SocScSurvey.sav) file.\n\n\nDownload the file to your computer and open it in SPSS. \n\nSuppose researchers are interested in the relationship between personality and social media use. In particular, they want to know if extraversion explains the number of Facebook friends. \n\nWhat is the independent variable here? ^[Extraversion]\n\n* (A) Extraversion  \n* (B) Facebook friends  \n\n\n\nRemember that the independent variable is the variable that predicts the other variable (which we call the dependent variable). The dependent variable is influenced by the independent variable (its value depends on the independent variable).\n \nRun a regression analysis in which you regress Facebook friends on extraversion (via analyze > regression > linear).\n\nKeep in mind that we \"regress the dependent variable Y on the independent variables (X)\".\n\nConsult the output.\n\n\nWrite down the estimated unstandardized regression equation.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n\n$\\text{Friends}_i = -62.377 + 26.788*\\text{Extraversion}_i + e_i$\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nWhich of the following statements is true?\n\n^[If Extraversion increases with one unit, the number of facebook friends increases with 26.788 units]\n\n* (A) If Facebook friends increases with one unit, extraversion increases with 26.788 units  \n* (B) If Extraversion increases with one unit, the number of facebook friends increases with 26.788 units  \n* (C) If Extraversion increases with 26.788 units, the number of facebook friends increases with 1 unit  \n\n \n\n\nRemember that the general form of interpretation of the unstandardized effect is: \"If X increases with 1 unit, Y increases/decreases with 'unstandardized regression coefficient' units\".\n \nWhat is the value of the standardized regression coefficient? _____^[0\\.438]\n\nYou can find the standardized regression coefficients in the column called 'Standardized Coefficients Beta'.\n \nWhich of the following statements about the standardized regression coefficients is correct?\n\n^[If extraversion increases with one SD, the number of facebook friends increases with 0.438 SDs]\n\n* (A) If extraversion increases with one SD, the number of facebook friends increases with 0.438 units  \n* (B) If extraversion increases with one SD, the number of facebook friends increases with 0.438 SDs  \n* (C) If extraversion increases with one unit, the number of facebook friends increases with 0.438 SDs  \n\n\n\nRemember that standardized regression coefficients are interpreted in a similar way as unstandardized regression coefficients are, with the one difference being they are interpreted in terms of standard deviations.\n \n\nConsider the first person in the data file. The person had an extraversion score of 9.\n\nWhat is the predicted number of Facebook friends for this person? ___^[179]\n\nConsider the first person again.\n\nGiven the predicted number of Facebook friends for this person, what is the prediction error (rounded to the nearest integer)? ____^[-159]\n\nPrediction error = yobserved - ypredicted\n\n\nConsider two people, one with an extraversion score of 10 and the other with an extraversion score of 15.\n\nWhat is the difference in the predicted number of Facebook friends between the two persons? (report the absolute value) ___^[134]\n\n\nConsult the output of the regression analysis.\n\n\nWhat percentage of the total variance in number of Facebook friends can be explained by extraversion? ____^[19\\.2]\n\nConsult the ANOVA table.\n\nThe table shows the results of an F-test.\n\nWhat is the default null hypothesis and alternative hypothesis for the reported test?\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n$H_0: \\rho^2 = 0$, $H_A: \\rho^2 > 0$\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nSuppose three researchers test the significance of the R-square.\n\nResearcher I tests at the 10% level, researcher II tests at the 5% level, and researcher III at the 1% level.\n\n\nWhich researcher will reject the null hypothesis? ^[All three researchers]\n\n* (A) Only researcher III  \n* (B) Only researcher II  \n* (C) Only researcher I  \n* (D) All three researchers  \n\n \n\n\nWhen reporting the F-test for the model, you would report $R^2$, the F-test statistic, its degrees of freedom, and the p-value.\n\nThe F-test has two distinct degrees of freedom. The first refers to the degrees of freedom for the regression equation, and the second to the degrees of freedom for the residuals. The degrees of freedom are given in brackets. For example, if regression has 2 degrees of freedom and the residuals 100, we write the F-value as F(2,100) = ....\n\nWhich of the following F value and corresponding degrees of freedom should be reported? ^[F(1,132) = 31.283]\n\n* (A) F(1,132) = 0.000  \n* (B) F(1,133) = 31.283  \n* (C) F(1,132) = 31.283  \n\n \n\n<!-- Keep the output for the next assignment. -->\n\n<!-- In the next assignment we will extend our analysis by including multiple predictors to predict the number of Facebook friends (i.e., we will carry out multiple regression analysis).     -->\n\n## Correlation\n\nCorrelations and regression analyses can both be used to study the relationship between variables, but there is an important difference.\n\nDiscuss with your group mates what the similarities and differences between the two methods are.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\nA correlation is a symmetric measure of association, meaning we are agnostic about which is the predictor and which is the outcome (or neither are predictor/outcome). The correlation between X and Y is the same as the one between Y and X.\n\nIn regression analysis, we do define an independent and dependent variable. The goal is to predict the outcome using the predictor. Most of the time, this implies an assumption of causality - but not necessarily. \n\nFor example, we can use regression to predict sales based on customer characteristics without assuming that those characteristics CAUSE sales. But if we want to cause an increase in sales, and we look at the regression coefficients to decide where to intervene - then it suddenly matters a lot whether the predictors are causes of sales or not.\n\nYou see this a lot with online marketing when you are receiving a lot of adds for a product that you recently bought. Their regression model knows that looking at the product page is a great predictor of intention to buy it - but they don't know that the reason you were looking at that page is because you were already buying it.\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nNow, let's have a look at the correlation between these two variables.\n\nAnalyze > correlate > bivariate.\n\nChoose as variables: Facebook Friends and Extraversion, and click OK.\n\nWhat is the correlation between Extraversion and number of Facebook friends? _____^[0\\.438]\n\nSuppose three researchers test the significance of the correlation between Extraversion and Facebook friends. Researcher I tests at the 10% level, researcher II tests at the 5% level, and researcher III at the 1% level.\n\n\nWhich researcher will reject the null hypothesis? ^[All three researchers]\n\n* (A) All three researchers  \n* (B) Only researcher III  \n* (C) Only researcher II  \n* (D) Only researcher I  \n\n\n\nWhich of the following interpretations is true? \n\n^[It would be very unlikely to observe a sample correlation of .44 by chance if the population correlation would be zero.]\n\n* (A) It would be very unlikely to observe a sample correlation of .44 by chance if the population correlation would be zero.  \n* (B) We have convincing evidence that Facebook friends and extraversion are associated in the population.  \n* (C) We don&apos;t have convincing evidence that Facebook friends and extraversion are associated in the population.  \n\n\n\nCompare the correlation coefficient to the standardized regression coefficient from the bivariate regression you conducted previously.\n\nThen, compare it to the value labeled \"R\" in the \"Model Summary\" table from the regression.\n\nSquare the correlation, and compare it to the value labeled \"R Square\".\n\nWhat do you observe?\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\nIf you did everything correctly, you should observe that the bivariate correlation is identical to the standardized regression coefficient. This is only the case with *bivariate* regression. \n\nFurthermore, the bivariate correlation should be identical to the R reported in the Model Summary table, because they are both just the correlation coefficient. R squared is the squared correlation coefficient, and we interpret it as the \"proportion of variance in the outcome explained by the predictor\". Only in bivariate regression is this identical to the squared correlation coefficient. \n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\n\n\n## R squared\n\nThe R squared expresses how well the predictors explain variance in the outcome of a regression. In the next few steps we will look in more detail at this concept.\n\nConsider the results of the regression model again.\n\nWrite down the (unstandardized) regression equation based on your previous results, and use the raw data in the Data View to answer the following question.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n$\\text{pressure}_i = 37.863 -.320 * \\text{variety}_i + e_i$\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nWhat is the predicted value (Y') for emotional pressure at work for the first person in the data file (i.e., the person with respondent number 1)? ______^[29\\.329]\n\nWhat is the prediction error (a.k.a. the residual) for the first person? _____^[8\\.771]\n\nRemember Residual = Yobserved - Ypredicted\n\nWe've just computed the predicted value and error by hand. It would be very tedious if we would have to do that for all respondents. Fortunately, SPSS offers the option to compute predicted values and errors for all cases for us!\n\nNavigate to Analyze > Regression > Linear\n\nClick on the ‘Save’ button. SPSS opens a new window.\n\nAsk for the Unstandardized predicted values and the unstandardized residuals.\nPaste and run the syntax.\n \nLet's inspect the Data View in SPSS again and verify that SPSS added two columns in the data file. One column is labeled PRE_1 and the other RES_1. These columns show the predicted values and residuals for each person, respectively.\n\nYou may verify this for the first person (i.e., the values should be the same as you computed in the previous steps).\n\nNow we will look at the variance of the observed values of Pleasure at work, the variance of the predicted values of pleasure at work, and variance of the residuals.\n\nCompute the variances of Emotional pressure, as well as for the predicted values of Emotional pressure, and for the residuals.\n\nNavigate to Analyze > Descriptive statistics > Descriptives\nSelect scemoti, PRE_1, and RES_1.\nClick on ‘options’ and ask for the Variance.\nPaste and run the syntax.\n \nHow large is the variance of the observed scores for Emotional pressure? _______^[145\\.168]\n\nHow large is the variance of predicted values of Work pleasure? ______^[33\\.361]\n\n\nHow large is the variance of the residuals? _______^[111\\.807]\n\n\nIn the previous questions, we looked at three variance components.\n\n\nDiscuss with your group what the three variances represent.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\nThe variance in observed values of Emotional pressure is the total variance in Y (i.e., the dependent variable).\nThe variance in the predicted values of Emotional pressure reflects “differences in emotional pressure that can be explained because some persons have a job with a lot of variety and some have a monotonous job”. This variance component is also known as the explained variance.\nThe variance of the residuals, also known as the residual variance, represents differences in emotional pressure that cannot be attributed to differences in variety at work. Hence, the residual describes differences that are unrelated to variety at work.\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n \n\n\nIn the previous step, we looked at the variances itself, but the numbers are not very informative. A more convenient way to look at the explained variance is proportion wise.\n\nSo, let's use the variances we just generated to calculate the proportion of variance in Emotional pressure that can be explained by Variety at work.\n\n\nWhat percentage of the total variance in emotional pressure can be explained by variety at work? ______^[22\\.981]\n\nConsult the output of the regression analysis again, particularly the table Model Summary.\n\nVerify that the R-square that is reported in the table is the same as the proportion of explained variance that you have calculated yourself.\n\nFinally, independently go through all the steps of a simple regression analysis using the data file [`Work.sav`](data/Work.sav).\n\nYour theory suggests that independence at work predicts emotional pressure.\n\n* Construct an appropriate research question and hypotheses.\n* Conduct the analysis\n* Describe the relationship (i.e., regression coefficient)?\n* Discuss the effect size in terms of R2.\n* Perform a significance test and report your results\n\nFinally, compare the standardized regression coefficient to the R coefficient in the Model Summary table, and optionally to a correlation computed via the Correlation interface. Verify that these are all identical.\n",
    "supporting": [
      "glm2_sum_squares_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}