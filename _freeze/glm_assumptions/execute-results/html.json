{
  "hash": "be767e17e7dfed5c52be0bd0c024fb13",
  "result": {
    "engine": "knitr",
    "markdown": "# Assumptions\n\n\n\nEvery time we use a statistical model to describe data,\nwe make certain simplifying assumptions.\nIf these assumptions are met, the model is a good representation of the data (descriptive statistics),\nand we can make valid inferences about the population based on the model's parameters (inferential statistics).\nHowever, when these assumptions are violated, the model is a bad descriptor of the data, and inferences based on the model can be misleading or difficult to interpret.\n\nTo de-mystify assumptions, let's examine one of the simplest statistical models possible: the normal distribution.\nThe normal distribution is a statistical model to describe the distribution of scores on a variable (or: in the population),\nand its two parameters are the mean and standard deviation.\nIf I draw a random sample of 1000 participants from the population of the Netherlands,\ntheir observed heights might be distributed as in the histogram below.\nI could use the normal distribution as a model for these data, and it would do a pretty good job (see the red normal distribution).\nIn this case, my assumption is that *height is normally distributed around a mean* $\\mu$ *and with standard deviation* $\\sigma$, or:\n\n$$\n\\text{Height} \\sim N(\\mu, \\sigma)\n$$\n\nIf this assumption holds, the mean and standard deviations will be pretty good descriptive statistics of the distribution of data in the sample.\nIf I assume that height is also normally distributed in the population, and that my sample is representative - then my sample statistics are also pretty good estimators for the population parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1)\nx <- rnorm(1000, mean = 175, sd = 15)\noutlrs <- (x < 25 | x > 215)\nwhile(any(outlrs)){\n  x[which(outlrs)] <- rnorm(sum(outlrs), mean = 175, sd = 15)\n  outlrs <- (x < 25 | x > 215)\n}\nlibrary(ggplot2)\n## parameters that will be passed to ``stat_function``\nn = length(x)\nmean = mean(x)\nsd = sd(x)\nbinwidth = 10 # passed to geom_histogram and stat_function\nset.seed(1)\ndf <- data.frame(x = x)\n\nggplot(df, aes(x = x, mean = mean, sd = sd, binwidth = binwidth, n = n)) +\n    theme_bw() +\n    geom_histogram(binwidth = binwidth, \n        colour = \"white\", fill = \"cornflowerblue\", size = 0.1) +\nstat_function(fun = function(x) dnorm(x, mean = mean, sd = sd) * n * binwidth,\n    color = \"darkred\", size = 1) + labs(x = \"Height (cm)\", y = \"Frequency\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](glm_assumptions_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nNow imagine that I draw a convenience sample of 200 members of my local basketball association (figure below).\nDo you think I can assume that their heights will be normally distributed?\nWhy (not)?\nDo you think these individuals will be representative of the Dutch population?\nWill they be representative of the population of Dutch basketball players?\nIf the assumption that these scores are normally distributed is violated,\nthen the mean and standard deviation of the normal distribution will be poor descriptive statistics.\nMoreover, these sample statistics will be poor estimators of the population parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2)\nx <- rnorm(200, mean = 190, sd = 15)\noutlrs <- (x < 185 | x > 240)\nwhile(any(outlrs)){\n  x[which(outlrs)] <- rnorm(sum(outlrs), mean = 175, sd = 15)\n  outlrs <- (x < 185 | x > 240)\n}\n## parameters that will be passed to ``stat_function``\nn = length(x)\nmean = mean(x)\nsd = sd(x)\nbinwidth = 10 # passed to geom_histogram and stat_function\nset.seed(1)\ndf <- data.frame(x = x)\n\nggplot(df, aes(x = x, mean = mean, sd = sd, binwidth = binwidth, n = n)) +\n    theme_bw() +\n    geom_histogram(binwidth = binwidth, \n        colour = \"white\", fill = \"cornflowerblue\", size = 0.1) +\nstat_function(fun = function(x) dnorm(x, mean = mean, sd = sd) * n * binwidth,\n    color = \"darkred\", size = 1) + labs(x = \"Height (cm)\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](glm_assumptions_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n### Assumptions for Linear Regression\n\nThe same principles apply to more complex models than the normal distribution - for example, linear regression.\nIn fact, linear regression can be written *as* a normal distribution whose mean depends on the value of a predictor variable.\nIf this equation says that height is normally distributed:\n\n$$\n\\text{Height} \\sim N(\\mu, \\sigma)\n$$\n\nThen this equation says that height is normally distributed with a mean value that depends on age:\n\n$$\n\\text{Height}_i \\sim N(\\alpha + \\beta*\\text{Age}_i, \\sigma)\n$$\nNotice that the overall (population) mean of height $\\mu$ is now replaced with a linear formula with population intercept $\\alpha$ and effect of Age $\\beta$.\nAnother way to rewrite this formula without changing the meaning is:\n\n\\begin{align}\n\\text{Height}_i &= \\alpha + \\beta * \\text{Age}_i + \\epsilon_i \\\\\n\\epsilon_i &\\sim N(0, \\sigma)\n\\end{align}\n\nThis is the familiar notation of a regression equation.\nThere are two points here: one, regression can be written as an extension of the normal distribution by plugging a linear formula in the place of the distribution mean. This means that regression inherits the assumptions of the normal distribution (e.g., no outliers), and gains a few more because of the added linear formula.\nTwo, all of the assumptions are right there, in the formula itself: \nthe fact that we specify height as a *linear function* of age means that we assume linearity.\nThe fact that we use a little subscript $_i$ for height and age means that we assume independent observations from different individuals for these variables.\nThe fact that we have one normal distribution for the error term $\\epsilon_i$ means that we do not expect the error distribution to vary at different values of the predictor, in other words, we expect homoscedasticity.\n\nBelow, we get deeper into the assumptions of linear regression, explains why each one matters, and shows how to check whether they are likely to hold in your data.\n\n#### Independence of Observations\n\nLinear regression assumes that every observation, or every row in the dataset, represents an independent observation, contributing unique information to the dataset.\nThis means that observations should not be systematically related to each other.\nFor example, participants should not be partners, friends, classmates, et cetera - any reason why participants might be more similar than randomly selected members of the population could introduce a violation of the assumption of independence.\n\n**Why it matters**  \n\nWhen observations are *clustered* - for example, when data come from students in the same classroom, patients treated by the same clinician, or repeated measurements from the same individual - the assumption of independence is violated. In such cases, the residuals of these observations are correlated,\nwhich causes the model to underestimate the true variability, leading to overconfident conclusions.\n\n**How to check**  \n\n- Consider the study design: Were the data collected from naturally grouped or repeated units, such as individuals within teams, families, schools, or measured over time?\n\n**When it is violated**  \n\nThis assumption is likely to be violated when:\n\n- Observations are nested within a shared context (e.g., students within schools).\n- The same individual or unit appears multiple times in the dataset.\n- There is a known time-based or spatial structure to the data.\n\n#### Correct Measurement Levels\n\nLinear regression requires the outcome variable (*Y*) to have a continuous measurement level.\nThis also follows from the linearity assumption:\nif we assume that an increase on X from 1 to 2 will have the same effect (regression slope) as an increase on X from 4 to 5,\nthen that also means that Y must have a measurement level where the same distance has the same numerical value (interval or ratio).\nExamples of appropriate variables are test scores, height, or income. Predictors (*X* variables) can have any measurement level,\nbut they must be *encoded* as continuous or binary (0 and 1-coded dummy variables).\nSome statistical software encodes nominal and ordinal variables as binary indicators behind the scenes, effectively doing this work for you.\n\n**Why it matters**  \n\nIf *Y* is nominal, it does not make sense to predict it numerically.\nIf *Y* is ordinal, we cannot be sure that steps of equal numerical size have the same meaning.\nSometimes a linear model works quite well for ordinal scales,\nbut it is always important to check for indications of model violations when you use it for such variables.\n\n**How to check**  \n\nFirst, review the codebook, metadata, or variable definitions in SPSS to confirm that:\n\n- The outcome (*Y*) is coded as a continuous numeric variable.\n- Categorical predictors are either dummy-coded or otherwise appropriately handled.\n\n**When it is violated**  \n\nThe assumption is violated when:\n\n- *Y* is nominal (categories like “red”, “green”, “blue”) or ordinal (e.g., Likert scales).\n- An ordinal *X* is treated as numeric without justification, leading the model to assume equal spacing between categories.\n\nNote that the operationalization of a variable is not the only factor that matters; its true measurement level also matters.\nFor example, if you want to measure height, you could put a mark on the doorpost and rate everyone taller than the mark as \"tall\", and anyone shorter than the mark as \"short\" - but the fact that you operationalized height this way doesn't negate the fact that it is inherently a continuous variable.\n\nMore pertinently: gender is often operationalized as binary.\nThis does not mean that gender *is* binary; its true measurement level is more complex.\nIf you are interested in gender as a social construct, then there are more then two discrete categories.\nIf you are interested in gender for its biological aspects, then there is both nominal variability.\nNominal variability includes aneuploidy of sex chromosomes,\nand continuous variability occurs in various biological aspects of male-ness and female-ness, like hormone balances and -sensitivities.\n\n\n#### Linearity – the “straight-line” assumption\n\nLinear regression assumes that the relationship between each predictor X and the outcome Y is linear, that is, that the same change in X corresponds to the same change in Y for all values of X. The slope $\\beta$, or its sample estimate $b$, tells us how much *Y* is expected to increase (or decrease) for a one-unit increase in *X*, but this only holds if the relationship is approximately linear.\n\n**Why it matters**\n\nIf the relationship is actually non-linear, fitting a straight line will misrepresent the nature or strength of the association.\nRemember Anscombe's quartet in @sec-anscombe.\nA straight line fitted to a pattern in data that, in reality/in the population, is non-linear (quadratic, S-shaped, etc) will result in inaccurate or meaningless slope estimates and misleading conclusions about the predictor’s effect.\n\n**How to check**  \n\n1. Create a scatterplot of *Y* against each *X* variable.  \n2. Add a straight trend line (e.g., “fit line at total” in your software).\n3. Visually assess whether the line aligns with the overall pattern of the data points.\n\n**When it is violated**  \n\nLinearity is likely violated if the plotted points form a clear curve, wave, or other systematic pattern that deviates from a straight path.\n\nAlternatively, outside the scope of this course:\n\n1. Estimate a linear model\n1. Estimate a model with a different functional form (e.g., quadratic)\n1. Compare the fit of the models using the BIC model fit index (lower is better) \n\n\n\n\n\n\n\n#### Normality of Residuals\n\nAs evident from the $\\epsilon_i \\sim N(0, \\sigma)$ part of the regression equation, linear regression assumes that the residuals, the differences between the observed and predicted values, are normally distributed.\n\n**Why it matters**  \n\nWhen the residuals deviate strongly from normality (e.g., they are skewed or have heavy tails), inferences based on the regression model may be misleading. Standard errors and metrics derived from them, like *p*-values, and confidence intervals, depend on this assumption.\n\n**How to check**  \n\n- Save the residuals and inspect their distribution using a **histogram** or a **Q–Q plot**.  \n- For very small datasets (e.g., *N* < 50), formal tests such as the **Shapiro–Wilk test** can be used to assess normality.\n\n**When it is violated**  \n\nThis assumption is often violated when:\n\n- The outcome variable is highly skewed or bounded\n- There are extreme values in the outcome that disproportionately influence the residuals.  \n- The sample size is small and the residual pattern does not resemble a bell-shaped curve.\n\n#### Homoscedasticity\n\nThe fact that linear regression has a single error term implies an assumption that the variance of the residuals remains constant across all levels of the predicted values. This condition, known as *homoscedasticity* (homo = equal, scedasticity = variance), implies that the model has equal predictive accuracy across the full range of the outcome.\n\n**Why it matters**  \n\nWhen residuals fan out, contract, or otherwise vary as the predicted values increase, this is called *heteroscedasticity* (hetero = different, scedasticity = variability). In such cases, the standard errors may be inaccurate, which undermines the reliability of p-values and confidence intervals.\n\n\n**How to check**  \n\n1. Save the residuals from the fitted model.  \n2. Create a scatter plot of residuals (Y-axis) and predicted values (X-axis).\n3. Visually determine whether the spread of residuals appears approximately constant across the range of predicted values.\n\n**When it is violated**  \n\nThis assumption is violated when:\n- The residuals become more dispersed or more concentrated as the predicted value increases.\n- The residual-versus-predicted plot reveals a funnel-like or cone-shaped pattern rather than a uniform band.\n- The residuals shows any pattern, other than a random dot cloud.\n\n#### No Outliers\n\nThe assumption of no outliers is related to the assumption of linearity and the assumptions of normal, homoscedastic residuals.\nAn extreme case can distort slope estimates (as in Anscombe's quartet, figure c) and standard errors, and consequently, confidence intervals and *p*-values which are based on those standard errors.\n\n**Why it matters**  \n\nOutliers can pull the regression line toward themselves, leading to misleading interpretations. Even a single influential point can change the direction, strength, or significance of a predictor's effect.\n\n**How to check**  \n\n- Calculate diagnostic statistics such as **leverage**, **Cook’s distance**, and **standardized residuals** to detect potential outliers.\n\n**When it is violated**  \n\nThis assumption may be violated when:\n\na. A case lies far from the bulk of the data on one or more predictors.  \na. The residual for a single observation is large relative to others.  \na. Diagnostic measures flag a case as both high-leverage and high-influence.\n\n#### Reliable Predictors\n\nIn the regression equation, the outcome has an error term, $\\epsilon_i$.\nIf the model makes imperfect predictions for any reason,\nthese prediction errors contribute to the error term.\nOne reason for prediction error is measurement error in the outcome.\nNote, however, that while the outcome has an error term - the predictor does not.\nThis implies an assumption that predictor variables (*X*) are measured without error.\nInaccurate or inconsistent measurement introduces noise, which can attenuate the estimated relationship between *X* and *Y*. As a result, regression coefficients may be biased toward zero, and the model may attribute true effects to random error.\n\n**Why it matters**  \n\nWhen predictors are measured with error (unreliable), the estimated slopes become less trustworthy. Even in large samples, measurement error in *X* can severely compromise the interpretability of regression results, leading to underestimation of effect sizes and increased standard errors.\n\n**How to check**  \n\n- For predictors based on multiple items (e.g., survey scales), compute internal consistency (e.g., Cronbach’s alpha). Values below 0.70 often indicate problematic measurement.\n- If repeated measurements of the same predictor are available, examine the test–retest correlation. High correlation supports reliability.\n\n**When it is violated**  \n\nThis assumption is violated when:\n- A predictor contains high random measurement error.\n- Multi-item scales exhibit low internal consistency.\n- Temporal stability of repeated measures is weak (e.g., inconsistent responses across time).\n\n\n\n#### *No Multicollinearity*\n\nAs a preview of what is to come: it is possible to include more than one predictor in a regression model.\nThis is called *multiple regression*, and it will be covered in Statistics 2.\nMultiple regression additionally assumes that each predictor contributes uniquely to the explanation of the outcome variable.\nWhen two or more predictors explain the *same* variance in the outcome,\nthe model struggles to estimate their unique effects.\nThis overlap in explained variance makes coefficient estimates unstable and difficult to interpret,\nand inflates the individual predictors' standard errors.\n\n**Why it matters**  \n\nMulticollinearity undermines the precision of regression coefficients. When predictors convey redundant information, the model’s ability to estimate each slope independently deteriorates. This can lead to wide confidence intervals, non-significant p-values, or coefficients with counterintuitive signs.\n\n**How to check**  \n\n- Examine the **Variance Inflation Factor (VIF)** for each predictor. A common guideline is that values above 5 may indicate multicolinearity, values above 10 indicate severe multicolinearity.  \n\n**When it is violated**  \n\nThis assumption may be violated when:\n\n- Two predictors are strongly correlated (e.g., income and years of education).\n- More than two predictors explain the same variance in the outcome (e.g., mother's income, father's income, and family Socio-Economic Status might explain the same variance in children's educational attainment)\n- VIF values are unusually high.  \n- The inclusion of additional predictors drastically alters the estimated slopes or increases their standard errors.\n\n\n#### Putting It All Together\n\nBefore interpreting regression results, it is essential to check for evidence of violations of assumptions.\nIf the assumptions are violated, interpretation of the results might not be straightforward.\nNote, however, that assumption checks are subject to the same caveats as other statistical inferences:\n\n\n* Assumptions are statements about the *population*; even if they appear to be violated in the *sample*, they might be met in the population\n* Consequently, it is possible to draw false positive (incorrectly concluding that an assumption is violated, while in reality it is not) or false negative (incorrectly concluding that an assumption is not violated, when in reality it is) conclusions about assumptions\n* Making data-driven analysis decisions incurs researcher degrees of freedom (see the chapter on questionable research practices). You run the risk of overfitting (customizing) your analysis so much to the sample at hand that it no longer generalizes well to the general population, or other samples from that population.\n\nNone of this diminishes the importance of checking assumptions, it is merely a call to exercise critical thinking when doing so.\nFor example, regression assumes that the dependent variable is normally distributed.\nIf your dependent variable is a Likert-type scale,\nsometimes, you may get away with making this assumption (figure @fig-figviolate panel a below).\nNevertheless, you should discuss this *potential* violation of the assumption of normality in the Discussion of your paper or report.\nHowever, if you notice after data collection that your dependent variable is distributed as in figure b below, the assumption of normality is so eggregiously violated that analysis results are probably meaningless.\nIn this case, you might still analyze the results as planned - but that analysis will likely be meaningless.\nYou might want to present a second analysis that treats the outcome as ordinal (which is possible, but outside the scope of this course),\nand emphasize that this was a data-driven analysis decision.\n\nIn sum:\n\n* Always check assumptions\n* Always report the result of assumption checks and discuss (in the Discussion) how potential violation might affect your conclusions\n* In case of strong evidence of violations, you might report a secondary analysis that is robust to the violation of the assumption, but make it clear that this analysis was performed *after* seeing the data.\n* Optionally, compare the results of both the planned analysis and the robust analysis. If the conclusions are the same (e.g., both analyses provide results consistent with your hypothesis), this is reassuring.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nset.seed(1)\nn = 100\nx <- rnorm(n)\ndf <- data.frame(\n  Value = c(cut(x, 7, labels = FALSE),\n            cut(x, breaks = c(-1, 0, 2, 3, 4), labels = FALSE)\n            ),\n  Panel = rep(c(\"a\", \"b\"), each = n)\n)\nggplot(df, aes(x = Value)) + geom_bar() + facet_wrap(~Panel) + scale_x_continuous(breaks = c(1:7)) +scale_y_continuous(expand = c(0,0)) + theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 11 rows containing non-finite outside the scale range\n(`stat_count()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Figure a shows a Likert-type scale that is approximately normally distributed. Figure b shows a Likert scale with extreme censoring](glm_assumptions_files/figure-html/figviolate-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n1. **Visual diagnostics** – Begin with graphical checks for linearity, constant variance, outliers, and normality of residuals. Plots often reveal violations at a glance.  \n2. **Statistical diagnostics** – Follow up with numerical checks, such as the Variance Inflation Factor (VIF) for multicollinearity and formal tests for heteroskedasticity or autocorrelation when appropriate.  \n3. **Assessment of model structure** – Consider whether the data meet requirements for independence and correct scale of measurement. \n\n## Formative Test\n\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\n\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nWhich statement best captures the linearity assumption in OLS regression? <div class='webex-radiogroup' id='radio_BBSQBKCCNS'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_BBSQBKCCNS\" value=\"\"></input> <span>The residuals must have constant variance</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_BBSQBKCCNS\" value=\"\"></input> <span>The predictors must be normally distributed</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_BBSQBKCCNS\" value=\"answer\"></input> <span>The expected change in the outcome is proportional to changes in each predictor across its range</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_BBSQBKCCNS\" value=\"\"></input> <span>The outcome must be measured without error</span></label></div>\n\n\n**Question 2**\n\nHomoscedasticity means: <div class='webex-radiogroup' id='radio_IRZXJOTEYE'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IRZXJOTEYE\" value=\"answer\"></input> <span>Residuals have constant variance across all levels of the predicted values</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IRZXJOTEYE\" value=\"\"></input> <span>Predictors are uncorrelated with each other</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IRZXJOTEYE\" value=\"\"></input> <span>Residuals are centered at zero</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IRZXJOTEYE\" value=\"\"></input> <span>Predictors are measured on a continuous scale</span></label></div>\n\n\n**Question 3**\n\nA residuals versus predicted plot shows a clear funnel shape. Which assumption is most likely violated? <div class='webex-radiogroup' id='radio_LNFTPOBACX'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LNFTPOBACX\" value=\"\"></input> <span>Independence of observations</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LNFTPOBACX\" value=\"\"></input> <span>Normality of predictors</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LNFTPOBACX\" value=\"answer\"></input> <span>Homoscedasticity</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LNFTPOBACX\" value=\"\"></input> <span>Linearity</span></label></div>\n\n\n**Question 4**\n\nWhy does normality of residuals matter most for small samples in OLS? <div class='webex-radiogroup' id='radio_EILUHDQIUA'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_EILUHDQIUA\" value=\"answer\"></input> <span>It underpins the accuracy of t tests and confidence intervals for coefficients</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_EILUHDQIUA\" value=\"\"></input> <span>It guarantees unbiased slope estimates</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_EILUHDQIUA\" value=\"\"></input> <span>It eliminates the need to check other assumptions</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_EILUHDQIUA\" value=\"\"></input> <span>It ensures predictors are measured reliably</span></label></div>\n\n\n**Question 5**\n\nYour data consist of students nested within classrooms but you fit a single level OLS model that treats all rows as independent. Which assumption is threatened? <div class='webex-radiogroup' id='radio_YXUUVFRPBH'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YXUUVFRPBH\" value=\"answer\"></input> <span>Independence of observations</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YXUUVFRPBH\" value=\"\"></input> <span>No outliers</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YXUUVFRPBH\" value=\"\"></input> <span>Normality of predictors</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YXUUVFRPBH\" value=\"\"></input> <span>Correct scale of measurement for the outcome</span></label></div>\n\n\n**Question 6**\n\nTwo predictors are highly correlated. What is the most common consequence for the regression coefficients? <div class='webex-radiogroup' id='radio_HUEHXGTDQJ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HUEHXGTDQJ\" value=\"\"></input> <span>They become unbiased and more precise</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HUEHXGTDQJ\" value=\"\"></input> <span>They remain unchanged but model fit worsens</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HUEHXGTDQJ\" value=\"\"></input> <span>They become systematically too large</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HUEHXGTDQJ\" value=\"answer\"></input> <span>They become unstable with inflated standard errors and may change sign with small data changes</span></label></div>\n\n\n**Question 7**\n\nWhich outcome variable violates the correct scale of measurement assumption for standard OLS regression? <div class='webex-radiogroup' id='radio_IOYFFDCCMI'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IOYFFDCCMI\" value=\"\"></input> <span>Height in centimeters</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IOYFFDCCMI\" value=\"answer\"></input> <span>A binary pass or fail indicator coded 0 and 1</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IOYFFDCCMI\" value=\"\"></input> <span>Test score from 0 to 100</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IOYFFDCCMI\" value=\"\"></input> <span>Annual income in dollars</span></label></div>\n\n\n**Question 8**\n\nWhat is the typical effect of measurement error in a predictor on its estimated slope in OLS? <div class='webex-radiogroup' id='radio_NEGTTJHANL'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NEGTTJHANL\" value=\"answer\"></input> <span>Bias toward zero</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NEGTTJHANL\" value=\"\"></input> <span>No bias but larger p values</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NEGTTJHANL\" value=\"\"></input> <span>No effect if the outcome is normal</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NEGTTJHANL\" value=\"\"></input> <span>Bias away from zero</span></label></div>\n\n\n**Question 9**\n\nYou see a systematic curve in the residuals versus predicted plot. Which assumption is most suspect? <div class='webex-radiogroup' id='radio_QADUUVYXEL'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_QADUUVYXEL\" value=\"answer\"></input> <span>Linearity</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_QADUUVYXEL\" value=\"\"></input> <span>Independence</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_QADUUVYXEL\" value=\"\"></input> <span>Homoscedasticity</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_QADUUVYXEL\" value=\"\"></input> <span>No outliers</span></label></div>\n\n\n**Question 10**\n\nWhich statement about outliers and influence in OLS is correct? <div class='webex-radiogroup' id='radio_ORAHUXJGGK'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ORAHUXJGGK\" value=\"\"></input> <span>Only points with extreme outcome values can be influential</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ORAHUXJGGK\" value=\"\"></input> <span>Any point far from the regression line is necessarily highly influential</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ORAHUXJGGK\" value=\"answer\"></input> <span>A point can be influential if it has unusual predictor values and substantially changes the fitted line</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ORAHUXJGGK\" value=\"\"></input> <span>Influential points affect p values but never change coefficient signs</span></label></div>\n\n\n:::\n\n\n<div class='webex-solution'><button>Show explanations</button>\n**Question 1**\n\nLinearity is about the average of Y at each X forming a straight line. The other options describe different assumptions.\n\n**Question 2**\n\nHomoscedasticity is constant spread of residuals across the range of fitted values.\n\n**Question 3**\n\nA funnel pattern indicates changing residual variance across fitted values.\n\n**Question 4**\n\nWith small samples inference for slopes uses normality. In large samples asymptotics help.\n\n**Question 5**\n\nClustering creates correlated errors. Rows are not independent.\n\n**Question 6**\n\nHigh correlation among predictors creates multicollinearity and unstable estimates.\n\n**Question 7**\n\nOLS assumes a continuous outcome. A binary outcome calls for a different model.\n\n**Question 8**\n\nClassical measurement error in X attenuates the slope.\n\n**Question 9**\n\nA patterned residual plot suggests a wrong functional form. The relation is not linear.\n\n**Question 10**\n\nInfluence depends on leverage and residual. Such points can change slopes and even signs.\n\n\n</div>\n:::\n\n\n## Tutorial\n\n### Before you start\n\n- In each exercise, fit an OLS model with y as the outcome and the listed predictors.\n\n### Assignment 1\n\nFile: [`reg_assump_check.sav`](data/reg_assump_check.sav)  \nVariables: y, x1, x2\n\nSteps (SPSS GUI):\n\n1) **Check linearity (for each predictor)**\n   - Graphs > Legacy Dialogs > Scatter/Dot > Simple Scatter\n     - Y axis: **y**; X axis: **x1**. Create plot.  \n     - Repeat for **x2**.  \n   - In the Chart Editor: Element > Fit Line at Total.  \n2) Analyze > Regression > Linear  \n   - Dependent: y  \n   - Independents: x1 x2  \n   - Statistics: Estimates, Confidence intervals, Collinearity diagnostics  \n   - Plots: ZPRED on X, ZRESID on Y  \n   - Save: Standardized residuals (ZRESID), Predicted values (ZPRED)\n3) Graphs > Legacy Dialogs > Histogram  \n   - Variable: ZRESID; check \"Display normal curve\"\n4) Graphs > Legacy Dialogs > Q-Q  \n   - Variable: ZRESID\n\nQuestions:\n\n- Is there a linear association among these variables? \n- Is the measurement scale of these variables appropriate for regression analysis?\n- Are residuals normally distributed?  \n- Is homoscedasticity supported?  \n- Are there any multicollinearity concerns?\n\n\n### Assignment 2 — Linearity check\n\nFile: [`reg_linearity_check.sav`](data/reg_linearity_check.sav)  \nVariables: y, x1\n\nSteps:\n\n1) Graphs > Legacy Dialogs > Scatter/Dot > Simple Scatter  \n   - Y axis: y; X axis: x1  \n   - In Chart Editor, add a straight fit line (Fit Line at Total).\n2) Analyze > Regression > Linear  \n   - y on x1; request the same plots and saves as in Assignment 1.\n\nQuestions:\n\n- Does the y vs x1 scatterplot suggest a straight-line relation?  \n- Do residual plots show a pattern (e.g., systematic bends) inconsistent with linearity?  \n- Based on the diagnostics, is a linear specification for x1 adequate in this dataset?\n\n\n### Assignment 3 — Homoscedasticity\n\nFile: [`reg_homoscedasticity_check.sav`](data/reg_homoscedasticity_check.sav)  \nVariables: y, x1\n\nSteps:\n1) Fit OLS as in Assignment 1 and save ZRESID and ZPRED.  \n2) Graphs > Legacy Dialogs > Scatter/Dot > Simple Scatter  \n   - X axis: ZPRED; Y axis: ZRESID.\n\nQuestions:\n\n- Do residuals show roughly constant spread across predicted values, or a cone/funnel?  \n\n\n### Assignment 4 — Normality of residuals\n\nFile: [`reg_normality_check.sav`](data/reg_normality_check.sav)  \nVariables: y, x1\n\nSteps:\n\n1) Fit the regression and save residuals\n   - Analyze > Regression > Linear\n     - Dependent: y\n     - Independent(s): your predictor(s)\n     - Save > Standardized residuals\n     - Plots > Tick Histogram \n2) To produce Q-Q plot\n   - Analyze > Descriptive Statistics > Explore\n   - Add the saved standardized residuals to the dependent list\n   - Plots > Check Normality plots with tests \n\nQuestions:\n\n- Is the residual distribution approximately symmetric and bell-shaped?  \n- Do Q-Q points track the diagonal?\n\n\n### Assignment 5 — Outliers\n\nFile: [`reg_outliers.sav`](data/reg_outliers.sav)  \nVariables: y, x1\n\nSteps:\n\n1) Analyze > Regression > Linear  \n   - Dependent: y  \n   - Independent: x1  \n   - Statistics: Estimates, Casewise diagnostics (e.g., standardized residuals > 3), Collinearity diagnostics (optional here)  \n   - Save: Cook’s distance, and Leverage (Hat)\n2) Graphs > Legacy Dialogs > Boxplot\n   - Summaries for separate variables: **y**, **x1** \n3) Graphs > Legacy Dialogs > Scatter/Dot > Simple Scatter  \n   - Y axis: **y**; X axis: **x1** \n\n\nQuestions:\n\n- Are any cases flagged in Casewise diagnostics (e.g., |Std. Residual| > 3)?  \n- Do any observations show high leverage* or large Cook’s distance relative to others?  \n- Based on these diagnostics, could a single case plausibly dominate the fitted line? Identify the case ID if so.\n\n\n### Exercise 6 — Multicollinearity\n\nFile: [`reg_multicollinearity.sav`](data/reg_multicollinearity.sav)  \n\nVariables: y, x1, x2, x3\n\nSteps:\n1) Analyze > Correlate > Bivariate  \n   - Inspect correlations among x1, x2, x3.  \n2) Analyze > Regression > Linear  \n   - y on x1 x2 x3  \n   - Statistics: Collinearity diagnostics, Estimates.\n\nQuestions:\n\n- Are any predictor pairs highly correlated in the correlation matrix?  \n- What are the VIF and Tolerance values for each predictor?  \n- Do the signs and standardized Betas align with the simple correlations, or do you see suppression patterns?\n\nWhat to look for:\n\n- VIF substantially above 5 (or Tolerance, which is $\\frac{1}{VIF}$, below .20) suggests collinearity.\n- Large divergence between simple r and Beta can signal overlap among predictors.\n\n\n## Assignment 7 — Putting it together (choose two datasets)\n\nFiles: pick any two from the set\n\nTask:\n\n- For each dataset, run the standard diagnostic workflow from Assignment 1. - Summarize, in a short paragraph per dataset, which assumptions are reasonably met and which are doubtful, citing the specific plot or statistic you used.\n\nReminder:\n\n- Focus on diagnosis only. Do not apply remedies or re-specify models here.\n",
    "supporting": [
      "glm_assumptions_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}