{
  "hash": "6b01a6d799f5d3e94f874ddd704941d9",
  "result": {
    "engine": "knitr",
    "markdown": "# Bivariate Descriptive Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the previous chapter, we covered univariate (= single variable) descriptive statistics.\nIn this chapter, we introduce the first *bivariate* (= two variables) descriptive statistics.\nLet's take stock of the road so far, and set a goal for where we want to go.\nLast week, we ended with the variance.\nThe variance is a statistic that tells us, on average, how much people's scores deviate from the mean.\nToday we will move into bivariate descriptive statistics.\nWe will learn about the *covariance*, which tells us:\nIf someone's score on one variable deviates positively from the mean,\nis their score on another variable also likely to deviate positively from the mean?\nWe will also learn about the *correlation*, which tells us:\nHow strong is the association between two variables, and is it positive or negative?\n\nIn this chapter, we will talk about two hypothetical variables, *X* and *Y*.\nIn your mind, you can substitute any two variables you like; for example, *X = hours studied*, *Y = grade obtained*, or *X = extraversion*, *Y = number of friends*.\n\nBefore arriving at the correlation coefficient, statisticians often begin with **covariance**—a preliminary measure of how two variables vary together. Covariance reflects direction: it is positive when high values of *X* accompany high values of *Y*, and negative when they move in opposite directions. However, its numerical value is not directly interpretable because it is tied to the units of the measurement. A covariance expressed in centimeters and kilograms will differ from one computed in meters and pounds, even if the underlying association remains unchanged. As a result, covariance cannot meaningfully convey the *strength* of a relationship—only whether the variables tend to move in the same or opposite directions.\n\n\nIt provides a concise summary of the association between pairs of scores across individuals. For example, a researcher might retrieve each student’s high school GPA (a measure of academic performance) and pair it with their family’s annual income. The goal is to determine whether higher grades tend to correspond with higher income. In correlational studies, each individual contributes two measurements, commonly referred to as *X* and *Y* forming the foundation for analysis.\n\n\nThe correlation coefficient is a statistic that quantifies the strength and direction of association between two variables.\nIt tells us the degree to which two variables move together.\nOne way to think of the correlation coefficient is as a *bivariate (= two variables) descriptive statistic*.\n\nTo explore this relationship visually, researchers often rely on scatter plots. In a scatter plot, *X* values appear along the horizontal axis and *Y* values along the vertical. Each point on the plot corresponds to one participant’s pair of scores. These plots allow immediate detection of linear trends, and outliers—patterns that may remain obscured when examining data in purely numerical or tabular form.\n\n## Covariance\n\nThe word \"covariance\" means: varying, or moving, together.\nLet's have a look at mock data from five students on hours studied and final grade obtained:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2)\ngrads <- data.frame(\n  Hours = round(runif(5, 2, 20))\n)\ngrads$Grade <- round(scales::rescale(.7*grads$Hours + rnorm(5), to = c(1, 10)))\nknitr::kable(grads)\n```\n\n::: {.cell-output-display}\n\n\n| Hours| Grade|\n|-----:|-----:|\n|     5|     3|\n|    15|     9|\n|    12|     6|\n|     5|     1|\n|    19|    10|\n\n\n:::\n:::\n\n\n\n\n\n\nWe can visualize these data using a \"scatterplot\"; a simple graph where each observation is shown as a dot with X-coordinate determined by their value on the X variable (Hours), and Y-coordinate determined by the Y variable (Grade):\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(grads, aes(x = Hours, y = Grade)) + geom_point() + theme_bw()\n```\n\n::: {.cell-output-display}\n![](Chapter3_Correlation_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n\nNotice that, if you squint, it appears like there might be some pattern in the data: more hours studied tends to go hand in hand with a higher grade.\nThere might be a positive association between these variables!\nIn the next sections, we go about quantifying this association numerically, step by step.\n\n### Sum of Products (SP)\n\nThe first stage in quantifying the association between two variables is to compute the **sum of products of deviations** (SP).\nThe SP is similar to the sum of squares (SS),\nbut whereas the SS captures the variability of one variable,\nthe SP measures how two variables vary together.\n\nTo calculate the SP, take the following steps:\n\n#### Step 1: Calculate the variables' means\n\nTake the mean of each column (bold in the table below):\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(kableExtra)\nmns <- grads\nmns[] <- lapply(mns, as.character)\nmns <- rbind(mns, colMeans(grads))\nnames(mns) <- c(\"X\", \"Y\")\nkable(mns) |>\n  kable_styling() |>\n  row_spec(nrow(mns),bold=T,hline_after = T)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> X </th>\n   <th style=\"text-align:left;\"> Y </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 5 </td>\n   <td style=\"text-align:left;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 15 </td>\n   <td style=\"text-align:left;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 12 </td>\n   <td style=\"text-align:left;\"> 6 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5 </td>\n   <td style=\"text-align:left;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 19 </td>\n   <td style=\"text-align:left;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 11.2 </td>\n   <td style=\"text-align:left;font-weight: bold;\"> 5.8 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n\n#### Step 2: Calculate Deviations\n\nFor each variable, calculate the deviations by subtracting the mean from the observed scores:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(kableExtra)\ndevs <- grads\ndevs <- cbind(devs, sweep(devs, 2, colMeans(devs)))\nnames(devs) <- c(\"X\", \"Y\", \"X-mean(X)\", \"Y-mean(Y)\")\nkable(devs)\n```\n\n::: {.cell-output-display}\n\n\n|  X|  Y| X-mean(X)| Y-mean(Y)|\n|--:|--:|---------:|---------:|\n|  5|  3|      -6.2|      -2.8|\n| 15|  9|       3.8|       3.2|\n| 12|  6|       0.8|       0.2|\n|  5|  1|      -6.2|      -4.8|\n| 19| 10|       7.8|       4.2|\n\n\n:::\n:::\n\n\n\n\n\n\n#### Step 3: Multiply Deviations\n\nIf we were to calculate the SS, we would now square the deviations and add them up in each column.\nTo get the SP, instead of squaring the deviations - we multiply them across variables.\nNote that if the deviations for both variables have the same sign, then this will give a positive result (positive times positive is positive, and negative times negative is positive too).\nMoreover, if the deviations from both variables are high, the product will be a high number too.\nSo the SP tends to be a large positive number if high positive (or negative) deviations on one variable go hand in hand with high positive (or negative) deviations on the other variable.\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprods <- devs\nprods <- cbind(prods, apply(devs[, 3:4], 1, prod))\nnames(prods)[5] <- \"Product\"\nkable(prods)\n```\n\n::: {.cell-output-display}\n\n\n|  X|  Y| X-mean(X)| Y-mean(Y)| Product|\n|--:|--:|---------:|---------:|-------:|\n|  5|  3|      -6.2|      -2.8|   17.36|\n| 15|  9|       3.8|       3.2|   12.16|\n| 12|  6|       0.8|       0.2|    0.16|\n|  5|  1|      -6.2|      -4.8|   29.76|\n| 19| 10|       7.8|       4.2|   32.76|\n\n\n:::\n:::\n\n\n\n\n\n\nNow, we calculate the SP just by taking the sum of the column of products: 92.2.\n\nNote that if the SP is positive, then there is a positive association between the variables;\nif it is negative, there is a negative association.\nIn this case, the association is positive.\n\nHere is a formula describing what we just did: we took the sum $\\Sigma$ of the product $()()$ of the deviations of X from the mean of X, $X-\\bar{X}$ times the deviations of Y from the mean of Y, $Y-\\bar{Y}$:\n\n$$\nSP = \\sum \\bigl(X - \\bar{X}\\bigr)\\bigl(Y - \\bar{Y}\\bigr)\n$$\n\n\n### Covariance\n\nTo get the covariance from the sum of products, we divide by the sample size, so in this case, $\\frac{92.2}{5}$.\n\nAnother way to think about this is: we standardize the SP by the sample size $n$.\nThis gives us the \"average co-deviation\" per participant.\nThat number is called the covariance.\n\nIf the covariance is positive, there is a positive association between the two variables.\nIf it is negative, there is a negative association.\n\nBut how strong is the association?\nIt is hard to say, because the *size* of the covariance depends on the units and scale of the two variables involved.\n\n## Correlation\n\nTo answer the question of how strong the association is,\nwe must standardize the covariance to drop the units of both variables.\nThis gives us the so-called **Pearson correlation coefficient (*r*)**.\nSpecifically, the covariance is divided by the product of the standard deviations of *X* and *Y*.\nThis standardization results in a number between -1 and +1, where 0 means no association, -1 means perfect negative association, and +1 means perfect positive association.\nThis number, the correlation coefficient, tells us both the **direction** (-/+) and **strength** (value) of association between two variables.\nBecause the correlation coefficients is unit-free, or standardized, it can also be compared across variables measured on different scales and across studies.\n\n## Limitations\n\nWhile correlation coefficients are useful,\nthey must be interpreted with care.\n\nTo illustrate the limitations of correlations, the statistician Anscombe (1973) created four data sets with identical correlation coefficients, $r = 0.82$.\nWhen plotting the data, however, it becomes clear that the correlation coefficient can only be meaningfully interpreted for the first dataset (figure a below).\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplts <- lapply(1:4, function(i){\n  df <- anscombe[, paste0(c(\"x\", \"y\"), i)]\n  names(df) <- c(\"X\", \"Y\")\n\n  ggplot(df, aes(x = X, y = Y)) + geom_point(shape = 21, size = 3, fill = \"orange\") + theme_linedraw()\n})\nggpubr::ggarrange(plotlist = plts, ncol = 2, nrow = 2, labels = \"auto\")\n```\n\n::: {.cell-output-display}\n![Anscombe's quartet, 1973](Chapter3_Correlation_files/figure-html/fig-anscombe-1.png){#fig-anscombe fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n\nThe first and most important limitation is that, Pearson's correlation coefficient only meaningfully captures *linear associations*, or: patterns that look like a straight line.\nNote that figure a in @fig-anscombe shows such a linear pattern of association;\nthe correlation coefficient of $r = .82$ tells us that there is a strong - but not perfect - positive association.\n\nFigure b in @fig-anscombe , on the other hand, shows a *perfect non-linear* association.\nAll dots are perfectly in line; the line is just not straight.\nThis illustrates that Pearson's correlation coefficient is not suited for capturing non-linear patterns, even if a strong relationship exists in another form.\n\nFigure c shows a correlation of $r = 1$ for most of the points - but one outlier brings it down to $r = .82$.\n\nFigure d shows no association at all for most of the points (they all have the same value for X, and if X does not vary, it cannot covary/correlate with Y) - but a single outlier makes it look like there is a strong correlation..\n\nSecondly, these plots illustrate that **outliers** can have a disproportionate impact.\nIn figures c and d, a single extreme observation artificially deflates (c) or inflates (d) the correlation coefficient,\npotentially leading to misleading conclusions.\n\nThirdly, a **restricted range** of scores can obscure or distort relationships.\nFor example, if you were to examine the pattern in figure b of @fig-anscombe for values of X between [4, 9], you would conclude that $r = 0.99$, or near perfect positive correlation.\nIf you examined the same pattern for values of X between (0, 13), you would conclude that $r = -0.07$, or near-zero.\nIf you examined the same pattern for values of X between [13, 20), you would conclude that $r = -1.00$, or perfect negative correlation.\nFigure @fig-nonlin below zooms into the pattern from figure b, by restricting the range of variable X into three segments:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf <- anscombe[, paste0(c(\"x\", \"y\"), 2)]\nnames(df) <- c(\"X\", \"Y\")\np1 <- ggplot(df[df$X <= 9, ], aes(x = X, y = Y)) + geom_point(shape = 21, size = 3, fill = \"orange\") + theme_linedraw()+ geom_smooth(method = \"lm\", se = FALSE)\np2 <- ggplot(df[df$X > 9 & df$X < 13, ], aes(x = X, y = Y)) + geom_point(shape = 21, size = 3, fill = \"orange\") + theme_linedraw() + geom_smooth(method = \"lm\", se = FALSE)\np3 <- ggplot(df[df$X >= 13, ], aes(x = X, y = Y)) + geom_point(shape = 21, size = 3, fill = \"orange\") + theme_linedraw() + geom_smooth(method = \"lm\", se = FALSE)\nggpubr::ggarrange(p1,p2,p3, ncol = 3, nrow = 1, labels = \"auto\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Zooming in on panel b of Anscombe's quartet, restricting the range of X](Chapter3_Correlation_files/figure-html/fig-nonlin-1.png){#fig-nonlin fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n\nRestriction of range can easily happen in real life.\nFor example, if your sample only consists of university students, you will probably have restriction of range on IQ.\n\nFinally, you may have heard the phrase **correlation does not imply causation**.\nObserving a strong association between two variables does not mean that one causes the other.\nIn general, it is not possible to conclude causality from statistics:\ncausality is an assumption, which can be either supported by a theory,\nor by a particular methodology.\nIn a randomized controlled experiment, participants are randomly assigned to receive either a treatment or control condition.\nThus, any differences between the two groups should be due to the experimental treatment, or random chance.\nWe will revisit the topic of causality later.\n\nAnscombe's quartet is a good illustration of the limitations of causality, and also demonstrates the value of **visually inspecting your data (including with  scatter plots)** before interpreting any statistics.\n\n## Summary\n\nIn summary, **covariance** offers an initial metric for gauging whether two variables tend to vary in the same or opposite direction. However, because its magnitude depends on the measurement units of the variables involved, it cannot be directly interpreted in terms of strength. The **Pearson correlation coefficient (*r*)** addresses this limitation by standardizing the covariance, yielding a unit-free statistic bounded between –1 and +1. This standardized measure expresses both the **direction** and **strength** of a linear relationship, enabling meaningful comparisons across contexts. Nevertheless, interpreting correlations requires caution, particularly with respect to restricted sampling ranges, the influence of outliers, and the fundamental distinction between correlation and causation.\n\n# Formative Test\n\nThis short quiz checks your grasp of **Chapter 3 – Covariance & Correlation**.  \nWork through it after you’ve studied the lecture slides (and before the next live session) so we can focus on anything that still feels uncertain. Each incorrect answer reveals a hint that sends you back to the exact slide or numerical example you need.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nadd_mcs(\"ch3_formative_corrected.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nquestion <div class='webex-radiogroup' id='radio_HGAVIPDKFI'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HGAVIPDKFI\" value=\"\"></input> <span>B</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HGAVIPDKFI\" value=\"\"></input> <span>A</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HGAVIPDKFI\" value=\"\"></input> <span>C</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HGAVIPDKFI\" value=\"answer\"></input> <span>answer</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HGAVIPDKFI\" value=\"\"></input> <span>D</span></label></div>\n\n\n**Question 2**\n\nA covariance of +120 cm·kg tells you… <div class='webex-radiogroup' id='radio_WHMFBWRCTC'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_WHMFBWRCTC\" value=\"\"></input> <span>The units have been standardised</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_WHMFBWRCTC\" value=\"\"></input> <span>A perfect linear trend</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_WHMFBWRCTC\" value=\"\"></input> <span>120 % of Y variance explained</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_WHMFBWRCTC\" value=\"answer\"></input> <span>X and Y tend to rise together</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_WHMFBWRCTC\" value=\"\"></input> <span>X and Y tend to rise together</span></label></div>\n\n\n**Question 3**\n\nIf the covariance between study hours and stress level is negative, what does that imply? <div class='webex-radiogroup' id='radio_ARHDMEBUVM'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ARHDMEBUVM\" value=\"answer\"></input> <span>Longer study → lower stress</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ARHDMEBUVM\" value=\"\"></input> <span>Longer study → lower stress</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ARHDMEBUVM\" value=\"\"></input> <span>Units are incomparable</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ARHDMEBUVM\" value=\"\"></input> <span>No relationship</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ARHDMEBUVM\" value=\"\"></input> <span>Longer study → higher stress</span></label></div>\n\n\n**Question 4**\n\nWhich statement about covariance magnitude is true? <div class='webex-radiogroup' id='radio_DIJORFVSOM'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DIJORFVSOM\" value=\"\"></input> <span>It ranges only from –1 to +1</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DIJORFVSOM\" value=\"\"></input> <span>It equals the regression slope</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DIJORFVSOM\" value=\"\"></input> <span>A larger value always means a stronger relationship</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DIJORFVSOM\" value=\"\"></input> <span>Its size depends on measurement units</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DIJORFVSOM\" value=\"answer\"></input> <span>Its size depends on measurement units</span></label></div>\n\n\n**Question 5**\n\nConverting temperatures from Celsius to Fahrenheit will make the covariance between temperature and ice-cream sales… <div class='webex-radiogroup' id='radio_MQEFQZIUXO'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MQEFQZIUXO\" value=\"\"></input> <span>Increase by a constant factor</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MQEFQZIUXO\" value=\"\"></input> <span>Switch sign</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MQEFQZIUXO\" value=\"\"></input> <span>Stay exactly the same</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MQEFQZIUXO\" value=\"answer\"></input> <span>Increase by a constant factor</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MQEFQZIUXO\" value=\"\"></input> <span>Become unit‑free</span></label></div>\n\n\n**Question 6**\n\nPearson’s r is best described as… <div class='webex-radiogroup' id='radio_UKIOJNEKQL'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_UKIOJNEKQL\" value=\"\"></input> <span>Mean of X and Y combined</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_UKIOJNEKQL\" value=\"answer\"></input> <span>Standardised (unit-free) covariance</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_UKIOJNEKQL\" value=\"\"></input> <span>Raw measure of joint variability</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_UKIOJNEKQL\" value=\"\"></input> <span>Ratio of two variances</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_UKIOJNEKQL\" value=\"\"></input> <span>Standardised (unit-free) covariance</span></label></div>\n\n\n**Question 7**\n\nIf r = 0, we can conclude that… <div class='webex-radiogroup' id='radio_TADVXYFWNG'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TADVXYFWNG\" value=\"answer\"></input> <span>No linear relationship is present</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TADVXYFWNG\" value=\"\"></input> <span>X causes Y</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TADVXYFWNG\" value=\"\"></input> <span>X and Y are unrelated in every way</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TADVXYFWNG\" value=\"\"></input> <span>The data contain no outliers</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TADVXYFWNG\" value=\"\"></input> <span>No linear relationship is present</span></label></div>\n\n\n**Question 8**\n\nA positive covariance but r ≈ 0.05 usually indicates that… <div class='webex-radiogroup' id='radio_IEZCAGNYKE'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IEZCAGNYKE\" value=\"\"></input> <span>The variables move together only slightly</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IEZCAGNYKE\" value=\"answer\"></input> <span>The variables move together only slightly</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IEZCAGNYKE\" value=\"\"></input> <span>Units have been standardised</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IEZCAGNYKE\" value=\"\"></input> <span>Data range is restricted to zero</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_IEZCAGNYKE\" value=\"\"></input> <span>The relationship is strong</span></label></div>\n\n\n**Question 9**\n\nWhich scatter‑plot feature primarily determines the sign of covariance (and r)? <div class='webex-radiogroup' id='radio_LBZTIQWSEH'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LBZTIQWSEH\" value=\"\"></input> <span>Overall slope direction</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LBZTIQWSEH\" value=\"\"></input> <span>Point density</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LBZTIQWSEH\" value=\"\"></input> <span>Sample size</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LBZTIQWSEH\" value=\"\"></input> <span>Presence of a mode</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LBZTIQWSEH\" value=\"answer\"></input> <span>Overall slope direction</span></label></div>\n\n\n**Question 10**\n\nYou multiply every X score by 10 but leave Y unchanged. What happens? <div class='webex-radiogroup' id='radio_HMQEPDGXSH'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HMQEPDGXSH\" value=\"\"></input> <span>Covariance × 10; r unchanged</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HMQEPDGXSH\" value=\"\"></input> <span>Both covariance and r unchanged</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HMQEPDGXSH\" value=\"\"></input> <span>Covariance unchanged; r × 10</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HMQEPDGXSH\" value=\"\"></input> <span>Covariance × 10; r × 10</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HMQEPDGXSH\" value=\"answer\"></input> <span>Covariance × 10; r unchanged</span></label></div>\n\n\n**Question 11**\n\nA covariance of 0 implies that… <div class='webex-radiogroup' id='radio_TJRTMRLVPV'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TJRTMRLVPV\" value=\"\"></input> <span>X and Y are unrelated in every way</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TJRTMRLVPV\" value=\"answer\"></input> <span>Their linear relationship (r) is 0</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TJRTMRLVPV\" value=\"\"></input> <span>Their linear relationship (r) is 0</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TJRTMRLVPV\" value=\"\"></input> <span>They have opposite scales</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TJRTMRLVPV\" value=\"\"></input> <span>X causes Z</span></label></div>\n\n\n**Question 12**\n\nTwo variables show r = 0.85. Which conclusion is justified? <div class='webex-radiogroup' id='radio_ORROVCURTK'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ORROVCURTK\" value=\"\"></input> <span>X and Y are associated; causal direction unknown</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ORROVCURTK\" value=\"\"></input> <span>Y causes X</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ORROVCURTK\" value=\"answer\"></input> <span>X and Y are associated; causal direction unknown</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ORROVCURTK\" value=\"\"></input> <span>A third variable is impossible</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ORROVCURTK\" value=\"\"></input> <span>X causes Y</span></label></div>\n\n\n**Question 13**\n\nAnalysing data with a restricted range typically makes r… <div class='webex-radiogroup' id='radio_XLKCVYNXFP'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XLKCVYNXFP\" value=\"\"></input> <span>Larger in magnitude</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XLKCVYNXFP\" value=\"\"></input> <span>Smaller in magnitude</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XLKCVYNXFP\" value=\"\"></input> <span>Exactly zero</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XLKCVYNXFP\" value=\"\"></input> <span>Change sign</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XLKCVYNXFP\" value=\"answer\"></input> <span>Smaller in magnitude</span></label></div>\n\n\n**Question 14**\n\nAn extreme outlier that follows the overall trend will most likely… <div class='webex-radiogroup' id='radio_PFXFMKRLCS'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_PFXFMKRLCS\" value=\"answer\"></input> <span>Inflate the magnitude of r</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_PFXFMKRLCS\" value=\"\"></input> <span>Inflate the magnitude of r</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_PFXFMKRLCS\" value=\"\"></input> <span>Drive r toward zero</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_PFXFMKRLCS\" value=\"\"></input> <span>Remove measurement error</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_PFXFMKRLCS\" value=\"\"></input> <span>Make covariance negative</span></label></div>\n\n\n**Question 15**\n\nA coefficient of determination (r²) of 0.49 means that… <div class='webex-radiogroup' id='radio_HWDADJCQNI'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HWDADJCQNI\" value=\"\"></input> <span>49 % of Y variance is explained by X</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HWDADJCQNI\" value=\"\"></input> <span>49 % of X variance is explained by Y</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HWDADJCQNI\" value=\"\"></input> <span>The correlation is −0.70</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HWDADJCQNI\" value=\"answer\"></input> <span>49 % of Y variance is explained by X</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HWDADJCQNI\" value=\"\"></input> <span>Covariance is unit‑free</span></label></div>\n\n\n**Question 16**\n\nAfter converting both X and Y to z‑scores, the covariance of those z‑variables equals… <div class='webex-radiogroup' id='radio_ZKRKUCTKUW'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ZKRKUCTKUW\" value=\"answer\"></input> <span>Pearson’s r</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ZKRKUCTKUW\" value=\"\"></input> <span>Pearson’s r</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ZKRKUCTKUW\" value=\"\"></input> <span>Their geometric mean</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ZKRKUCTKUW\" value=\"\"></input> <span>Always zero</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ZKRKUCTKUW\" value=\"\"></input> <span>Sample size</span></label></div>\n\n\n:::\n\n\n<div class='webex-solution'><button>Show explanations</button>\n**Question 1**\n\nexplanation\n\n**Question 2**\n\nPositive sign = same-direction movement; magnitude is unit-dependent so strength not directly interpretable.\n\n**Question 3**\n\nNegative covariance means high X pairs with low Y and vice‑versa.\n\n**Question 4**\n\nRescaling either variable rescales covariance; therefore magnitude alone is not comparable across units.\n\n**Question 5**\n\nMultiplying Celsius by 1.8 and adding 32 rescales covariance by 1.8; adding a constant does not affect it.\n\n**Question 6**\n\nDividing covariance by the product of SDs removes units and bounds the result between –1 and +1.\n\n**Question 7**\n\nr only detects linear association; other patterns may still exist.\n\n**Question 8**\n\nSmall r means weak linear association despite positive sign.\n\n**Question 9**\n\nPositive slope → positive sign; negative slope → negative sign.\n\n**Question 10**\n\nScaling one variable scales covariance by that factor but leaves r (unit‑free) unchanged.\n\n**Question 11**\n\nZero covariance means no linear co‑movement; nonlinear links could still exist.\n\n**Question 12**\n\nCorrelation quantifies association but cannot establish causality without experimental control.\n\n**Question 13**\n\nLess variability reduces covariance relative to the SDs, shrinking r.\n\n**Question 14**\n\nTrend‑consistent outliers add leverage, increasing |r|.\n\n**Question 15**\n\nr² translates correlation into variance‑explained terms.\n\n**Question 16**\n\nStandardising divides by SDs, so covariance in z‑space equals r.\n\n\n</div>\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n# Tutorial\n\n## Load Data\n\nOpen **`LAS_SocSc_DataLab2.sav`** (find it in the **`data`** folder you downloaded earlier).  \nThe file contains six variables (`X1` … `X6`). You’ll inspect three bivariate relationships.\n\n### Plot the pairs\n\nGenerate three simple scatterplots:\n\n1. `Graphs` › `Legacy Dialogs` › `Scatter/Dot` ► **Simple Scatter**  \n2. Pairings & axis order  \n   * `X1` (X-axis) vs `X2` (Y-axis)  \n   * `X3` (X-axis) vs `X4` (Y-axis)  \n   * `X5` (X-axis) vs `X6` (Y-axis)  \n3. *Paste* and *Run* each syntax block.\n\nDescribe **linearity**, **direction**, and **strength** for each plot.\n\n::: {.webex-check .webex-box}\n\n“The relationship between `X1` and `X2` is positive.” <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value='NA'>FALSE</option></select>\n\n“The relationship between `X5` and `X6` is positive.” <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select>\n\n“The relationship between `X1` and `X2` is linear.” <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value='NA'>FALSE</option></select>\n\n“The relationship between `X3` and `X4` is linear.” <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select>\n\nStrength of `X1`–`X2`:  \n<select class='webex-select'><option value='blank'></option><option value='answer'>moderate</option><option value=''>strong</option><option value=''>weak</option><option value=''>zero</option></select>\n\nStrength of `X3`–`X4`:  \n<select class='webex-select'><option value='blank'></option><option value='answer'>strong</option><option value=''>weak</option><option value=''>zero</option><option value=''>moderate</option></select>\n\nStrength of `X5`–`X6`:  \n<select class='webex-select'><option value='blank'></option><option value=''>strong</option><option value=''>weak</option><option value=''>zero</option><option value='answer'>moderate</option></select>\n\n:::\n\n\n\n### Correlation coefficients\n\nEven when the pattern is non-linear it’s useful to see why Pearson *r* can mislead.\n\n*Analyze* › *Correlate* › *Bivariate*  \nSelect all six variables → **OK**.\n\n::: {.webex-check .webex-box}\n\n`X1`–`X2` correlation: <input class='webex-solveme nospaces' data-tol='0.01' size='3' data-answer='[\"0.5\",\".5\"]'/>\n\n`X2`–`X6` correlation: <input class='webex-solveme nospaces' data-tol='0.01' size='4' data-answer='[\"0.06\",\".06\"]'/>\n\n`X3`–`X4` correlation: <input class='webex-solveme nospaces' data-tol='0.01' size='4' data-answer='[\"-0.8\",\"-.8\"]'/>\n\nCan we interpret `X3`–`X4`’s *r* at face value?  \n<select class='webex-select'><option value='blank'></option><option value=''>No, assumption of association violated</option><option value='answer'>No, assumption of linearity violated</option><option value=''>No, assumption of normality violated</option><option value=''>Yes, otherwise SPSS would give an error</option></select>\n\nInterpret `X5`–`X6`:  \n<select class='webex-select'><option value='blank'></option><option value=''>Weak negative</option><option value='answer'>Moderate negative</option><option value=''>Weak positive</option><option value=''>Moderate positive</option></select>\n\n:::\n\n*Take-away:* Pearson’s *r* is good at detecting linear patterns (like X1–X2), but it may be close to zero even when the variables have a strong curved pattern (like X3–X4).\n\n## Correlation – Work Dataset (`Work.sav`)\n\nHaving practiced on simulated data, let’s now apply the same workflow to a real dataset related to the workplace.\n\n> **File location:** `data/Work.sav`\n\n### Why inspect the plot first?\n\nBefore trusting Pearson *r* we check for\n\n* an **approximately linear** pattern, and  \n* **extreme values** that could distort the statistic.\n\n::: {.webex-check .webex-box}\n\nSelect the correct reason:\n\n<select class='webex-select'><option value='blank'></option><option value='answer'>To check if the relationship is linear</option><option value=''>To check if the relationship is positive</option><option value=''>To check if the relationship is strong enough</option></select>\n\n:::\n\n### Create the scatter-plot\n\n`Graphs` › `Legacy Dialogs` › `Scatter/Dot` → **Simple Scatter**\n\n* X-axis =`scmental` (Mental Pressure)  \n* Y-axis =`scemoti`  (Emotional Pressure)\n\n*Paste* and *Run*.\n\n::: {.webex-check .webex-box}\n\nThe cloud of data points is roughly **linear**: <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value='NA'>FALSE</option></select>\n\nThere are obvious **outliers**: <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select>\n\nApproximate strength:  \n\n<select class='webex-select'><option value='blank'></option><option value='answer'>Moderately strong and positive</option><option value=''>Moderately weak and positive</option><option value=''>Moderately strong and negative</option><option value=''>Moderately weak and negative</option></select>\n\n:::\n\n### Compute Pearson *r*\n\n`Analyze` › `Correlate` › `Bivariate` → (`scmental`, `scemoti`) → **OK**\n\nThe correlation coefficient is (2 decimals): <input class='webex-solveme nospaces' data-tol='0.01' size='4' data-answer='[\"0.54\",\".54\"]'/>\n\nInterpretation:\n\n<select class='webex-select'><option value='blank'></option><option value='answer'>There is a relationship between mental and emotional pressure.</option><option value=''>There is no relationship between mental and emotional pressure.</option><option value=''>We cannot draw a conclusion on whether or not there is a relationship.</option></select>\n\n*Take-away:* Mental and emotional pressure show a **moderately strong, significant positive relationship**—employees who feel more mentally pressured also tend to feel more emotionally pressured.\n",
    "supporting": [
      "Chapter3_Correlation_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}