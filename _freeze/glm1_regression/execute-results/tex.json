{
  "hash": "da6109610db742814d767029336b05cd",
  "result": {
    "engine": "knitr",
    "markdown": "# GLM-I: Linear Regression {#sec-glm1}\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe General Linear Model (GLM) is a family of models used to analyze the relationship between an outcome variable and one or more predictors. In this lecture, we will focus on bivariate linear regression, which describes a linear relationship between a continuous outcome variable and a continuous predictor. However, it's important to note that the GLM encompasses other members that can handle predictors of any measurement level (continuous or categorical), multiple predictors, transformations of the outcome and predictors, and different error distributions.\n\nLinear regression is based on the concept of using information about other variables associated with the outcome to improve predictions. It begins with the understanding that the mean is the best predictor (expected value) when no further relevant information is available. However, if we have information about other variables, such as the number of hours studied being strongly associated with exam grades, we can use that information to enhance our predictions. This process is known as regression.\n\nTo visually explore associations between two variables, we often use scatterplots. Scatterplots require both variables to be at least of ordinal measurement level. By plotting the data points, we can observe whether there is a linear pattern or trend. In linear regression, we aim to find a line that represents the best possible predictions. This line, called the regression line, goes through the middle of the cloud of data points.\n\nThe regression line is described by the formula Y = a + bX, where \"a\" is the intercept (the predicted value when X equals 0) and \"b\" is the slope (how steeply the line increases or decreases). The predictions made using the regression line are not identical to the observed values, as there is always some prediction error. The Ordinary Least Squares method is used to obtain the line that minimizes the sum of squared prediction errors.\n\nIn a bivariate regression, the regression formula expands to include the individual prediction error, assuming that the errors are normally distributed around the regression line with a mean of zero. The regression model is represented as Yi = a + b * Xi + ei, where Yi is the individual's score on the dependent variable, a is the intercept, b is the slope, Xi is the individual's score on the independent variable, and ei is the individual prediction error.\n\nHypothesis tests can be conducted on the regression coefficients to determine their significance. The default null hypothesis for the intercept is that it is equal to zero, while the null hypothesis for the slope is also zero. The t-test is commonly used, with the degrees of freedom being n - p, where n is the sample size and p is the number of parameters. By testing the coefficients, we can determine the statistical significance of the relationship between the predictor and the outcome.\n\nWhile linear regression offers valuable insights, it is essential to consider the assumptions underlying the model. These assumptions include linearity of the relationship between the predictor and the outcome, normality of residuals (prediction errors), homoscedasticity (equal variance of residuals), and independence of observations. Violations of these assumptions can affect the validity of the model and lead to misleading results. Checking and addressing these assumptions is crucial for accurate and reliable regression analysis.\n\nLinear regression is a powerful tool for analyzing the relationship between variables,\nand a building block for many more advanced analysis techniques.\nIt allows us to make predictions based on available information and understand the strength and significance of the relationship between a continuous predictor and continuous outcome.\nBy considering the assumptions and conducting hypothesis tests, we can ensure the validity of our regression models and draw meaningful conclusions from the analysis.\n\n# Lecture\n\n\n\n\n\n\n{{< video https://www.youtube.com/embed/Mkc17DG4KdI >}}\n\n\n\n\n\n\n\n\n\n\n# Formative Test\n\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\n\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nWhat is the General Linear Model (GLM)? ^[A family of models to analyze the relationship between one outcome and one or more predictors]\n\n* (A) A family of models to analyze the relationship between categorical outcomes and continuous predictors  \n* (B) A family of models to analyze the relationship between continuous outcomes and categorical predictors  \n* (C) A family of models to analyze the relationship between multiple outcomes and multiple predictors  \n* (D) A family of models to analyze the relationship between one outcome and one or more predictors  \n\n\n\n**Question 2**\n\nWhat type of relationship does bivariate linear regression describe? ^[A linear relationship between a continuous outcome variable and a predictor of any measurement level with normally distributed prediction errors]\n\n* (A) A linear relationship between a categorical outcome variable and a continuous predictor  \n* (B) A linear relationship between a continuous outcome variable and a predictor of any measurement level with normally distributed prediction errors  \n* (C) A nonlinear relationship between a continuous outcome variable and a continuous predictor  \n* (D) A relationship of any shape between a continuous outcome variable and a predictor of any measurement level, with normally distributed prediction errors  \n\n\n\n**Question 3**\n\nWhat does it mean when we say 'The mean is the best predictor when there's no further relevant information' in the context of regression? ^[The mean is the expected value when we have no additional information about predictors]\n\n* (A) The mean is only a good predictor when there&apos;s no variability in the outcome  \n* (B) The mean is the best predictor regardless of whether we have additional information about predictors  \n* (C) The mean is the expected value when we have no additional information about predictors  \n* (D) The mean is the least accurate predictor when we have no additional information about predictors  \n\n\n\n**Question 4**\n\nWhat is the purpose of a scatterplot in the context of regression analysis? ^[To visualize associations between two variables]\n\n* (A) To explore causal relationships between two variables  \n* (B) To calculate the mean and standard deviation of two variables  \n* (C) To visualize associations between two variables  \n* (D) To determine the distribution of two variables  \n\n\n\n**Question 5**\n\nWhat is the primary goal of ordinary least squares regression in linear modeling? ^[To find the line that gives the best possible predictions by minimizing the sum of squared prediction errors]\n\n* (A) To find the line that predicts the maximum number of data points correctly  \n* (B) To find the line that gives the best possible predictions by minimizing the sum of squared prediction errors  \n* (C) To find the line that passes through the mean of the data points  \n* (D) To find the line that fits the data exactly by minimizing the sum of absolute prediction errors  \n\n\n\n**Question 6**\n\nIn the formula 'Yi = a + bXi + ei', what are the parameters?, ^[a and b]\n\n* (A) X and Y  \n* (B) Xi and ei  \n* (C) a and b  \n* (D) Yi, Xi, and ei  \n\n\n\n**Question 7**\n\nHow are the coefficients 'a' and 'b' interpreted in the context of linear regression? ^['a' is the intercept where the line crosses the Y-axis, and 'b' is the slope indicating how steeply the line increases or decreases]\n\n* (A) &apos;a&apos; is the intercept where the line crosses the Y-axis, and &apos;b&apos; is the predicted value when X equals 0  \n* (B) &apos;a&apos; is the intercept where the line crosses the Y-axis, and &apos;b&apos; is the slope indicating how steeply the line increases or decreases  \n* (C) &apos;a&apos; is the slope indicating how steeply the line increases or decreases, and &apos;b&apos; is the intercept where the line crosses the Y-axis  \n* (D) &apos;a&apos; is the predicted value when b equals 0, and &apos;b&apos; is the slope indicating how steeply the line increases or decreases  \n\n\n\n**Question 8**\n\nWhat is the purpose of checking assumptions in linear regression? ^[To ensure that the model accurately represents the data and that inferences are valid]\n\n* (A) To find ways to manipulate the data to fit the model better  \n* (B) To determine the significance of the predictors  \n* (C) To improve the visualization of the scatterplot  \n* (D) To ensure that the model accurately represents the data and that inferences are valid  \n\n\n\n**Question 9**\n\nWhat is the assumption of homoscedasticity? ^[That prediction errors are equally distributed for all values of the predictor]\n\n* (A) That the prediction errors are normally distributed  \n* (B) That prediction errors are equally distributed for all values of the predictor  \n* (C) That the effect of the predictor on the outcome is linear and monotonous  \n* (D) That the dependent variable is normally distributed  \n\n\n\n**Question 10**\n\nGiven regression formula Yi = 65.13 + 95.27*Xi+ei, what is the predicted score for a person who scores 15 on X? ^[1494.18]\n\n* (A) Can&apos;t say  \n* (B) 1494.18  \n* (C) 1072.22  \n* (D) 80.13  \n\n\n\n**Question 11**\n\nFrank scores 22 on Yi and has a prediction error of 7.33. What was his predicted value, given regression formula Yi = 65.13 + 95.27*Xi+ei? ^[14.67]\n\n* (A) 22  \n* (B) 14.67  \n* (C) 27.33  \n* (D) -0.53  \n\n\n\n:::\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Show explanations]\n\n```\n**Question 1**\n\nThe GLM is used to analyze the relationship between a single outcome and one or more predictors.\n\n**Question 2**\n\nBivariate linear regression specifically describes a linear relationship between two continuous variables.\n\n**Question 3**\n\nWhen there's no further information available, the mean is the most reasonable estimate for the outcome.\n\n**Question 4**\n\nScatterplots visually depict the relationships and associations between two variables.\n\n**Question 5**\n\nOrdinary least squares regression aims to minimize the sum of squared prediction errors to find the best-fitting line.\n\n**Question 6**\n\n The parameters of a model are the quantities estimated from data. Yi and Xi are the data; ei is calculated based on the model-implied predictions.\n\n**Question 7**\n\nThe coefficient 'a' represents the intercept, and the coefficient 'b' represents the slope of the regression line.\n\n**Question 8**\n\nAssumption checks ensure that the model accurately represents the data and that any inferences drawn from the model are valid.\n\n**Question 9**\n\n Homoscedasticity literally means: equal variances; this assumption means that the variance of prediction errors is equal at all values of the predictor.\n\n**Question 10**\n\n 65.13 + 95.27*15 = 1494.18\n\n**Question 11**\n\n The observed score Yi is equal to the predicted score plus prediction error. If prediction error was 7.33, the predicted score must have been 22-7.33 = 14.67\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n:::\n\n\n\n\n\n\n\n# In SPSS\n\n## Linear Regression\n\n\n\n\n\n\n{{< video https://youtu.be/0AGLdgUtIJg?si=SZQxa2Qt9oOTEg-O >}}\n\n\n{{< video https://www.youtube.com/watch?v=VEQPX6d-EQw >}}\n\n\n\n\n\n\n\n\n\n\n\n# Tutorial\n\n## Regression Analysis\n\nIn this assignment we will make a start with regression analysis.\n\nWe will go through the different steps of running and interpreting a regression analysis.\n\nOpen the file [`Work.sav`](data/Work.sav) to get started.\n\n\nConsider the following research question: “Does variety at work predict pleasure at work?”\n\n\nWhat is the dependent variable in this case? ^[Pleasure]\n\n* (A) Pleasure  \n* (B) Variety  \n\n\n\n\nTo answer the research question, we will run a linear regression analysis.\n\nSelect the following menu item: Analyze > Regression > Linear\n\nChoose the dependent variable (scpleasure) and independent variable (scvariety).\nPaste and run the syntax.\n\nIf you look in the output, you will see that SPSS shows four tables in the output file.\n\nIn the table labeled \"Model Summary\" we can find the R2 value. R2 indicates the total proportion of explained variance in the dependent variable in the model; this is the focus of next week's class.\n\nWhat proportion of the variance Emotional pressure (scpleasure) is explained by our single predictor Variety at work (scvariety)? _____^[0\\.195]\n\nConsider the unstandardized Coefficients in the table labeled \"Coefficients\".\n\nWhat is the value of the intercept (b0) for the regression line? ______^[-9\\.024]\n\nHow should we interpret the intercept (or \"constant\") within the context of this analysis?\n\n^[Someone who reports zero Variety at work (meaning a score of 0 on scvariety) has an expected value of this many points on Pleasure.]\n\n* (A) Someone who reports zero Variety at work (meaning a score of 0 on scvariety) has an expected value of this many points on Pleasure.  \n* (B) For every point in Variety at work, we expect an increase of this many points in Pleasure.  \n* (C) Everyone who reports zero Variety at work (meaning a score of 0 on scvariety) has a value of this many points on Pleasure.  \n* (D) The sample average of Pleasure is this many points  \n\n\n\nConsider the unstandardized regression coefficients again.\n\nWhat is the value of the regression coefficient of scpleasure on scemoti (b1)? _____^[0\\.618]\n\nHow should we interpret the regression coefficient of scvariety within the context of this analysis?\n\n^[If someone’s score on Variety at work increases with 1 point, their score on Pleasure increases by this many  points.]\n\n* (A) This is the sample average score of Variety at work.  \n* (B) This is the sample average of Pleasure  \n* (C) If someone’s score on Variety at work increases with 1 SD, their score on Pleasure increases by this many  SDs.  \n* (D) If someone’s score on Variety at work increases with 1 point, their score on Pleasure increases by this many  points.  \n\n\n\nThe \"Coefficients\" table also shows whether or not the effect of scvariety on scpleasure is significant.\n\nWhat is the p-value for the regression coefficient for scvariety? _^[0]\n \nCan we conclude that the effect of scvariety on scpleasure is significant? (use $\\alpha$ = .05). ^[Yes]\n\n* (A) Yes  \n* (B) No  \n\n\n\n\n## Assumptions\n\nRecall that regression assumes linearity, normality of residuals, homoscedasticity (equal variance of residuals), and independence of observations. We will check each of these assumptions in turn, except for independence of observations because this is a property of our sampling method and cannot be checked statistically.\n\n\n### Scatterplot\n\nA scatter plot can provide some insight into linearity.\n\nTo make a scatter plot: Graphs > Legacy Dialogs > Scatter/Dot > Simple Scatter\n\nPlace variety at work on the X axis and emotional pressure on the Y axis.\n\nIs the assumption of linearity met in this case?  ^[Yes]\n\n* (A) Yes  \n* (B) No  \n\n\n\n### Regression Diagnostics\n\nAside from the scatterplot, we can check the assumptions of regression by requesting additional options in the analysis.\n\nGo back to the analysis dialog via Analyze –> Regression –> Linear. Verify that you still have the correct predictor and outcome. \n\nThen, click the Plots button. You want a plot of the predicted values against the residual values, so put ZPRED in the X box and ZRESID in the Y box.\n\nAlso check the boxes for a Histogram and normal probability plot, then hit continue.\n\nNow paste and run the syntax. You should see the following added to your previous regression syntax:\n\n```\n  /SCATTERPLOT=(*ZRESID ,*ZPRED)\n  /RESIDUALS HISTOGRAM(ZRESID) NORMPROB(ZRESID)\n```\n\n### Linearity\n\nHow can we test linearity using this additional output?\n\nFirst, we can use the \"Normal P-P plot\". If the relationship is perfectly linear, all dots should be on the diagonal line. If the points are deviating from the line, the relationship is not perfectly linear. Small deviations are OK; for example, the plot below shows a linear association:\n\n![](images/assumptions4.png)\n\nDoes the P-P plot for your regression give cause for concern for violation of the assumption of linearity? ^[No]\n\n* (A) Yes  \n* (B) No  \n* (C) Unclear  \n\n  \n\n### Normality\n\nOne way to check normality is by examining the histogram of residuals. This histogram displays a normal curve by default. If the observed residuals deviate strongly from this histogram, there may be a problem.\n\nThe plot below shows a residual histogram with some minor deviations from normality (too few scores near the mean). This is probably still fine:\n\n![](images/assumptions5.png)\n\nDoes the residual histogram for your regression give cause for concern for violation of the assumption of normal residuals? ^[Yes]\n\n* (A) Yes  \n* (B) No  \n* (C) Unclear  \n\n\n\n### Homoscedasticity\n\nWe examine homoscedasticity using a plot of standardized predicted values against standardized residuals. We want residuals to be identically distributed on the Y-axis for all values on the X-axs. \nIn other words, this scatterplot should look like a dot cloud (no pattern) around the zero line (left picture below), and not like a pattern (right picture below).\n\n![](images/residuals2.png)\n\nDoes the scatterplot for standardized predicted values against residual values for your regression give cause for concern for violation of the assumption of homoscedasticity? ^[Yes]\n\n* (A) Yes  \n* (B) No  \n* (C) Unclear  \n\n\n\nWrite up a discussion of potential violations of the assumptions for your regression, then check your answer.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Explanation]\n\n```\n\nWe observed that the observed scores deviated from the P-P plot in an S-shaped pattern. We further observed that, in a histogram of standardized residuals, the observed residuals were right-skewed. Finally, we observed less variance around the regression line for low scores and more variance around the regression line for high scores.\n\nThese findings give cause for concern of violations of the assumptions of regression. *One potential explanation is that the effect might be quadratic instead of linear. (Optional)*\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n",
    "supporting": [
      "glm1_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}