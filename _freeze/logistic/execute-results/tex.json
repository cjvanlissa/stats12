{
  "hash": "849b4874f903b808c6acee939735eca3",
  "result": {
    "engine": "knitr",
    "markdown": "# GLM VIII - Logistic Regression {#sec-logistic}\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen dealing with binary dependent variables (nominal or ordinal),\nwe could model the probability of observing the outcome (coded as 0 or 1) with a conventional linear regression model. The problem with this approach is that linear regression predicts probabilities outside the range of [0, 1],\nand will have heteroscedastic and non-normal residuals because there are only two discrete values for the dependent variable.\n\n**Logistic Regression** overcomes these limitations of linear regression.\nThe core idea behind logistic regression is to predict a transformation of the dependent variable, Y, rather than the raw scores. Specifically, we model the **log odds** of the probability of Y being one category (e.g., 1) versus the other category (e.g., 0). The logit function, denoted as `log(p/(1-p))`, models the probability p using an s-shaped curve bounded by 0 and 1.\nFurthermore, logistic regression assumes a Bernoulli error distribution, instead of a normal error distribution, which accounts for the fact that observed outcomes can only take the values 0 or 1.\n\n### Introducing the logit\n\nLogistic regression predicts the **logit function** of the individual probabilities of observing the outcome, pi. The logit of the probability (pi) is given by `log(pi/(1-pi))`, ensuring that the predicted values remain within the valid probability range. The outcome, Yi, is assumed to follow a Bernoulli distribution with an individual probability of success, pi. The logit of this success probability is modeled as a linear function of the predictors, allowing us to use the familiar linear regression model to predict the logit of pi.\n\nUnderstanding the distinction between **probability**, **odds**, and the **logit** is crucial in logistic regression. Probability is defined as the long-run proportion of outcomes of a random experiment in which a particular outcome is observed.\nOdds describe the ratio of the probability of an event occurring relative to the probability of it not occurring. Finally, the logit transforms odds into a linear function, enabling us to use the regression model. The transformations from probability to odds and logit, and back, are given by:\n\nOperation | Formula\n----------|---------\nProbability to odds | $\\text{odds}= \\frac{P}{1-P}$\nOdds to probability  | $P = \\frac{\\text{odds}}{1+\\text{odds}}$\nOdds to logit | \t\t$\\text{logit} = \\ln(odds)$\nLogit to odds | $\\text{odds} = e^{\\text{logit}}$\nProbability to logit | $\\text{logit} = \\ln(\\frac{p}{1-p})$\nLogit to probability  | $p = \\frac{e^{logit}}{1+e^{logit}}$\n\n## Maximum Likelihood Estimation (MLE)\n\nIn traditional linear regression, we use \"ordinary least squares\" (OLS) estimation to obtain the model parameters. This method involves simple matrix algebra and always yields a unique solution. However, for logistic regression, there is no OLS solution due to the binary nature of the dependent variable. Instead, we turn to **Maximum Likelihood Estimation (MLE)**. A complete explanation is beyond the scope of this course, but here is a basic intuitive explanation of the procedure: \n\n1. Start with random values for the coefficients (a and b)\n2. Using those parameter values, calculate the individual probabilities predicted by the logistic regression formula\n3. For each individual, calculate the likelihood of observing their true outcome in a Bernoulli distribution with model-implied probability pi\n4. Multiply these probabilities across all individuals to get the overall likelihood of observing these data, given the chosen coefficient values\n5. Adjust the values of a and b a little bit\n6. Check if the likelihood has become larger\n7. Repeat steps 2-6 until until we find the coefficient values that maximize the likelihood and no further improvement can be found.\n\nIn other words, we look for the values of a and b that maximize the likelihood  of observing the observed outcome values.\n\n### Interpreting Coefficients\n\nThe **intercept (a)** represents the log odds of the outcome (Y) for someone who scores 0 on all predictors. We can convert this log odds to the probability of the outcome for an individual who scores 0 on all predictors using the formula `P = e^(a) / (1 + e^(a))`. We can also solve for the inflection point at which the model stops predicting 0 and starts predicting 1, or vice versa, using $X_{p = .5} = \\frac{-a}{b}$.\n\nThe **slope (b)** of the logistic regression equation determines how steeply the logistic function switches from predicting 0 to predicting 1 as the predictor variable (Xi) increases. Larger absolute values of b indicate a steeper transition between the two outcomes. If the slope is positive, the function ascends (starts at 0 and goes to 1), resulting in an S-shaped curve. Conversely, if the slope is negative, the function descends (starts at 1 and goes to 0), resulting in a Z-shaped curve.\n\n### Odds Ratio\n\nThe **odds ratio** is another important concept when interpreting logistic regression coefficients. It represents the odds of the outcome occurring given a one-unit increase in the predictor variable (Xi), relative to the odds of the outcome occurring when Xi remains unchanged. For binary predictors (e.g., conditions), the odds ratio provides a sensible effect size. For continuous predictors, the odds ratio is a multiplyer by which the odds increase when the predictor increases by one unit.\n\nTo calculate the odds ratio for a logistic regression coefficient (b), we use the formula `OR = e^(b)`. A value greater than 1 indicates that the predictor is associated with higher odds of the outcome, while a value less than 1 indicates lower odds of the outcome. For example, if the odds ratio for the test score coefficient (b = 2.12) is 8.35, it means that for each unit increase in the test score, the odds of the outcome are multiplied by 8.35.\n\n\n### Model Fit\n\nTo assess how well the logistic regression model fits the data, we can use the likelihood obtained from maximum likelihood estimation (MLE). By multiplying the log likelihood by -2, we obtain the $-2LL$, which is a chi-square distributed test statistic. Performing a chi-square test allows us to determine if the overall model is significant. The null hypothesis is that the model does not significantly differ from a model with no predictors. If the chi-square test is significant, it indicates that the model provides a better fit than a null model.\n\n### Likelihood Ratio Test\n\nIn logistic regression, we can also conduct a **Likelihood Ratio (LR) test**, which is a chi-square test for the difference in log likelihood between two nested models, $-2LL_0 - -2LL_1$. The first model is the restricted model with fewer parameters, and the second is the full model with more parameters. The LR test helps us compare the two models and determine if the additional predictors in the full model significantly improve its fit. The degrees of freedom for the LR test are equal to the difference in the number of parameters between the two models.\n\n### Pseudo R2\n\nUnlike linear regression, logistic regression doesn't have a traditional R-squared to measure explained variance. However, researchers have proposed several **Pseudo R2** statistics to approximate the concept of explained variance in logistic regression. These Pseudo R2 statistics rescale the -2 log likelihood of the model. Two common Pseudo R2 statistics are Cox & Snell and Nagelkerke.\n\nCox & Snell is a generalization of the \"normal\" R2, which provides the same value for ordinary least squares regression. For logistic regression, however, its value can never reach 1; it will be somewhere between 0 and < 1. Nagelkerke aims to \"fix\" this property by rescaling Cox & Snell to a range of [0, 1], by dividing it by its maximum possible value. While these statistics can provide a measure of relative model fit and help compare models on the same dataset, they do not represent absolute model fit or effect size.\n\n### Classification Accuracy\n\nOne way to evaluate the model's predictive performance is by using a **classification table**. The classification table compares the predicted outcomes with the actual outcomes to determine how well the model predicts true positives and true negatives. The table is constructed by calculating the predicted probability for each individual and then dichotomizing these probabilities using a specific cutoff, typically 0.5. Individuals with predicted probabilities above the cutoff are classified as \"1,\" and those below as \"0.\" The observed outcomes are then cross-tabulated against the dichotomized predictions.\n\nBy examining the classification table, we can assess the accuracy of the model's predictions and identify areas of improvement. Researchers could choose a different cutoff to optimize the trade-off between false positives and false negatives, depending on the specific goals of the analysis.\n\nIn summary, evaluating logistic regression models involves assessing model fit through chi-square tests, using Pseudo R2 statistics for relative fit comparison, and examining classification accuracy to understand the model's predictive performance.\n\n## Lecture\n\n\n\n## Formative Test\n\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\n\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nWhat type of dependent variable is suitable for logistic regression? ^[Binary]\n\n* (A) Ordinal  \n* (B) Binary  \n* (C) Interval  \n* (D) Categorical  \n\n\n\n**Question 2**\n\nWhat is the primary purpose of maximum likelihood estimation in logistic regression? ^[To maximize the likelihood of observing the data given the model]\n\n* (A) To minimize the sum of squared errors  \n* (B) To maximize the parameter values  \n* (C) To maximize the likelihood of observing the data given the model  \n* (D) To maximize the variance explained by the predictors  \n\n\n\n**Question 3**\n\nHow is the likelihood ratio test used in logistic regression? ^[To compare the fit of nested models]\n\n* (A) To test for normality of residuals  \n* (B) To compare the fit of nested models  \n* (C) To check for multicollinearity  \n* (D) To assess the effect size of predictors  \n\n\n\n**Question 4**\n\nWhich of the following is a Pseudo R2 statistic commonly used in logistic regression? ^[Cox & Snell R2]\n\n* (A) Cox & Snell R2  \n* (B) Pearson&apos;s R2  \n* (C) Adjusted R2  \n* (D) -2LL  \n\n\n\n**Question 5**\n\nWhat does the logit function do in logistic regression? ^[Transforms the predicted probabilities to log odds]\n\n* (A) Transforms the predicted probabilities to log odds  \n* (B) Standardizes the dependent variable  \n* (C) Calculates the Wald test statistic  \n* (D) Converts continuous predictors to categorical  \n\n\n\n**Question 6**\n\nWhat is the range of the Cox & Snell Pseudo R2 statistic in logistic regression? ^[0 to < 1]\n\n* (A) -Infinite to 1  \n* (B) 0 to < 1  \n* (C) 0 to Infinite  \n* (D) 0 to 1  \n\n\n\n**Question 7**\n\nHow is the classification table used in logistic regression evaluation? ^[To assess the model's predictive accuracy]\n\n* (A) To assess the model&apos;s predictive accuracy  \n* (B) To compare model fit using likelihood ratio test  \n* (C) To determine effect size of predictors  \n* (D) To test for multicollinearity  \n\n\n\n**Question 8**\n\nWhat does a significant chi-square test in logistic regression indicate? ^[The model provides a better fit than a null model]\n\n* (A) The model provides a better fit than a null model  \n* (B) The model&apos;s predicted probabilities are accurate  \n* (C) The model&apos;s predictors are collinear  \n* (D) The model&apos;s residuals are normally distributed  \n\n\n\n**Question 9**\n\nWhich parameter represents the odds ratio associated with a one-unit increase in the predictor? ^[The exponent of the coefficient b, e^b]\n\n* (A) The logit function  \n* (B) The coefficient b  \n* (C) The p-value  \n* (D) The exponent of the coefficient b, e^b  \n\n\n\n**Question 10**\n\nWhen calculating the likelihood in logistic regression, what does a high value of likelihood imply? ^[The observed outcome values are very likely given the parameters]\n\n* (A) The model has a high R-squared value  \n* (B) The observed outcome values are very likely given the parameters  \n* (C) The model is overfitting the data  \n* (D) The model&apos;s residuals are normally distributed  \n\n\n\n**Question 11**\n\nWhat is the main purpose of the likelihood ratio test in logistic regression? ^[To compare model fit between two nested models]\n\n* (A) To assess the normality of residuals  \n* (B) To test the significance of individual predictors  \n* (C) To evaluate multicollinearity among predictors  \n* (D) To compare model fit between two nested models  \n\n\n\n**Question 12**\n\nWhat can help researchers optimize the trade-off between false positives and false negatives in logistic regression? ^[The classification table]\n\n* (A) The odds ratio  \n* (B) The -2LL  \n* (C) The pseudo R2  \n* (D) The classification table  \n\n\n\n**Question 13**\n\nWhat is the range of the Nagelkerke Pseudo R2 statistic in logistic regression? ^[0 to 1]\n\n* (A) -1 to 1  \n* (B) 0 to < 1  \n* (C) 0 to 1  \n* (D) -Infinity to 1  \n\n\n\n:::\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Show explanations]\n\n```\n**Question 1**\n\n Logistic regression is used for binary categorical outcomes.\n\n**Question 2**\n\n Maximum likelihood estimation aims to maximize the likelihood of observing the data given the model.\n\n**Question 3**\n\n The likelihood ratio test compares the fit of nested models to determine if additional predictors significantly improve the model.\n\n**Question 4**\n\n Cox & Snell R2 is a Pseudo R2 statistic used in logistic regression.\n\n**Question 5**\n\n The logit function transforms the predicted probabilities to odds.\n\n**Question 6**\n\n The Cox & Snell Pseudo R2 statistic never reaches 1 for logistic regression, so it ranges from 0 to < 1.\n\n**Question 7**\n\n The classification table helps assess the model's predictive accuracy by comparing predicted outcomes with actual outcomes.\n\n**Question 8**\n\n A significant chi-square test indicates that the model provides a better fit than a null model.\n\n**Question 9**\n\n The exponent of the coefficient b represents the odds ratio associated with a one-unit increase in the predictor.\n\n**Question 10**\n\n A high value of likelihood indicates that the observed outcome values are very likely given the parameters.\n\n**Question 11**\n\n The likelihood ratio test is used to compare model fit between two nested models.\n\n**Question 12**\n\n The classification table helps researchers optimize the trade-off between false positives and false negatives in logistic regression.\n\n**Question 13**\n\n The Nagelkerke Pseudo R2 statistic rescales the Cox & Snell statistic to range from 0 to 1.\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n:::\n\n\n\n\n\n\n\n## Tutorial\n\n### Probability, Odds, and Logits\n\nWe will start this session on logistic regression with some theoretical exercises. This way, you will learn how to work with probability, odds, and logits.\n \nThe given probability is P = 0.36.\n\nWhat are the corresponding odds? _____^[0\\.563]\n\nPlease find the formula’s below:\n\nGoal | Function\n-----|----------\nProbability -> Odds | $odds = P/(1-P)$\nOdds -> Probability | $P = odds/(1+Odds)$\nOdds -> Logit | $logit = \\ln(odds)$\nLogit -> Odds | $odds = e^{logit}$\n\n \n\nAgain, the given probability is P = 0.36.\n\nWhat is the corresponding logit? ______^[-0\\.575]\n\nThe given logit is – 2.7.\n\nWhat are the corresponding odds? _____^[0\\.067]\n\n\nAgain, the given logit is – 2.7. _____^[0\\.063]\n\n\n\nDiscuss with your group mates when and why we should carry out logistic regression analysis.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Explanation]\n\n```\n\n\nWe carry out logistic regression analysis when we want to carry out regression analysis and we have a dichotomous outcome variable (i.e., a dependent variable with two answer categories). In that case we cannot use linear regression because several of the assumptions of linear regression are violated.\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n \n\nIn a study concerning the smoking behavior amongst adolescents, a logistic regression analysis is conducted to check the effect of the image of smoking (Image) on smoking behavior (Smoking).\n\nImage: to what degree the young adult thinks smoking is perceived as “cool”.\nThe variable image is measured on a scale from 10 to 30, in which higher scores indicate that smoking is perceived as cooler.\n\nSmoking: whether or not the adolescent smokes.\n\n* 0 = the adolescent is a non-smoker\n* 1 = the adolescent is a smoker\n \n\nBelow you can find part of the output the researchers retrieved.\n\n![](images/Figure 5-2.png)\n\nFirst, write down the estimated regression model.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n\n\n$Logit(Smoking = 1) = -5.65 + 0.28*Image$\n \n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nWhat is the probability that an adolescent with a score of 15 on image smokes? ____^[0\\.33]\n \nTake a look at the regression coefficient.\n\n![](images/Figure 5-1-1.png)\n\nImagine that one's score on Image will increase from 15 to 16.\n\nTo what degree will the logit and odds change?\n\nAnd what can you conclude about the increase in probability?\n\n\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\nThe logit increases with 0.284. So −1.387+0.284=−1.103\n\nThe odds increase with a factor 1.328. So, the odds become 0.25×1.328=0.332\n\nBy just looking at the output, it is unclear what *exactly* will happen with the probability. We DO know that the probability will increase because the logits and the odds increase.\n\nIf we would calculate the probability by hand, we would see that the probability increases with 0.05.\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nAs you can see, the logit follows a linear function, the odds follow an exponential function, and probability follows a logistic function (perhaps less clear from the example but see the graphs below for an illustration). It is nice to work with a linear model (i.e., work with the logits), but from an interpretation point of view odds and probabilities are nicer to work with.\n\n\n![](images/Figure 7-1.png)\n\n\n### Logistic Regression\n\nToday we will study whether the probability to pass a Statistics course depends on exam fear and the math grade obtained in high school (prior math ability).\n\nOpen LAS BE LR.sav.\n\nIn the table below you find the variables included in this fictional data set.\n\n\nWe want to study the following research question:\n\nDoes the probability of passing the statistics exam depend on exam stress and math ability?\n\nTo answer this question, we first need to dichotomize the dependent variable. \nThis is an unusual step, because dichotomizing variables loses information!\nStill, for the purpose of the present exercise, we will do so.\nWe will create the new dependent variable Passed (0 = fail, 1 = pass), where a 5.5 or higher counts as a pass. Use the following steps:\n\nNavigate to Transform → Recode into different variables.\nSelect GradeStats as input variable and use Passed as the name for the new recoded variable. Do not forget to click on CHANGE.\nClick on Old and new values and specify the recoding in such a way that all grades of 5.5 or higher will get the new value 1 and all grades lower than 5.5 (i.e., all other grades) the value 0.\nPaste and run the syntax.\nGo to variable view and specify the value labels.\n \n\nObtain the frequency distribution for Passed (via Analyze --> Descriptive Statistics --> Frequencies).\n\nWhat percentage of students passed? ____^[82\\.2]%\n\n\nWe will use logistic regression to study the effect of exam stress and math ability on the probability to pass. Take the following steps:\n\nNavigate to Analyze --> Regression --> Binary logistic regression\nSelect Passed as dependent variable and ExamStress and Math as independent variables (in SPSS called “covariates”).\nPaste and run the syntax.\n \n\n\nInspect the output corresponding to Block 1. Take a look at the table “Variables in the Equation”.\n\nWhat would be good description of the effects on the sample level? (i.e., what the effect of ExamStress and Math on the probability to pass looks like).\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\nTurns out Exam Stress has a negative effect on the probability to pass (controlled for Math grade), and Math a positive effect (controlled for Exam Stress). \n \n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nWhat is the value of the regression coefficient for the variable Exam Stress? ______^[-0\\.025]\n \n\n\nGive a detailed interpretation of this number.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n\nIf we would increase one unit in Exam Stress the logits to pass the exam will decrease with 0.025, while keeping the variable Math constant.\n \n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nTrue or false: The independent variables Math and Exam Stress together have a significant effect on the probability to pass. TRUE / FALSE^[TRUE]\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Explanation]\n\n```\n\n\nWhen we inspect the model test, we see that, together, Exam Stress and Math grade have a significant effect on the probability to pass, χ2(2) = 81.043, p < .001.\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n \n\nSecond, carry out the significance tests for the individual predictors as well.\n\nTrue or false: There is a significant effect of Math on the probability to pass, controlled for Exam stress. TRUE / FALSE^[TRUE]\n\nTrue or false: There is a significant effect of Exam stress on the probability to pass, controlled for Math. TRUE / FALSE^[FALSE]\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Explanation]\n\n```\n\n\nWhen we inspect the separate regression coefficients together, we see that:\n\nControlled for Exam stress, Math grade has a significant effect on the probability to pass, χ2(1)=54.500, p<.001.\nControlled for Math grade, Exam stress has no significant effect on the probability to pass, χ2(1)=0.040, p=.841.\n \n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\n### Logistic Regression with Categorical Predictor\n\nNow, we will carry out a logistic regression analysis to study whether the probability to pass can be predicted from the amount of preparation (Preparation), controlled for prior math ability (Math).\n\nBut... the variable Preparation is a nominal (categorical) variable. Use dummy coding to include it in the model. The variable Preparation has three categories. Create all three dummy variables!\n\nTake the following approach to carry out the analysis.\n\nNavigate to Analyze --> Regression --> Binary Logistic\n\nSelect Math as covariate.\n\nSelect the dummies for Preparation as covariates. At this stage, use “only reading the book” as the reference group.\n\nPaste and run the syntax.\n \n\nTake a look at the table Variables in the Equation.\n\n\nInspect the estimated regression coefficients. Consider the results on the sample level, ignoring inferential statistics for now.\n\nWhich group has the highest estimated probability to pass? ^[Students who read the book and did the exercises.]\n\n* (A) Students who read the book and did the exercises.  \n* (B) Student who only read the book.  \n* (C) Students who only did the exercises.  \n\n \n \n\nWhich group has the lowest estimated probability to pass (controlled for math ability)? ^[Student who only read the book.]\n\n* (A) Student who only read the book.  \n* (B) Students who only did the exercises.  \n* (C) Students who read the book and did the exercises.  \n\n \n\nWrite down the full model equation, filling in the values of coefficients. Then check your answer.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n$logit_{passed} = -5.833 + .300*D_{exercises} + .946*D_{both} + .999 * Math$\n\n\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n \nControlled for Math Grade, what is the difference in predicted odds between people who only read the book and people who read the book and made the exercises? _____^[2\\.576]\n\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Explanation]\n\n```\n\nTake the exponent of the regression coefficient for the dummy for people who read the book and made the exercises (see table).\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n \n\nBy hand, calculate the probability that the third person in the file (respID=3) passes the exam. _____^[0\\.752]\n \n\nImagine that you do not know whether this person passed the exam or not.\n\nTrue or false: Based on your answer in the previous step and using a decision threshold of .5, you would predict this student to pass. TRUE / FALSE^[TRUE]\n\n\nSPSS can easily calculate the probability to pass. \n\nEither use the visual interface: \n\nClick on the SAVE button in the menu for logistic regression.\n\nSelect Probabilities under the header Predicted Values.\n\nClick on continue.\n\nOr add this line of code to your model:\n\n```\n  /SAVE=PRED\n```\n\nSPSS adds a new variable to the data file, which gives the predicted probability for each person. Check whether you calculated the predicted probability for person 3 correctly.\n\n \n\nImagine that we would have used this model to predict the probability to pass for the students in this sample before they even made the exam. Use a decision threshold of .5 to classify students as those likely to pass vs fail. \n\nOut of all students that were classified as \"pass\", which proportion actually failed? _____^[0\\.141]\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Explanation]\n\n```\n\nOf the 370 students who were predicted to pass, 52 failed the exam (.141). As you can see, the model is not completely flawless in making predictions.\n \n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nWhat is worse in your opinion? Incorrectly predicting that students will pass (when they end up failing), or incorrectly predicting that students will fail (when they end up passing)? Why?\n\nIf you chose a different decision criteria to predict passing - say .7, what would happen to the proportion of students that were classified as \"pass\", but actually failed? ^[gets smaller]\n\n* (A) can't say  \n* (B) stays the same  \n* (C) gets smaller  \n* (D) gets bigger  \n\n\n\n\n### Hierarchical Logistic Regression\n\nIn this assignment we will compare the following two models against each other.\n\n* Model1: $Logit(Pass) = b0+b1∗Math$\n* Model2: $Logit(Pass) = b0+b1∗Math+b2∗ExamStress+b3∗Evaluation$\n\nLater on, we want to carry out a model comparison test to check whether the larger model predicts the probability of passing the exam better than the smaller model.\n\nWhat would be the regression df for the model comparison test, in which we compare the larger model 2 to the smaller model 1? _^[2]\n\nNow we will carry out the model comparison test. Take the following steps.\n\nNavigate to Analyze -> Regression -> Binary Logistic\n\nSelect Math as predictor (covariates).\n\nClick on Next (upper right); we can now indicate which block of predictors we like to add in addition to the variables added in the first model.\n\nEnter ExamStress and Evaluation as predictors (covariates).\n\nPaste and run the syntax.\n \n\nInspect the output. SPSS organizes the output in three blocks. The results in Block 0 refer to the null model without any predictors. Note that this null model also exists when you use the \"Linear Regression\" interface, but it's not featured in the output.\n\nBlock 1 refers to the results of Model 1 and Block to the results of Model 2.\n \n\nTake a look at the results of the model comparison test. \n\nWhat test statistic is used to compare nested logistic regression models? ^[Chi squared]\n\n* (A) Z  \n* (B) t  \n* (C) F  \n* (D) Chi squared  \n\n\n\nWhat is the value of the appropriate test statistic for comparing models 1 and 2? _____^[1\\.297]\n\nTrue or false: Adding predictors ExamStress and Evaluation lead to significantly better predictions, compared to a model with only high school math grade as predictor. TRUE / FALSE^[FALSE]\n",
    "supporting": [
      "logistic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}