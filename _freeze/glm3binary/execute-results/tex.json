{
  "hash": "a361800e657f5e44ec84a4c540e4e3bd",
  "result": {
    "engine": "knitr",
    "markdown": "# GLM-III: Binary Predictors {#sec-glm3}\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can examine group differences in a continuous outcome variable using bivariate regression.\nTo do this, group membership must be represented as a binary variable (e.g., gender or ethnicity).\nTo ensure meaningful results, we use dummy coding to represent the binary variable.\nDummy coding assigns the value 0 to one category, which serves as the reference category, and the value 1 to the other category. \nWhen we include this dummy variable as the predictor in a bivariate linear regression analysis,\nit will estimate the mean value of the reference category and test the difference between the means of the two categories.\n\nRegression with a binary predictor is completely equivalent to the independent samples t-test. The independent samples t-test is also used to compare the means of two independent groups.\nIn regression, we estimate the slope (b) for the binary predictor, which represents the difference between the means of the two groups.\nThis t-test of the slope in regression is the same as an independent samples t-test.\n\nBoth regression with a binary predictor and the independent samples t-test rely on certain assumptions. These include the linearity of the relationship between the binary predictor and the outcome variable, the normality of residuals (the outcome variable should be normally distributed within each group), homoscedasticity (equal variances in both groups), and independence of observations. To check for equal variances, we can use Levene's test - but keep in mind that \"assumption checks\" are questionable. If you assume equal variances, report the normal t-test; if you do not assume equal variances, you can report a corrected t-test that allows for different variances. Both are included in SPSS output by default.\n\nTo determine the practical significance of a mean difference, we can calculate an effect size measure. Cohen's d is a commonly used effect size for mean differences. It standardizes the difference between the two group means by the pooled standard deviation.\nA larger Cohen's d indicates a greater magnitude of difference between the groups.\nAs a rule of thumb, a small effect size is typically considered around d ≈ 0.2, a medium effect size around d ≈ 0.5, and a large effect size around d ≈ 0.8.\n\n## Lecture\n\n\n\n\n\n\n{{< video https://www.youtube.com/embed/QeKr2R8Eyhk >}}\n\n\n\n\n\n\n\n\n\n\n\n## Formative Test\n\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\n\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nWhat is the purpose of dummy coding in regression with binary predictors? ^[To estimate the mean of the reference category and test the difference between categories.]\n\n* (A) To increase the number of predictors in the model.  \n* (B) To convert binary variables into continuous variables.  \n* (C) To estimate the mean of the reference category and test the difference between categories.  \n* (D) To simplify the model by removing categorical predictors.  \n\n\n\n**Question 2**\n\nWhat does the slope coefficient (b) represent in regression with a binary predictor? ^[The difference in means between the two categories.]\n\n* (A) The difference in means between the two categories.  \n* (B) The intercept of the reference category.  \n* (C) The mean of the second category.  \n* (D) The ratio of the two categories&apos; means.  \n\n\n\n**Question 3**\n\nWhich assumption is not relevant for the independent samples t-test and bivariate linear regression with only a binary predictor? ^[Linearity of relationship between X and Y]\n\n* (A) Normality of residuals  \n* (B) Linearity of relationship between X and Y  \n* (C) Homoscedasticity  \n* (D) Independence of observations  \n\n\n\n**Question 4**\n\nWhat does the Levene's test check in the context of the independent samples t-test? ^[Equality of variances in both groups.]\n\n* (A) Normality of residuals.  \n* (B) Equality of variances in both groups.  \n* (C) Linearity of relationship between X and Y.  \n* (D) Normality of variances in both groups.  \n\n\n\n**Question 5**\n\nHow is the independent samples t-test related to the t-test of the slope in regression with a binary predictor? ^[They are equivalent.]\n\n* (A) The t-test of the slope is a subset of the independent samples t-test.  \n* (B) The independent samples t-test is a subset of the t-test of the slope.  \n* (C) They are equivalent.  \n* (D) They are not related.  \n\n\n\n**Question 6**\n\nWhat does the p-value in the context of the independent samples t-test indicate? ^[The probability of observing a group difference at least as extreme as the one observed, if the null hypothesis is true.]\n\n* (A) The probability of observing a statistically significant result.  \n* (B) The probability that the null hypothesis is true.  \n* (C) The probability of observing a group difference at least as extreme as the one observed, if the null hypothesis is true.  \n* (D) The probability that the null hypothesis is rejected.  \n\n\n\n**Question 7**\n\nWhat does Cohen's d represent? ^[An effect size for the mean difference, expressed in number of standard deviations.]\n\n* (A) An effect size for the mean difference, expressed in number of standard deviations.  \n* (B) The difference between the two categories&apos; means.  \n* (C) A measure of explained variance for mean differences.  \n* (D) An effect size for the difference between standardized group means.  \n\n\n\n**Question 8**\n\nWhat is the recommended approach when assumption checks for homoscedasticity are significant? ^[It depends - in exploratory analyses, you may switch to a robust test; in confirmatory analyses, you would report the violation and proceed as planned.]\n\n* (A) It depends - in confirmatory analyses, you may switch to a robust test; in exploratory analyses, you would report the violation and proceed as planned.  \n* (B) Exclude outliers to ensure homoscedasticity.  \n* (C) Use a robust t-test.  \n* (D) It depends - in exploratory analyses, you may switch to a robust test; in confirmatory analyses, you would report the violation and proceed as planned.  \n\n\n\n**Question 9**\n\nThe observed mean difference between two groups is 2.50, and Cohen's D is 1.25. What is the pooled standard deviation? ^[2]\n\n* (A) 1.25  \n* (B) 2  \n* (C) 2.50  \n* (D) Can&apos;t say based on this information.  \n\n\n\n**Question 10**\n\nA researcher accidentally coded a dummy variable as 0 and 2, instead of 0 and 1. The regression equation is Y = 5.66 + 3*D. What is the mean value of the group coded 2? ^[11.66]\n\n* (A) 7.16  \n* (B) 8.66  \n* (C) 11.66  \n* (D) 3  \n\n\n\n:::\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Show explanations]\n\n```\n**Question 1**\n\n Dummy coding allows regression to include binary predictors by assigning numerical values to each category, estimating the mean of the reference category, and testing the difference between categories.\n\n**Question 2**\n\n The slope coefficient (b) in regression with a binary predictor represents the difference in means between the two categories, indicating how much the dependent variable changes when the binary predictor changes from 0 to 1.\n\n**Question 3**\n\n The assumption of linearity is not relevant, because the difference between two binary values of the predictor is linear by definition.\n\n**Question 4**\n\n Levene's test checks the assumption of equality of variances in both groups for the independent samples t-test.\n\n**Question 5**\n\n The independent samples t-test and the t-test of the slope in regression with a binary predictor are equivalent tests that compare means between two independent groups.\n\n**Question 6**\n\n The p-value indicates the probability of observing a group difference at least as extreme as the one observed, assuming that the null hypothesis is true.\n\n**Question 7**\n\n Cohen's d is an effect size that standardizes the difference between group means by the (pooled) standard deviation, making it interpretable on a meaningful scale.\n\n**Question 8**\n\n Assumption checks can alert you that important assumptions of the test are violated, but you should not blindly adapt analyses based on their results either - particularly in confirmatory research. You can always perform a sensitivity analysis in which you report both the planned analysis and the robust version.\n\n**Question 9**\n\n Cohen's D = mean difference/pooled sd.\n\n**Question 10**\n\n The slope tells you how much the predicted value goes up for a 1-unit increase in the predictor D. Since D is coded 0 and 2, a 1-unit increase only gets you halfway!\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n:::\n\n\n\n\n\n\n## In SPSS\n\n### Independent Samples t-test\n\nAs a t-test and as regression with a dummy predictor:\n\n\n\n\n\n\n{{< video https://www.youtube.com/watch?v=PbXMMN4YTQc >}}\n\n\n\n\n\n\n\n\n\n\n\n\n## Tutorial\n\n### Independent Samples T-Test\n\nIn this assignment we will use the data file [`5groups.sav`](data/5groups.sav). Download the file and open it in SPSS.\n \nThis time, we will compare the means of the variable y of two specific groups: group 1 and group 4. To test the difference between two sample means, we will use the t-test for independent samples.\n\n\nWhat is the null hypothesis of this test? And what is the alternative hypothesis?\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n\n$H_0: \\mu_1=\\mu_4$, against $H_1: \\mu_1\\neq \\mu_4$\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nCreate the necessary syntax for the t-test that compares the means of group 1 and group 4.\n\nYou can find the dialog for the two-sample t-test under Analyze > Compare Means > Independent Samples T Test\n\nIn the SPSS dialog you have to specify which two groups you want to compare. In our case, it's group 1 and group 4. After placing the variable in the box named \"Grouping Variable\", click the button named \"Define Groups\" to define the groups.\n \n\nCompare your syntax to the correct syntax:\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n\nT-TEST GROUPS=group(1 4)\n  /MISSING=ANALYSIS\n  /VARIABLES=y\n  /CRITERIA=CI(.95).\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\n\nOne of the assumptions of the independent samples t-test is homoscedasticity (equal variances for all levels of the predictor). We can compare the sizes of the variances of the two groups with a simple F-test, which we call Levene's test.\n\nHave a look at Levene's test and try to interpret it. Discuss with your group what null-hypothesis is being tested here.\n\n\nWhat is the p-value of the Levene's test? ____^[0\\.05]\n\n\nWhat do you conclude from this? What's the practical use of the outcome of this test?\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Explanation]\n\n```\n\nLevene’s test is not significant.\nRemember that the null hypothesis of Levene’s test is that the population variances of the group are equal. As the p-value is not significant, we cannot reject the null hypothesis. Consequently, there is no evidence that the population variances of two groups are unequal.\nThus, there is no reason to question the assumption.\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n \nNow you will have to decide on the outcome of the actual t-test. SPSS reports two versions: one that assumes equal variances (top row) and one that relaxes this assumption (bottom row).\n\nYou should pick one of these. In principle, you should decide which one you will use before seeing the results - although if there is clear evidence of violation of assumptions, you might want to discuss in your report whether the results change if you use the robust version (bottom row).\n\nFor now remember: we assume equal variances.\n\nWhat is the two-sided p-value? _____^[0\\.021]\n\nDo you reject the null hypothesis of this t-test at alpha 0.05? ^[Yes]\n\n* (A) Yes  \n* (B) No  \n\n\n\n\n### Regression with dummies\n\nWe will now perform the exact same analysis, but with regression and dummies.\n \nTo test the difference between group 1 and group 4, we first create a dummy variable to distinguish these two groups. Use group 1 as reference category.\nYou can use either Transform -> Recode into different variables, or syntax:\n\n```\nRECODE group (1=0) (4=1) INTO dgroup4.\nEXECUTE.\n```\n\nNote that all other groups are coded as missing on this variable, which is exactly what we want!\n\nWe will use regression to perform our t-test. The hypothesis is the same as in the previous assignment, but you could also rewrite it in terms of regression coefficient(s). What is the null hypothesis of this test in terms of regression coefficient(s)? And what is the alternative hypothesis?\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n\n$H_0: \\beta_{group1 vs group2}=0$ which is the same as $H_0: \\mu_{group1} = \\mu_{group2}$, versus $H_1: \\beta_{group1 vs group2} \\neq 0$ which is the same as $H_0: \\mu_{group1} \\ne \\mu_{group2}$\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nCreate the necessary syntax for a regression with the dummy variable that compares the means of group 1 and group 4.\n\nYou can find the dialog under Analyze > Regression > Linear\n\nIn the SPSS dialog you have to specify the Dependent and Independent variable. In our case, the independent variable is the dummy we created.\n \nCompare your syntax to the correct syntax:\n\n```\nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT y\n  /METHOD=ENTER dgroup4.\n```\n\nNote that, unlike the t-test interface, the regression interface does not provide a Levene's test.\nThis is one reason you might want to use the t-test interface.\nThe regression interface provides a more generic way to test the assumption of homoscedasticity: a residual plot.\n\nGo back through the regression interface, but this time click the Plots button and plot the predicted value (X = ZPRED) against the residual value (Y = ZRESID).\n\nYour syntax will now say:\n\n```\nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT y\n  /METHOD=ENTER dgroup4\n  /SCATTERPLOT=(*ZRESID ,*ZPRED)\n```\n\nIf the assumption of homoscedasticity is met, we should see that the dots in this plot are equally distributed around the zero line for all values on the X-axis. In this case, we see much narrower spread on the right side than on the left side.\n\nWhat can you conclude from this, and does it match your conclusion from Levene's test?\n\nNow you will have to decide on the outcome of the actual t-test.\n\nRemember that the t-test of the dummy variable should be the same as the t-test we conducted before. Verify that this is true.\n\nWhat is the two-sided p-value? _____^[0\\.021]\n\nWe see one more t-test: for the \"(Constant)\" or intercept. How do we interpret this?\n\n^[The mean value in Group 1 is 7, and this differs significantly from zero.]\n\n* (A) The mean difference between both Groups is 7, and this differs significantly from zero.  \n* (B) The mean value in Group 4 is 7, and this differs significantly from zero.  \n* (C) The mean value across both Groups is 7, and this differs significantly from zero.  \n* (D) The mean value in Group 1 is 7, and this differs significantly from zero.  \n\n\n",
    "supporting": [
      "glm3binary_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}