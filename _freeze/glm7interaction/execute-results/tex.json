{
  "hash": "0ef9a25a92eef85e966263ab415c17b5",
  "result": {
    "engine": "knitr",
    "markdown": "# GLM-VII: Interaction {#sec-interaction}\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe foundation of the regression model is the equation that describes the relationship between two variables. In a simple bivariate linear regression, we use the equation:\n\n$Y_i = a + b \\cdot X_i + e_i$\n\nWhere:\n\n- $Y_i$ represents an individual's score on the dependent variable $Y$.\n- $a$ is the intercept coefficient of the regression line.\n- $b$ is the slope coefficient of the regression line.\n- $X_i$ is an individual's score on the independent variable $X$.\n- $e_i$ is the prediction error for individual $i$.\n\nThe regression line provides us with predicted values based on the model. It can be represented as:\n\n$\\hat{Y}_i = a + b \\cdot X_i$\n\nWhere $Y^i$ is the predicted score for individual $i$ on the dependent variable $Y$.\n\n## Introducing Interactions\n\nNow, let's take our regression analysis to the next level by introducing the concept of interactions. An interaction implies that the effect of one predictor variable depends on the level of another predictor variable.\n\nTo incorporate interactions, we add a special building block to our regression equation:\n\n$Y = a + b_1X_1 + b_2X_2 + b_3(X_1 \\times X_2)$\n\nTo include an interaction term in your regression model:\n\n1. Calculate a new variable as the product of the two interacting variables.\n2. Add this interaction term to the regression equation along with the original variables. **Note:** You should never include an interaction term in the model without including its constituent terms!\n\nIn summary, interactions add a new layer of complexity to regression analysis by acknowledging that the relationships between variables can be contingent on other factors. By incorporating interactions, we can gain deeper insights into the nuanced dynamics between predictors and outcomes in our statistical models.\n\n## Interaction between one Continuous and one Binary Predictor\n\nInteraction is easiest to explain using one binary predictor, coded as 0 and 1, and one continuous predictor. \n\nRecall that using this type of \"dummy coding\" allows us to represent different intercepts for the two groups in our regression model. The intercept of the regression equation applies to the group coded 0 (the reference group), and the intercept of the group coded 1 is equal to the overall intercept plus the effect of the dummy variable. In formulas this is represented as:\n\n\\begin{align}\nY &= a + bD \\text{, where D} \\in(0,1)\\\\\nY_{D=0} &= a + b*0 = a\\\\\nY_{D=1} &= a + b*1 = a + b\n\\end{align}\n\nAn interaction implies that not only the intercept but also the regression slope differs between two groups. Imagine we add continuous predictor $X$ to the model, as well as the interaction term $D*X$:\n\n\\begin{align}\nY &= a + b_1D + b_2X +b_3 (D*X)\n\\end{align}\n\n\nIn some cases, we might want to estimate not only distinct intercepts but also distinct slopes for different groups. For instance, we might be interested in understanding how the effect of gender role attitudes on involvement differs for men and women.\n\nThe complete formula for the regression line varies based the value of dummy variable $D$:\n\n\n\\begin{align}\nY_{D=0} &= a + b_1*0 + b_2X +b_3 (0*X) = a + b_2X\\\\\nY_{D=1} &= a + b_1*1 + b_2X +b_3 (1*X) = (a + b_1) + (b_2 + b_3)X\n\\end{align}\n\n\nNote that both groups' regression equations can be simplified into a basic linear formula of the form $a + bX$, except that they each have a unique value for the intercept and regression slope! This is how interaction terms allow you to make the effect of one variable contingent on the value of another.\n\nIf the interaction term is significant (i.e., the slope for the product of the interacting variables), we conclude that there is significant interaction and the slope of the effect of one interacting variable depends on the value of the other interacting variable.\n\n## Simple Effects\n\nWhen the interaction between a binary moderator and a continuous predictor is significant, we often want to know how big the regression effect of the continuous predictor is in each group of the binary predictor. This is called simple effects analysis.\n\nOne straightforward way to perform simple effects analysis involves creating dummy variables for both categories of the binary moderator and computing interaction terms with these dummies. Then, specifying two regression models with different reference categories. This gives the effect of the continuous moderator for each group, along with a significance test.\n\n## Interaction with Two Continuous Predictors\n\nWhen we previously discussed interaction effects involving one binary and one continuous predictor, we discovered that such interactions result in distinct regression lines for each unique value of the binary predictor. Now, consider an interaction between two continuous predictors. In this scenario, each variable can theoretically take infinite possible values. Therefore, we can no longer think of this as a distinct regression line for each value of the moderator. Instead, we can imagine how an increase in the value of one interacting variable leads to an adjustment of the effect of the other interacting variable.\n\nTo grasp the concept of interaction effects with two continuous predictors, let's dive into a concrete example. Imagine we're investigating the relationship between outcome Y, and continuous predictors X1 and X2. We've determined the coefficients for our regression model:\n\n$Y = 12.50 + 1.50 \\cdot X_{1} - 0.20 \\cdot X_{2} + 0.07 \\cdot (X_{1} \\times X_{2})$\n\nLet's do the same we did for understanding the regression equation with a binary moderator, and simplify it by plugging in a specific value for one of the interacting variables. Suppose we want to know the effect of X1 for someone who scores 0 on the continuous variable X2:\n\n$Y = 12.50 + 1.50 \\cdot X_{1} - 0.20 \\cdot 0 + 0.07 \\cdot (X_{1} \\times 0) = 12.50 + \\textbf{1.50} \\cdot X_{1}$\n\nThe effect for such a person is 1.50. Now, let's compare this to a person who scores 1 on the continuous variable X2:\n\n$Y = 12.50 + 1.50 \\cdot X_{1} - 0.20 \\cdot 1 + 0.07 \\cdot (X_{1} \\times 1) = (12.50-0.20) + (1.50 + 0.07) \\cdot X_1 = 12.30 + \\textbf{1.57} \\cdot X_1$\n\nNow, the effect of $X_1$ has increased by 0.07 - which was exactly the size of the regression slope for the interaction term.\n\n## Centering for Interpretability\n\nWhen working with interactions between two continuous predictors, it's essential to center the variables. Centering aids interpretability - the effect of one predictor is now given for the average value of the other predictors. Moreover, centering avoids artificial multicollinearity between the two interacting variables and their interaction term.\n\n### Simple Slopes\n\nIf the interaction effect between two continuous predictors is significant, we might want to understand how the effect of one of the interacting predictors varies across levels of the other interacting predictor. This is similar to the simple effects approach from before, except now it's called simple slopes.\n\nInstead of computing the effect of one variable for all unique values of a binary moderator, we pick specific values of the continuous moderator - typically +/- 1SD - and calculate the effect of the other predictor at those specific values.\n\nBy centering the interacting predictors at their mean value +/- 1SD and re-computing the interaction term using those transformed predictors, we obtain simple slopes at different levels of the moderator. Note that centering at $M + 1SD$ gives us the effect for people who score 1SD **below** the mean (you're sliding the distribution to the right on the number line, until people who used to score -1SD are centered at 0).\n\n# Lecture\n\n\n\n\n\n\n{{< video https://www.youtube.com/embed/fGj_t72VJPk >}}\n\n\n\n\n\n\n\n\n\n\n# Formative Test\n\nComplete the formative test ideally after youâ€™ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nAn interaction effect is when... ^[The effect of one predictor depends on the value of another predictor.]\n\n* (A) The effect of one predictor is added to the effect of another predictor.  \n* (B) The effect of one predictor depends on the value of another predictor.  \n* (C) The effect of two predictors is cumulative.  \n* (D) You interact with your partipants.  \n\n\n\n**Question 2**\n\nWhat is the purpose of centering continuous predictors in interaction analysis? ^[Centering continuous predictors helps in enhancing interpretability and reducing multicollinearity.]\n\n* (A) Centering continuous predictors helps in enhancing interpretability and reducing multicollinearity.  \n* (B) Centering continuous predictors increases interpretability.  \n* (C) Centering continuous predictors prevents multicollinearity.  \n* (D) Centering continuous predictors is necessary for regression analysis to provide correct results.  \n\n\n\n**Question 3**\n\nWhen two continuous predictors interact, how many unique regression lines are generated? ^[A theoretically infinite number of unique regression lines, as both predictors can take on an infinite number of values.]\n\n* (A) Two unique regression lines are generated, one for each predictor.  \n* (B) Only one unique regression line is generated regardless of the interaction.  \n* (C) A theoretically infinite number of unique regression lines, as both predictors can take on an infinite number of values.  \n* (D) The number of unique regression lines depends on the number of unique values of the predictors.  \n\n\n\n**Question 4**\n\nWhat is the concept of simple slopes in interaction analysis? ^[Simple slopes refer to the effect of one predictor variable, evaluated at specific levels of another variable.]\n\n* (A) Simple slopes refer to the effect of one predictor variable, evaluated at specific levels of another variable.  \n* (B) Simple slopes are the slopes of predictors before they are centered.  \n* (C) Simple slopes refer to the linear effects of a predictor on the outcome variable.  \n* (D) Simple slopes are the partial effects of two predictors, controlling for their interaction effect.  \n\n\n\n**Question 5**\n\nIn a multiple regression model with two continuous predictors, how would you assess the effect of one predictor at different levels of the other predictor? ^[By centering one predictor at a specific level and re-computing the interaction term.]\n\n* (A) By standardizing the predictors.  \n* (B) By excluding the interaction term from the model.  \n* (C) By computing the mean of both predictors.  \n* (D) By centering one predictor at a specific level and re-computing the interaction term.  \n\n\n\n**Question 6**\n\nWhat does it mean when an interaction term in a regression model is not significant? ^[When an interaction term is not significant, it suggests that the effect of one predictor variable on the outcome variable is consistent across all levels of the other predictor variable.]\n\n* (A) An insignificant interaction term indicates a problem with the data collection process.  \n* (B) When an interaction term is not significant, it suggests that the effect of one predictor variable on the outcome variable is consistent across all levels of the other predictor variable.  \n* (C) An insignificant interaction term means that the outcome variable is not related to the predictor variables.  \n* (D) An insignificant interaction term indicates that the model is overfitting.  \n\n\n\n**Question 7**\n\nWhat is the predicted value for an individual with a score of 2.5 on X1 and score of 35 on X2, given the following regression equation: Y = 10 + 2 * X1 - 0.5 * X2 + 0.1 * (X1 * X2)? ^[6.25]\n\n* (A) -3.75  \n* (B) 8.75  \n* (C) -2.4  \n* (D) 6.25  \n\n\n\n**Question 8**\n\nWhat is the effect of a one-unit increase in X1 on the predicted value for an individual who scores 40 on X2, given the following regression equation: Y = 8 + 1.2 * X1 - 0.3 * X2 + 0.05 * (X1 * X2)? ^[3.2]\n\n* (A) 0.1  \n* (B) 3.2  \n* (C) 1.5  \n* (D) 1.2  \n\n\n\n**Question 9**\n\nAssume that X1 and X2 are centered around the mean. Given the following regression equation: Y = 6 + 1.5 * X1 - 0.4 * X2 + 0.08 * (X1 * X2), what is the simple slope of X1 on Y for people who score 1SD above the mean on X2, if the SD of X2 is 0.5? ^[1.54]\n\n* (A) 1.54  \n* (B) 1.46  \n* (C) 1.58  \n* (D) 1.5  \n\n\n\n:::\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Show explanations]\n\n```\n**Question 1**\n\n An interaction effect involves the combined influence of two or more predictor variables on the outcome variable, which is different from their individual effects.\n\n**Question 2**\n\n The two purposes of centering are to avoid artificial multicollinearity between the interacting variables and their product, and aids the model's interpretability.\n\n**Question 3**\n\n In interactions involving continuous predictors, the relationship between the predictors and the outcome variable can vary infinitely, leading to an infinite number of possible regression lines.\n\n**Question 4**\n\n Simple slopes allow us to assess how the relationship between two predictors changes at different levels of the moderator variable, helping to understand conditional effects.\n\n**Question 5**\n\n Centering and re-computing the interaction term allows us to obtain the slopes of the predictor of interest at different levels of the moderator, helping us understand its conditional effects.\n\n**Question 6**\n\n When an interaction term is not significant, it implies that the relationship between the predictors and the outcome remains relatively constant regardless of the values of the interacting predictors.\n\n**Question 7**\n\n Plug in the values: Y = 10 + 2 * 2.5 - 0.5 * 35 + 0.1 * (2.5 * 35)\n\n**Question 8**\n\n You have to add the interaction term to the effect of X1; when X2 has the value 0, the effect of X1 is 1.2. When X2 has the value 40, add 0.05*40 to that effect.\n\n**Question 9**\n\nCalculating the simple slope works just the same as calculating the effect of X1 for specific values of X2, so you calculate the effect for +1 SD of .05: 1.5*X1 + 0.08*.5*X1 = 1.54*X1.\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n:::\n\n\n\n\n\n\n\n# In SPSS\n\n## Multiple Regression\n\n\n\n\n\n\n{{< video https://www.youtube.com/watch?v=l3Aoikhaxtg >}}\n\n\n{{< video https://www.youtube.com/watch?v=aeT8MkG3bx8 >}}\n\n\n{{< video https://www.youtube.com/watch?v=aVV7KnAr-qY >}}\n\n\n{{< video https://www.youtube.com/watch?v=vYsjJpyrHFc >}}\n\n\n{{< video https://www.youtube.com/watch?v=SdOrkPn7d8Y >}}\n\n\n\n\n\n\n\n\n\n\n# Tutorial\n\n## Interaction\n\nIn this assignment we work with the [`PublicParticipation.sav`](data/PublicParticipation.sav) data.\nIt contains (fictional) data on the following variables:  income (higher scores, more income), public participation, education, age, and gender (0 = females; 1 = males). Public participation involves being member of school boards, municipal councillor, etc.\n\nIn this assignment we will see how we can model interaction between a continuous predictor and a dichotomous predictor. \n\nSuppose we are interested in relationship between age and public participation, and we want to know if the relationship is moderated by gender. \nAn interaction model is conceptually represented as follows (these two diagrams are interchangeable):\n\n::: {layout-ncol=2}\n\n![](images/y_on_x1x2int2.png)\n\n![](images/y_on_x1x2int1.png)\n\n\n:::\n\n\nModeling Interactions\n\nThe regression model for testing the interaction is:\n\n$Y' = b_{0} + b_{1}X + b_{2}D_{g} + b_{3}XD_{g}$\n\nwhere  X = age, and D_g = gender (0 = women; 1 = men). Notice that women are our reference group.\n\nTo model interaction we need to create a new variable, which is the product of the dummy variable (gender in our case) and  (age in our case).\n\nThis is best done via syntax, but to use the graphical interface proceed as follows: \n\nvia Transform > compute variable \n\nGive the new variable a name (i.e., the target variable), say GenderTAge. \n\nThen specify the product at the right (see more information button). Click on Paste, select and run the code. Check in Data View whether the product term was added correctly.\n\nAlternatively, the syntax is:\n\n```\nCOMPUTE GenderTAge = Gender * Age.\nEXECUTE.\n```\n\nNow run the regression analysis that includes the interaction effect.\n\n**Important:** Just like with dummies you must include all dummies that belong to the same variable in the model together, with an interaction term, you must always include its constituent variables as well. This is because the interaction term only *modifies* the effect of its constituent variables; the effect of those constituent variables must thus also be in the model.\n\nSo, if you add variable intXTZ into the model, you must also include X and Z.\n\nVia analyze > regression > linear; choose age, gender and GenderTAge as the independent variables, and public participation as the dependent variable. \n\nConsult the table Regression coefficients. Write down the general estimated model.\n\nFinish the following equation, then check your answer.\n\n$\\text{Public Participation' = .....}$\n\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n$\\text{Public Participation}â€² = 3.252 + 0.137*\\text{Age} + 12.439*\\text{Gender} âˆ’ 0.116*\\text{Gender}*\\text{Age}$\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n \n\nNow write down the estimated models down for women and men separately. Hint: fill in 0 and 1 in the general estimated model mentioned in the previous step, then simplify the formula.\n\nComplete the equations for women (W) and men (M):\n\n$\\text{PP}_W'=$ _____^[3\\.252] $+$ _____^[0\\.137] $*\\text{Age}$\n\n$\\text{PP}_M'=$ ______^[15\\.691] $+$ _____^[0\\.021] $*\\text{Age}$\n\nNow draw (on a piece of paper) a graph of the results. That is, put age on the x-axis, the predicted public participation on the y-axis, and draw separate regression lines for males and females.\n\nTrue or false\n\nIn the sample, age has a positive effect on public participation for women but a negative effect for men? TRUE / FALSE^[FALSE]\n\n\nThe researchers tested at the 5% level and concluded:\n\n\"We have convincing evidence that the population effect of age on public participation is different for men and women.\" TRUE / FALSE^[FALSE]\n\nThe estimated regression model was:\n\n$Y'= 3.252 + 0.137Age + 12.439D_g - 0.116(Age \\times D_g)$\n\nWhat would the regression equation look like if we would have used the men as the reference group? Use logic to answer this question, instead of re-running the analysis.\n\n$Y'=$ ______^[15\\.691] $+$ _____^[0\\.021] $*\\text{Age}+$\n_______^[-12\\.439]\n$*D_g+$\n_____^[0\\.116]\n$*(\\text{Age} \\times D_g)$\n\n<!-- LaTeX: Å· = 15.691 + .021Age - 12.439D_{g}  + .116AgeD_{g}  -->\n\n\nTo verify our answer to the previous question, we will recode the variable Gender such that males are scored 0 (= reference group) and females are scored 1.\n\nProceed as follows:\n\n- via Transform > Recode into different variables \n\n- Select Gender.\n\n- Give a name to the new output variable (say GenderFem), give a label (say: \"Gender (ref=males)\" click on change.\n\n- Specify old and new values: old value 0 becomes 1 and old value 1 becomes 0 (don't forget to click on add in between).\n\n- Click OK. Verify that SPSS added a new column with a dummy variable where males are the reference group.\n\n- Compute the product variable for the interaction between age and gender but now use the dummy having males as reference group.\n\n- Rerun the regression analysis, but now using the new gender variable and interaction term. If you're answer in the previous step is correct you should find the values back in the table Regression Coefficients.\n\n\n\n\n## Categorical Predictors with Three or more Categories\n\nThe categorical predictor Education has three levels (low, middle, high). If we want to include such a variable we need to use dummies.\n\nCode the dummy variables as follows:\n\nValue | D1 | D2\n------|----|------\nLow   |0   |0\nMiddle   |1   |0\nHigh   |0   |1\n\nWhich group is the reference group according to this coding? ^[Low]\n\n* (A) Low  \n* (B) Middle  \n* (C) High  \n\n\n\n\nUse syntax to create the dummies.\n\nWe are now ready for the regression analysis.\n\nRun a hierarchical regression analysis with public participation as dependent variable. Model 1 only includes age. Model 2 includes age and the dummies. So we have the following nested models:\n\nThis model does not include the interaction effects yet! This means that we assume that the regression lines are parallel to one another. In the next assignment we check whether this assumption is reasonable.\n\nProceed as follows: \n\n- via analyze > regression > linear. \n- Select public participation as the dependent variable and only age as the independent variable. Click on next. \n- Now select the two dummies we have created in the previous step. The two dummies together represent education. Always enter dummies into the model together!\n- Via Statistics ask for the R-change statistics.\n\nConsult the output and answer the questions in the next few steps. \n\nEducation and age together explain  ___^[9\\.7] % of the total variance.\n\nWhat is the value of the test statistic that tests the unique effect of education, controlled for age? _____^[0\\.895] \n\nReport the results for the unique effect of education, then check your answer.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\nEducation does not have a significant unique effect on public participation after controlling for age, $\\Delta R^2 = .04, F(2,38) = 0.895, p = .417$. \n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n \n \nConsult the table with the regression coefficients. \n\nWrite down the estimated regression equation of Model 2. \n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n$PublicParticipation'\\:=\\:10.478\\:+\\:.097\\:\\cdot \\:Age\\:-\\:2.042\\:\\cdot \\:D1\\:-\\:3.071\\:\\cdot \\:D2$\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nWrite down the estimated model for each of the three groups.\n\nThen make a graph of the regression equations. Put age on the x-axis, the predicted public participation on the y-axis, and draw the lines for each education group. \n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\nThe models were:\n\n$PP'_l = 10.478 + .097Age$\n\n$PP'_m = (10.478-2.042) + .097Age = 8.436 + .097Age$\n\n$PP'_h = (10.478-3.071) + .097Age = 7.407 + .097Age$\n\nDid you get it right?\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n \n\nSuppose we have two persons, both are 40 years old, but one had middle level education and the other had high-level education.\n\nWhat is the expected (absolute) difference in public participation between these two persons? _____^[1\\.029]\n\nThe researchers conclude:\n\n\"Controlled for age, low educated people in the sample show highest level of public participation\".\n\nIs this a valid conclusion? TRUE / FALSE^[FALSE]\n\n\n## Interaction with more than Two Categories\n\nIn the previous assignment, we assumed that the effect of Age on Public participation was equal for each of the education level groups. However, we do not know whether this assumption is reasonable. In this assignment, we will check whether the interaction effect between Age and Education level is statistically significant or not.\n \nCreate the two interaction terms using syntax, with the Compute variable command. Note that we need two interaction terms: D1Tage and D2Tage.\n \n\nWe are now ready for the regression analysis.\n\nRun a hierarchical regression analysis. Model 1 only includes age and the two dummy variables. Model 2 additionally includes the interaction terms. \n\nWrite down the formulas for the two nested models, then check your answer.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n\n- Model 1: $Y'= b_0 + b_1Age + b_2D_1 + b_3 D_2$\n- Model 2: $Y'= b_0 + b_1Age + b_2D_1 + b_3 D_2 + b_4D_1Age + b_5 D_2Age$\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nProceed as follows (or, preferably, use syntax): \n\n- via analyze > regression > linear. \n- Select public participation as the dependent and age, D1 and D2 as the independent variables. Click on next. \n- Now select the two interaction terms we have created in the previous step. The two interaction terms together represent the interaction effect between education and age.\n- Via Statistics ask for the R-change statistics.\n\nConsult the output and answer the questions in the next few steps. \n\n Before we carry out any of the significance tests, let's take a look at the coefficients table. Look at the unstandardized coefficients in Model2. First, write down the entire estimated model.\n\nComplete the following equation:\n\n$Y'=$ ______^[11\\.426] \n$+$ _____^[0\\.073] $*Age+$\n_____^[-5\\.19] $*D_{middle}+$\n_____^[1\\.577] $*D_{high}+$\n_____^[0\\.067] $*(D_{middle}*Age)+$\n______^[-0\\.088] $*(D_{high}*Age)$\n\n\n\nNext, write down the estimated model for each of the three education groups.\n\nRemember, fill in 0 and 1 for the dummy variables, then simplify:\n\n\n$Y_{low}'=$ ______^[11\\.426] \n$+$ _____^[0\\.073] $*Age$\n\n$Y_{middle}'=$ _____^[6\\.236] \n$+$ ____^[0\\.14] $*Age$\n\n\n$Y_{high}'=$ ______^[13\\.003] \n$+$ ______^[-0\\.015] $*Age$\n\nNow, answer the following questions.\n\n \n\nTrue or False? \n\nThe effect of Age on Publication Participation in the sample is positive for all education groups. TRUE / FALSE^[FALSE]\n\nFor which group is the effect of Age on publication participation the strongest? ^[Middle]\n\n* (A) Low  \n* (B) Middle  \n* (C) High  \n\n\n\n\nWe inspected the estimated model. But is there a significant interaction effect to begin with? To answer that question we inspect the Model Summary Table. \n\nFirst of all, write down the $R^2$ for the model without- and with interactions. What do these numbers mean?\n\nWithout interactions: _____^[0\\.097]\nWith interactions: _____^[0\\.127] \n\n\nFinish the following sentence:\n\nModel 2 with the interaction effects explains an additional _^[3] % of the variance in Public Participation compared to Model 1 (on top of what was already explained by the main effects of Age and Education).\n\n\nWe will now carry out the F-change test. Write down the null hypothesis and alternative hypothesis that we test with this F-change test.\n\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n$H_0:\\:R^2\\:=\\:0$\n\n$H_1:\\:R^2 \\ne 0$\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\n\nWrite down the F-value, the df and the p-value.\n\n* F-value: _____^[0\\.618] \n* df: (_^[2] , __^[36] )\n* p-value: _____^[0\\.545] \n \n\nTrue or false: there is a significant interaction effect: TRUE / FALSE^[FALSE]\n\nTrue or False: As a follow-up analysis, we should perform a simple effects analysis. TRUE / FALSE^[FALSE]\n\nInterpret the results of Model 1 (without interaction) and report your results.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\nThere is no evidence for a significant effect of Age and Education on Participation, $R^2 = .10, F(3, 38) = 1.36, p = .27$.\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\n\n## Interaction Effects\n\nIn this assignment, you will examine whether the effect of relationship with coworkers (sccowork; higher score = better relationship) on the emotional pressure at work (scemoti) has an interation effect with gender (0 = male, 1 = female).\n\nIf there is an interaction effect, the effect of sccowork on scemoti depends on the value of the variable gender. \n\nOpen [`Work.sav`](data/Work.sav). \n\nTo be able to examine the interaction effect, you should first create a product variable. \n\n- Go to Transform > Compute Variable\n- Give a name to the new product variable in Target Variable (GenderTRelco for example).\n- In Nummeric Expression you need to specify how the new variable should be computed. You have to enter gender * sccowork to compute the product of gender and sccowork. \n- Paste and run the syntax, and check whether the product variable was added\n\n \n\nConduct a multiple regression analysis (using Analyze > Regression > Linear) with scemoti as dependent variable. The independent variables are the main effects (gender and sccowork) and the interaction effect (genderTsccowork).\n\nWhat is the p-value of the interaction effect? _____^[0\\.083]\n\nTrue or false: The interaction effect is significant at $\\alpha = .10$ TRUE / FALSE^[TRUE]\n \n\nThe regression equation for the entire sample is:\n\n$\\text{scemoti}'=$ ______^[27\\.166] \n$+$ ______^[-7\\.103] $*\\text{Gender}+$\n______^[-0\\.237] $*\\text{Relationship}+$\n_____^[0\\.439] $*(\\text{Gender}*\\text{Relationship})$\n\n\nFor males, the value of Gender is 0. That means that GenderTRelco is also 0. The regression equation for males then becomes:\n\n$\\text{scemoti}'=$ ______^[27\\.166] \n______^[-0\\.237] $*\\text{Relationship}$\n\nFor females, the value of Gender is 1. What is the regression equation for females?\n\n$\\text{scemoti}'=$ ______^[20\\.063] \n$+$ \n_____^[0\\.202] $*\\text{Relationship}$\n\nDraw (on paper, not in SPSS) a schematic graph of the interaction effect. Put relationship with coworkers on the X-axis, and emotional pressure on the Y-axis. Draw a schematic regression line for each group. \n\n\nIn what group is the effect of relationship with coworkers on emotional pressure the strongest: males or females? ^[Males]\n\n* (A) Males  \n* (B) Females  \n\n \n\n\nIn practice, you'd often want to know whether the effects within the groups are significant.\n\nCan you use the output of this regression analysis to draw conclusions about the significance of the effect within each group? ^[Yes, but only for the group of males]\n\n* (A) No  \n* (B) Yes, but only for the group of males  \n* (C) Yes, but only for the group of females  \n* (D) Yes, for both groups  \n\n\n\nAt this moment, we don't have enough information in the output yet to test the effect within the female group. But we can test the effect within the male group! \n\nWhat is the p-value of the effect of sccowork on scemoti within the male group? _____^[0\\.237]\n\nTo test the significance within the the group of females, we can simply switch the reference groups. \n\n- Make a new dummy variable called `male`, on which males score 1, and females 0\n- Compute a new product variable: `COMPUTE maleTsccowork = male * sccowork.`\n\nPerform a new regression analysis with these predictors. This is exactly the same analysis, but now with women as reference group instead of men.\n\nLook at the table with the estimated coefficients. What is the p-value of the effect of sccowork on scemoti within the female group? _____^[0\\.187]\n",
    "supporting": [
      "glm7interaction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}