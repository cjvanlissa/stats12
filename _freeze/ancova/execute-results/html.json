{
  "hash": "09a1c643819f6c6372d9dadb5d39381e",
  "result": {
    "engine": "knitr",
    "markdown": "# GLM: ANCOVA {#sec-ancova}\n\n\n\n\n\n\n\n\n\n\n\n\n\nANCOVA, which stands for Analysis of Covariance, is an extension of the concepts we've covered in bivariate linear regression and multiple regression.\nIt is essentially a multiple regression with a categorical predictor and one or more continuous predictors. What's \"special\" about this technique is that it is commonly used when the predictor of interest is that categorical variable, and the continuous predictor(s) are so-called \"covariates\": predictors that are only included to improve our estimate of the effect of the categorical predictor of interest.\n\nYou will often see this technique used to analyze data from experiments or \"natural experiments\", where participants self-select into a treatment group.\n\nWhile ANCOVA is a useful technique, it comes with some serious pitfalls: any time control variables are used, we are making assumptions about causality. If these assumptions are incorrect, our estimates of the effect of interest will be (severely) biased.\n\n### Covariates and Their Role\n\nCovariates are variables that have a relationship with the dependent variable but are not the primary focus of the study. They are often referred to as control variables, as they help control for unwanted variability and improve the precision of the analysis. Examples of common covariates include age, gender, education level, or any other variables that might influence the dependent variable.\n\nIn terms of causality, it's crucial to consider the relationships between covariates, predictors, and the outcome variable. Control variables should ideally be confounders – variables that influence both the predictor of interest and the outcome. It's essential to avoid controlling for colliders, which are variables *caused by* both the predictor and the outcome. A thorough understanding of causal relationships is crucial for proper interpretation.\n\nOne reason why researchers use control variables in ANCOVA is because they reduce the residual variance in the outcome variable, which in turn increases the power to detect the effect of the predictor of interest. Another reason to use covariates is when the goal is making causal inferences, especially in quasi-experimental designs. The proper selection of covariates that enable causal inference requires careful consideration and is beyond the scope of this course.\n\n### Good, Neutral, and Bad Controls\n\nCovariates fall into different categories based on their relationship with the predictor of interest and the outcome. An example of a good control is a confounder: a variable that causes both the predictor and the outcome. These need to be controlled to avoid spurious relationships. An example of a neutral control is a covariate that is unrelated to the predictor but can reduce error variance in the outcome, thereby increasing statistical power. Bad controls, on the other hand, can introduce biases, such as collider bias (controlling for an outcome of predictor and outcome), case control bias (controlling for an outcome of the outcome), or overcontrol bias (controlling for a mediator of the effect of the focal predictor on the outcome).\n\nOne crucial insight is that in randomized controlled experiments, the random assignment of participants to different groups breaks the relationship between confounders and the treatment variable. This makes control variables related to the confounders unnecessary. Controlling for them could even introduce bias into the analysis.\n\n\n### Calculating Adjusted Means\n\nAdjusting for covariates involves calculating adjusted means – the means that groups would have had if they scored equally on the covariate. There are two ways to mathematically calculate the adjusted means. One way is to fill in the regression equation for the desired value of the covariate. \n\nThe other way is to calculate the adjusted means from the group means:\n\n$$\n\\bar{Y}_g^{adj} = \\bar{Y}_g - b(\\bar{X}_g-\\bar{X})\n$$\n\nWhere:\n\n* $\\bar{Y}_g^{adj}$: Adjusted mean of the outcome for group g\n* $\\bar{Y}_g$: Unadjusted mean of the outcome for group g\n* $b$: Regression coefficient of the covariate\n* $\\bar{X}_g$: Group mean of covariate X\n* $\\bar{X}$: Overall mean of covariate X\n\n\nIn sum, ANCOVA is a different name for regression with a categorical predictor of interest, and continuous predictor(s) that are included to improve our estimate of the effect of the predictor of interest. ANCOVA can enhance statistical power and help make more accurate (potentiall causal) inferences. However, the careful selection of covariates and an understanding of causal relationships are paramount to its proper implementation and interpretation.\n\n## Lecture\n\n\n\n\n\n\n{{< video https://www.youtube.com/embed/MewqUBfQYok >}}\n\n\n\n\n\n\n\n\n\n\n\n## Formative Test\n\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\n\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nWhat is a valid reason for using covariates in ANCOVA? <div class='webex-radiogroup' id='radio_VMKNPPNPXR'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VMKNPPNPXR\" value=\"\"></input> <span>To create more complex models</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VMKNPPNPXR\" value=\"\"></input> <span>To replace categorical predictors</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VMKNPPNPXR\" value=\"\"></input> <span>To increase the total variance</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VMKNPPNPXR\" value=\"answer\"></input> <span>To increase statistical power</span></label></div>\n\n\n**Question 2**\n\nWhat is an example of a 'good control' variable in ANCOVA? <div class='webex-radiogroup' id='radio_OCIPDDYAOE'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OCIPDDYAOE\" value=\"\"></input> <span>A variable unrelated to the predictor</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OCIPDDYAOE\" value=\"answer\"></input> <span>A confounder</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OCIPDDYAOE\" value=\"\"></input> <span>A variable related to the predictor</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OCIPDDYAOE\" value=\"\"></input> <span>A variable affected by the outcome</span></label></div>\n\n\n**Question 3**\n\nWhich of the following is an example of a 'bad control' variable in ANCOVA? <div class='webex-radiogroup' id='radio_SAIEUVHZAT'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_SAIEUVHZAT\" value=\"answer\"></input> <span>A variable affected by the outcome</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_SAIEUVHZAT\" value=\"\"></input> <span>A confounder</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_SAIEUVHZAT\" value=\"\"></input> <span>A variable that causes the predictor</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_SAIEUVHZAT\" value=\"\"></input> <span>A variable unrelated to the predictor or outcome</span></label></div>\n\n\n**Question 4**\n\nIn a randomized controlled experiment, are there benefits to controling for any observed differences between the two experimental groups? <div class='webex-radiogroup' id='radio_YJPSNSMLDG'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YJPSNSMLDG\" value=\"answer\"></input> <span>No, random assignment breaks the predictor&apos;s relationships with confounders</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YJPSNSMLDG\" value=\"\"></input> <span>Yes, controlling for relevant covariates is essential</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YJPSNSMLDG\" value=\"\"></input> <span>Only if the covariates are significantly related to the outcome</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YJPSNSMLDG\" value=\"\"></input> <span>Only if the covariates are significantly related to the predictor</span></label></div>\n\n\n**Question 5**\n\nWhat is the purpose of calculating adjusted means in ANCOVA? <div class='webex-radiogroup' id='radio_MKZTQYIDQF'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MKZTQYIDQF\" value=\"\"></input> <span>To create new variables</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MKZTQYIDQF\" value=\"answer\"></input> <span>To account for covariate effects</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MKZTQYIDQF\" value=\"\"></input> <span>To improve the model&apos;s fit</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MKZTQYIDQF\" value=\"\"></input> <span>To replace the original means</span></label></div>\n\n\n**Question 6**\n\nWhich of the following is a key factor in choosing appropriate covariates for ANCOVA? <div class='webex-radiogroup' id='radio_VHFTDYMGVT'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VHFTDYMGVT\" value=\"\"></input> <span>Covariates with the smallest p-values</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VHFTDYMGVT\" value=\"answer\"></input> <span>Causal relationships</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VHFTDYMGVT\" value=\"\"></input> <span>Covariates with the highest correlations</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VHFTDYMGVT\" value=\"\"></input> <span>Number of covariates</span></label></div>\n\n\n**Question 7**\n\nWhat is the distinguishing feature of ANCOVA relative to multiple regression? <div class='webex-radiogroup' id='radio_ONLXWIZASX'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ONLXWIZASX\" value=\"\"></input> <span>ANCOVA controls for confounding variables</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ONLXWIZASX\" value=\"\"></input> <span>Multiple regression includes only covariates</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ONLXWIZASX\" value=\"answer\"></input> <span>ANCOVA always includes a categorical predictor and control variable(s)</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ONLXWIZASX\" value=\"\"></input> <span>Multiple regression includes only continuous predictors</span></label></div>\n\n\n**Question 8**\n\nIn which of these situations should you avoid controlling a particular covariate in ANCOVA? <div class='webex-radiogroup' id='radio_TPQXPBFFEK'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TPQXPBFFEK\" value=\"\"></input> <span>When the covariate is perfectly correlated with another covariate</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TPQXPBFFEK\" value=\"answer\"></input> <span>When the covariate is a mediator</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TPQXPBFFEK\" value=\"\"></input> <span>When the covariate is irrelevant</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TPQXPBFFEK\" value=\"\"></input> <span>When the covariate is a confounder</span></label></div>\n\n\n**Question 9**\n\nAn ANCOVA model compares statistics grades for two classes of students, controlling for number of hours studied per week. The regression model is $\\text{Grade} = -1.27 + 0.50*D_{class2} + .30*Hours$. The unadjusted mean grades are 5.2 for class 1 and 7.3 for class 2. The mean hours studied were 21.56 for class 1 and 30.23 for class 2. The overall mean hours studied was 25.90. What are the adjusted means? <div class='webex-radiogroup' id='radio_DUNFCTLXGC'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DUNFCTLXGC\" value=\"answer\"></input> <span>6.50 and 6.00</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DUNFCTLXGC\" value=\"\"></input> <span>-1.27 and -1.77</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_DUNFCTLXGC\" value=\"\"></input> <span>9.35 and 2.97</span></label></div>\n\n\n:::\n\n\n<div class='webex-solution'><button>Show explanations</button>\n**Question 1**\n\nCovariates can be used in ANCOVA to reduce error variance caused by factors other than the predictor of interest.\n\n**Question 2**\n\nA confounder is a 'good control' variable in ANCOVA – a variable that influences both the predictor of interest and the outcome.\n\n**Question 3**\n\nA variable that is affected by the outcome is a 'bad control' variable in ANCOVA and can introduce biases.\n\n**Question 4**\n\n In a randomized controlled experiment, random assignment breaks the relationships between confounders and the treatment. Therefore, controlling for covariates related to the predictor is unnecessary.\n\n**Question 5**\n\nAdjusted means in ANCOVA are controlled for the effects of covariates, allowing us to understand how groups would differ on the outcome variable if they had equal scores on the covariate.\n\n**Question 6**\n\nCausal relationships are crucial when selecting covariates for ANCOVA. It's important to choose covariates that are related to the outcome and predictor in a meaningful way.\n\n**Question 7**\n\nANCOVA is one example of the broader category of models known as multiple regression; it is characterized by having a categorical predictor of interest and (continuous) covariates.\n\n**Question 8**\n\nAvoid controlling for covariates that are mediators, as controlling for mediators could lead to overcontrol bias and distort the relationships between the predictor and outcome.\n\n**Question 9**\n\n Use the formula $\\bar{Y}_g^{adj} = \\bar{Y}_g - b(\\bar{X}_g-\\bar{X})$\n\n\n</div>\n:::\n\n\n\n\n\n\n## Tutorial\n\n## Bivariate Regression (RECAP)\n\nResearchers are interested in the relationship between age and depression.\n\nThey hypothesized that older people are more vulnerable to depressive thoughts than younger people.\n\nTo test their research hypothesis, they collected data in a random sample of 164 persons from the general population. Open the dataset [`HADShealthyGroup.sav`](data/HADShealthyGroup.sav).\n\nRun a linear regression analysis using age as the independent variable and depression as the dependent variable.\n\nProceed as follows:\n\nNavigate to Analyze > Regression > Linear\n\nSelect the correct dependent and independent variable.\n\nPaste and run the syntax.\n \nHow much of the total variance in Depression is explained by Age? <input class='webex-solveme nospaces' data-tol='0.005' size='4' data-answer='[\"0.02\",\".02\"]'/>\n\nWhat can you say about the effect size? Would you say it’s a lot?\n\n\n<div class='webex-solution'><button>Answer</button>\n\nTo me, 2% of explained variance does not seem like a lot. There are probably better predictors of depression (i.e., predictors that explain more of the variance in depression).\n\n\n</div>\n\n\nTrue or false: The explained variance is significant at the 10% level. <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select> \n\n\n<div class='webex-solution'><button>Answer</button>\n\nTo conclude anything about the significance of the proportion explain variance of this model, we can look at the ANOVA table or ask SPSS to show the R2-change. This shows us that the explained variance in this model is not significant compared to an empty model (i.e. including no predictors), F(1,140) = 2.836, p = .094.\n \n\n\n</div>\n\n\nWrite down the estimated regression line using the unstandardized coefficients.\n\nY'= <input class='webex-solveme nospaces' data-tol='0.01' size='4' data-answer='[\"1.89\"]'/> + <input class='webex-solveme nospaces' data-tol='0.005' size='4' data-answer='[\"0.02\",\".02\"]'/> *Age\n\nHow can we interpret the constant?\n\n<div class='webex-radiogroup' id='radio_ILLKNTRPXE'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ILLKNTRPXE\" value=\"\"></input> <span>The predicted level of depression for the average age.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ILLKNTRPXE\" value=\"answer\"></input> <span>The predicted level of depression when age would be 0.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ILLKNTRPXE\" value=\"\"></input> <span>The average age in the sample.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ILLKNTRPXE\" value=\"\"></input> <span>The average level of depression.</span></label></div>\n\n\nConsult the table with the coefficients again\n\nTrue or false: We can conclude from this table that the effect of age is significant at the 10% level. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value='NA'>FALSE</option></select>\n\n\n<div class='webex-solution'><button>Answer</button>\n\nTo conclude whether the effect of Age on Depression is significant, we look at the t-test for the estimated coefficient.\n\nWe should conclude that the effect of Age on Depression is significant when using $\\alpha$=.10, t(140) = 1.684, p = .094.\n\n</div>\n\n\nOne of the assumptions of bivariate regression analysis is that the relationship between the independent and dependent variable is linear.\n\nState in your own words what this assumption entails.\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe assumption of a linear relationship entails that the relationship between the variables can be described with a straight line.\n\n\n</div>\n\n\n \nHow would you evaluate the assumption of linearity graphically? Do it for the data at hand.\n\nTrue or false: The relationship is linear. <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select>\n\nIf the assumption is not met, speculate about other possible relationships between age and depression.\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe scatter plot does not have the shape of a cigar, so it does not unambiguously suggest a linear relationship. Thus, you may doubt whether the relationship between age and depression is best described by a linear model.\nPerhaps the relationship is quadratic. Especially persons in middle ages may be vulnerable to depressive thoughts.\nNext to the plot, you find both the estimated linear trend and a non-linear trend. It seems that the quadratic curve fits better with the data. Moreover, the quadratic model explains 6% of the variance, whereas the linear model only 2%.\n\n</div>\n\n\nRun a regression analysis using Age as the independent variable and Anxiety as the dependent variable.\n\nSummarize the results.\n\nInclude in your answer the proportion of explained variance (R-square), a description of the effect based on the estimated regression coefficients, and evaluate the significance of the effect of age on anxiety.\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe proportion explained variance in Anxiety by Age is .071. The explained variance in this model is not significant compared to an empty model (i.e. including no predictors), F(1,140) = 0.710, p = .401.\n\nThe effect is Age on Anxiety is negative ($\\beta$ = -0.013), meaning that anxiety decreases with age.\nHowever, the effect of Age on Anxiety is not significant when using $\\alpha$=.05, t(140) = -0.842, p = .401.\n\n</div>\n\n\n## ANOVA (RECAP)\n\nConsider the following hypothetical research situation…\n\nResearchers are interested in effects of stereotyping on cognitive performance. For their research they performed a quasi-experiment. They selected three schools and asked girls from eight grade to do a math test. However, the teacher in School A says that boys do particularly well on the test (i.e., negative stereotyping for girls). In School B the teachers says that girls do particularly well on the test (i.e., positive stereotyping for girls). In the third school, the teacher gives gender-neutral information (control group).\n\nAfterwards the researchers compare the average math grades across the three groups. Because the schools may also differ in the student population, researchers also measured scholastic aptitude and use that as a covariate in the analysis.\n\nOpen the dataset [`stereotyping.sav`](data/stereotyping.sav).\n\nIn your own words, explain what a covariate is and give two examples of covariates that should/can be included in neuro-psychological research.\n\n\n<div class='webex-solution'><button>Answer</button>\n\nCovariates are \"nuissance\" variables that we are not directly interested in, but that allow us to better estimate the effect of another variable of interest.\n\nThis is related to causal inference.\n\nWe must thus justify our covariates by reference to their putative causal role with relation to our predictor and outcome.\n\nA covariate that *causes* both our predictor and our outcome is a *confounder*, and should be controlled for. This sometimes happens in \"natural experiments\", where the factor is not randomly assigned to participants.\n\nA covariate that *causes* our outcome, but is unrelated to our predictor, can be controlled for to reduce error variance in the outcome and increase statistical power of the effect of interest. This typically happens in randomized controlled experiments, but it may also happen in natural experiments.\n\nAs a counter-example: A variable that *is caused by* our predictor and our outcome is a collider, and should never be controlled for! This will bias the effect of our predictor.\n\n</div>\n\n\nMention two often-used covariates in research.\n\n\n<div class='webex-solution'><button>Answer</button>\n\nGender and Age are two covariates that are often used in research.\n\n</div>\n \n\nLet’s start analyzing the math scores using an ANOVA, thus ignoring any covariates for now.\n\n* Compute the means across the three groups (Analyze à Compare means à Means.\n* Select MATH for the dependent list and STEREO as the independent.\n* Paste and run the syntax.\n* Inspect the table that displays the mean differences between the groups.\n\nWhat is the first impression of stereotyping that we have from the mean differences?\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe table shows that no stereotyping results in the lowest mean score on the math test. Positive stereotyping results in the highest mean score, and negative stereotyping is in between.\n\n</div>\n\n\nRun an ANOVA: (Analyze -> general linear model -> univariate). Select MATH as the dependent variable and STEREO as fixed factor.\n\nWrite down the null and alternative hypothesis of the ANOVA, then check your answer.\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nH0: $\\mu1 = \\mu2 = \\mu3$\nH1: not $\\mu1 = \\mu2 = \\mu3$\n\n\n</div>\n\n\nTrue or false: The effect of stereotyping is significant (use $\\alpha$=0.05). <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value='NA'>FALSE</option></select>\n  \n\n<div class='webex-solution'><button>Answer</button>\n\nYes, the F-test for the effect of Stereotyping on Math performance is significant, F(2,27) = 5.614, p = .009. Hence, we reject H0.\n\nWe have convincing evidence that, also at the population level, the means differ.\n\nIn other words, we have convincing evidence that the mean differences in math performance are not the result of sampling fluctuations but reflect true differences due to the manipulation (i.e. stereotyping).\n\n</div>\n\n \nHow large is the R-square? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.294\",\".294\"]'/>\n \n\nHow do you interpretat this value of the R-square?\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe R2 is 0.294.\n\nThis means that 29.4% of the variance in Math performance is explained by the Stereotyping.\n\n</div>\n\n\nThe R-square should be equal to:\n\n<div class='webex-radiogroup' id='radio_MJYCHJAELC'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MJYCHJAELC\" value=\"answer\"></input> <span>The ratio of the sum of squares for STEREO to the (corrected) total sum of squares.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MJYCHJAELC\" value=\"\"></input> <span>The ratio of the mean square for STEREO to the error mean square.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MJYCHJAELC\" value=\"\"></input> <span>The ratio of the sum of squares for STEREO to the error sum of squares.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MJYCHJAELC\" value=\"\"></input> <span>The ratio of the mean square for STEREO to the (corrected) total mean square.</span></label></div>\n\n\n\n \nTest whether there is an effect of stereotyping (regardless of whether it is positive or negative stereotyping).\n\n\n \n\nTrue or false: The effect of stereotyping (regardless of whether it is positive or negative stereotyping) is significant. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value='NA'>FALSE</option></select>\n\nReport Levene’s test, significance of the contrast that you tested, and an interpretation of the difference between the two means.\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe Levene’s test is not significant (p = .805), so there is no evidence for violation of the assumption of homoscedasticity.\n\n</div>\n\n \nTest the mean math score for each experimental group against the control group; that is, you have to test two planned contrasts.\n\nAre the means different when tested at an experiment-wise alpha of .05 and using a Bonferroni corrected alpha per test? Substantiate your answer.\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe contrast is significant at $\\alpha$ = .05.\n\nThese results suggest that stereotyping (positive/negative) results in better math performance than non-stereotyping.   \n\n</div>\n\n\n### ANCOVA\n\nThe next step is to add scholastic aptitude as a covariate in our analysis.\n\nIn the lecture, we considered two situations in which ANCOVA is used; one in which the covariate was not related to the grouping variable, and one in which the covariate is associated.\n\nWhich situation do we have in this study?\nTo answer the question, you need to ask for some additional statistics in SPSS.\n\n\n<div class='webex-solution'><button>Answer</button>\n\nWe can check if the covariate is associated with the experimental factor by inspecting the means of the experimental groups on the covariate, or by running a one-way ANOVA.\n\nThe covariate is associated with the grouping variable, and thus mean differences in math between the three groups may be confounded by mean differences in scholastic aptitude between the three groups.\n\n</div>\n\n\n#### Assumption or not?\n\nSome texts state that ANCOVA has an additional assumption, beyond those of multiple regression.\nThis \"extra assumption\" is the assumption of \"homogeneity in regression slopes\".\n\nThe assumption of homogeneity in regression slopes states that the within-group effect of the covariate is the same across groups. That is, the covariate does not interact with the grouping variable.\n\nFor example, in this assignment, the assumption would imply that the effect of scholastic aptitude is independent of the experimental condition.\n\nBut: ANCOVA is just a special name for multiple regression with a categorical predictor of interest and control variables that we're not interested in (\"nuissance variables\"). If ANCOVA is multiple regression, then how can it have differen assumptions than multiple regression?\n\nThe answer is that ANCOVA does **not** have different assumptions, but if we were to allow for interaction between the factor and covariate, we would simply no longer call the resulting model an ANCOVA.\n\nSo instead of saying \"ANCOVA has an assumption of homogeneity in regression slopes\", we could say: \"it is conventional to call a model ANCOVA if it contains a factor and some control variables, but no interaction\". \nOf course we can add interaction terms in such a model - but then we just call it multiple regression with an interaction, not ANCOVA.\n\nOmitting an interaction between the factor and the covariate means that we force the within-group effect of the covariate to be the same across levels of the factor. In this study, that means that we do not allow the effect of scholastic aptitude to depend on the experimental condition.\n\nBefore we carry out the actual ANCOVA, we can check whether this model is correctly specified, or whether we are missing a significant interaction. We can check whether there is a significant interaction effect using the following syntax:\n\n```\n    UNIANOVA SA BY STEREO WITH MATH\n      /METHOD=SSTYPE(3)\n      /INTERCEPT=INCLUDE\n      /PRINT=DESCRIPTIVE PARAMETER HOMOGENEITY\n      /CRITERIA=ALPHA(.05)\n      /DESIGN=STEREO MATH MATH*STEREO.\n```\n\nCopy and run the syntax. Go to the table \"Tests of Between-Subjects Effects\". Take a look at the row STEREO*MATH.\n\nTrue or false: There is a significant interaction effect. <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select> \n\nIf there is a significant interaction effect, what do we do?\n\n\n<div class='webex-solution'><button>Answer</button>\n\nRemember we can check for a significant interaction effect for two reasons:\n\n1. Our hypothesis is about the interaction effect\n2. As an assumption check for correct model specification (i.e., we're interested in main effects, but we want to make sure that we're not ignoring an interaction when we do). This is similar to checking for linearity.\n\nIn the first case, we should not be conducting ANCOVA at all; we should start with multiple regression with interaction, because the interaction effect is our effect of interest.\n\nIn the second case, we can do two things: We can change our analysis based on the results of the \"assumption check\" - but such data-dependent decisions increase the risk of overfitting and Type I errors. Alternatively, we can just report the results of our planned ANCOVA analysis, but mention in the discussion that we observed a significant interaction, which means that the model may have been misspecified. We can also report results from both models, and see if/how the conclusions change if we allow for interaction (this is called a *sensitivity analysis*).\n\n\n</div>\n\n\n#### Back to ANCOVA\n\nLet’s run an ANCOVA, proceed as follows:\n\n* Navigate to Analyze > General linear model ? Univariate\n* Select MATH as the dependent variable, STEREO as fixed factor, and SA as the covariate.\n* Also, via OPTIONS ask for the Parameter estimates.\nPaste and run the syntax.\n \n\nConsider the Tests of Between Subjects Effects table (henceforth referred to as the “ANCOVA table”) and the FF-test for the grouping factor (STEREO).\n\nWhat's the p-value for the overall test of model fit? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.117\",\".117\"]'/>\n\nWhat conclusions can be drawn from the F-test?\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe F-test is a test of the effect of Stereotyping on Math performance controlled for Scholastic aptitude. Conceptually, it tests whether differences in the adjusted means in Math performance are significant. Adjusted means are the means we would expect if the group had an average level of Scholastic aptitude.\n\nIn other words, it tests the differences in the hypothetical situation we would have had three groups that had exactly the same level of Scholastic aptitude.\n\nThe F-test is not significant, F(2,26) = 2.333, p = .117, which means that, controlled for Scholastic aptitude, we don’t have convincing evidence that Stereotyping had an effect on Match performance.\n\n\n</div>\n\n\nCompare the results ANOVA and ANCOVA. What important difference do we see and how would you explain those?\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe ANOVA suggested a significant effect, whereas once controlled for Scholastic aptitude (ANCOVA) the effect was no longer significant.\n\nThus, the mean differences between the experimental groups we saw before were indeed confounded with differences in Scholastic aptitude!\n\n</div>\n\n \nConsult the table parameter estimates.\n\nWhat is the regression slope for scholastic aptitude? <input class='webex-solveme nospaces' data-tol='0.01' size='4' data-answer='[\"0.47\",\".47\"]'/>\n\nExplain the meaning of estimated parameter for scholastic aptitude.\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe parameter estimate for Scholastic aptitude is 0.470. The effect is significant when using $\\alpha$ = .05.\n\nIt is the pooled within-group regression effect of Scholastic aptitude on Math performance, controlled for Stereotyping.\n\nThus, if Scholastic aptitude increases by one unit, the predicted Math score increases by .470 units, while controlling for Stereotyping.\n\n\n</div>\n\n \nBased on the Parameters Estimates and the group-specific means, compute the adjusted group means (for each of the groups!) on MATH for an average scholastic aptitude.\n\nWhat is the adjusted group mean for the group that received the Negative Stereotype manipulation? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"6.359\"]'/>\n\n$\\bar{Y}_k^{adj} = \\bar{Y}_k -b_w(\\bar{X}_k- \\bar{X})$\n\nTo use the formula, you need to know the group means on MATH (you computed them before), you have to know the group means and overall mean SA (you can compute them via means), and the regression effect which is given in the table with parameter estimates.\n\nCheck your adjusted means against this answer model:\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\n![](images/Figure13-1.png)\n\n</div>\n\n\nRerun the ANCOVA. Now via OPTIONS also ask for the estimated means. You do so by selecting stereo in the list of Display Means for (at the top of the menu). Look in the table Estimated Marginal Means and verify your answer to the previous question.\n\nWrite down in your own words – and as precise as possible – the meaning of adjusted means.\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe estimated marginal means (i.e., the adjusted means) are the group means if all groups would’ve had an average of 6.20 on the covariate.\n\n</div>\n\n\nFinally, we want to look at several effect size estimates.\n\nHow much of the variance in Math do SA and STEREO explain? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.421\",\".421\"]'/>\n\n \n\nControlled for SA, how much of the remaining variance in Math does STEREO explain? Use the formula mentioned in the lecture slides to calculate the partial $\\eta^2$: <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.152\",\".152\"]'/>\n\nVerify your answer by running the ANCOVA again. Now, in options, select the box Estimates if effect size.\n\nTrue or false: SPSS reports the same partial η2. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value='NA'>FALSE</option></select>\n\nCould you summarize your findings of the ANCOVA in a few brief sentences?\n\nMention the significance tests, (un)adjusted means and the effect size estimates.    \n\n",
    "supporting": [
      "ancova_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}