{
  "hash": "7c76337411b7cca5ed7a81d04d35b093",
  "result": {
    "engine": "knitr",
    "markdown": "# Dimension Reduction {#sec-factoranalysis}\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the previous section, we explored how multiple items can be used to measure a single underlying construct. Today, we will delve into three powerful techniques for reducing multiple items to a smaller number of variables: Principal Components Analysis (PCA), Exploratory Factor Analysis (EFA), and a little bit of Confirmatory Factor Analysis (CFA). Our focus will be on PCA and EFA, as they are particularly useful for understanding underlying structures in social science research.\nThis section will primarily discuss Principal Components Analysis (PCA) and Exploratory Factor Analysis (EFA). These techniques help us explore relationships among items and identify latent constructs that explain the observed patterns in data. They serve as effective tools for dimensionality reduction, enabling us to summarize complex datasets with a smaller set of variables. While we do introduce CFA and reflect on its relationship to EFA, a more in-depth discussion of CFA is beyond the scope of this course.\n\nThroughout this lecture, assume that we have k items and n participants. Let's now dive into the details of different data reduction methods.\n\n## Principal Components Analysis (PCA)\n\n\n\nPCA is a data rotation technique designed to transform original items into uncorrelated components. These components represent linear combinations of the original items. The primary goal of PCA is dimension reduction, where a small number of components are used to explain most of the variance in the items. This allows us to represent the variance in the items more efficiently. For instance, if ten items measure extraversion, and one component explains most of the variance, we can retain that one component and discard the remaining nine.\n\n## Exploratory Factor Analysis (EFA)\n\nUnlike PCA, EFA is a latent variable method that assumes that latent variables (factors) cause people's responses to the items. For example, extraversion may cause individuals to respond positively to questions about partying and socializing. EFA models the item covariance matrix as a function of a fixed number of factors. It is called \"exploratory\" because all items are allowed to load on (contribute to) all factors, without a predefined structure. In practice, well-constructed questionnaires will exhibit high loadings of items on one factor and low loadings on others.\n\n## Confirmatory Factor Analysis (CFA)\n\nConfirmatory Factor Analysis (CFA) tests a theory about the specific associations between latent variables and observed indicators. Unlike Principal Components Analysis (PCA) and Exploratory Factor Analysis (EFA), which are exploratory, CFA is a confirmatory approach that tests how well a hypothesized measurement model fits the data. In CFA, researchers specify a theoretical model that defines the relationships between observed variables and latent constructs (factors). These latent constructs are not directly measured but are assumed to explain the correlations among the observed variables. The primary goal of CFA is to evaluate whether the data support the hypothesized model. By doing so, researchers can determine if their theoretical model fits the observed data well, providing evidence for the validity of the underlying construct and the measurement instrument. CFA is part of a family of statistical modeling techniques known as \"Structural Equation Modeling\" (SEM).\n\n## Comparing Method\n\nPurpose:\n\n* PCA: Dimensionality reduction.\n* EFA: Exploration of relationships among items and identification of latent constructs.\n* CFA: Testing a predefined theory about which items relate to specific latent constructs.\n\nAssumption:\n\n* PCA: Does not assume latent variables; dropping components assumes they are irrelevant or represent error variance.\n* EFA: Assumes all items are caused by a smaller number of latent variables (factors).\n* CFA: Assumes specific items are caused by specific latent variables.\n\nInterpretation:\n\n* PCA: Components are mathematical constructs with no further meaning.\n* EFA: Factors represent theoretical latent constructs.\n* CFA: Factors represent known theoretical latent constructs.\n\n## Principal Components Analysis\n\nPCA is a data rotation technique that aligns the largest amount of variance with the first component, the second-largest variance with the second component, and so on. These components are uncorrelated by definition, and they serve as linear combinations of the original items. The primary use of PCA is dimension reduction by retaining only components that explain a significant amount of variance, thus providing a lower-dimensional representation of the data.\n\nWe can understand PCA in different ways. Firstly, as rotation of the data. PCA rotates the data so that the first component best reproduces the correlation matrix, and each subsequent component improves the reproduction. Secondly, we can understand PCA as a way to summarize k items using fewer than k components, without significant information loss (lossy compression of data).\n\nSelecting the Number of Components:\n\nVarious strategies exist to determine the number of components to retain, including Kaiser's criterion (Eigenvalue > 1), Cattell's scree plot (inflection point), and Horn's Parallel Analysis (comparison with random data's Eigenvalues). Additionally, theoretical knowledge about the underlying data can guide the choice of components.\n\nInterpreting PCA Loadings:\n\nInterpreting PCA loadings can be challenging, especially in cases where multiple components are correlated. Orthogonal rotation, such as Varimax, can be employed to simplify the pattern of loadings and improve interpretability. However, it is essential to remember that rotated loadings should not be directly interpreted as correlations between items and factors as in PCA.\n\n## Exploratory Factor Analysis\n\nEFA is a model-based approach that assumes the existence of latent variables that cause item responses. It is suitable when you expect clusters of items to be correlated (multicollinear) and seeks to explain correlations between items. EFA assumes that unexplained variance in the items can be attributed to measurement error. This aligns with test theory, where it is assumed that observed items measure latent constructs with error. EFA is particularly suitable when there is a theoretical basis for assuming the existence of latent variables, such as when developing a new questionnaire that has not been validated yet. However, if a theoretical model already exists, Confirmatory Factor Analysis (CFA) may be more appropriate.\n\nTo conduct EFA, we estimate the unknown factor loadings. Two common estimation methods are Principal Axis Factoring (PAF) and Maximum Likelihood (ML). PAF is a default method in SPSS and is based on an iterative procedure involving matrix algebra. It provides a solution even when the model is complex or the data are non-normal. On the other hand, ML is the same estimator used for CFA and works well when the data are multivariate normal. However, ML may not perform well when the model is overly complex (which is not necessarily a bad thing). ML estimation also allows for a test of model fit, which is useful for evaluating the appropriateness of the chosen model.\n\nFactor loadings represent the correlations between each item and the extracted factors. They indicate the strength and direction of the relationship between the observed item and the underlying factor. Factor loadings range from -1 to +1, with values closer to 1 indicating a stronger relationship. In our example, we can see the factor loadings in a factor matrix, where each row corresponds to an item and each column corresponds to a factor. The factor loadings help us identify which items load more strongly on specific factors.\n\nWe can compute Eigenvalues in EFA just as in PCA by taking the column sums of the squared loadings and indicate the amount of variance explained by each factor. Eigenvalues are always smaller than the initial eigenvalues obtained in Principal Component Analysis (PCA) because some variance is now attributed to error variance. Consequently, the sum of the Eigenvalues is also less than the number of indicators, and some Eigenvalues may even be negative.\n\nSimilarly, communalities in EFA are always < 1 because EFA assumes the existence of error variance.\n\n### Selecting the Number of Factors\n\nDetermining the appropriate number of factors to extract is a critical step in EFA.\nResearchers often use eigenvalues as a cue to determine the number of factors to extract, similar to the Kaiser's criterion and Scree plot used in PCA - but note that in EFA, this can be misleading as Eigenvalues now depend on the number of extracted factors. Also, by default, SPSS applies Kaiser's criterion and the Scree plot to PCA Eigenvalues, even if you request EFA!\n\nAn alternative criterion for determining the number of factors is using theoretical knowledge to guide the decision. For example, if emotions are believed to break down into positive and negative emotions, we may choose to extract two factors. Additionally, the chi-square test can be used to evaluate the appropriateness of different factor solutions and assist in selecting the best-fitting model. To directly compare models, one can compute the Bayesian Information Criterion (BIC) - a relative model fit index designed for comparing models, which balances model fit and complexity. It is computed from the chi square as follows:\n\n$$\nBIC = \\chi^2 - df ∗ log(n)\n$$\n\n\n## EFA Assumption Checks\n\nBefore conducting exploratory factor analysis (EFA), it is good practice to perform several assumption checks to ensure the validity and appropriateness of the analysis. One critical aspect to consider is multicollinearity. While factor analysis aims to identify clusters of items that are correlated, excessive multicollinearity can lead to issues. This occurs when multiple items are perfectly linearly dependent, meaning that one item's score can be exactly reproduced using other variables. In such cases, it becomes difficult to discern the unique contribution of collinear items to the underlying factor model. To detect multicollinearity, researchers can examine the determinant, a value between 0 and 1. It has been argued that the determinant should be greater than 0.00001, which indicates multicollinearity is not too high.\n\nAnother assumption check for EFA is the proportion of common variance among items. The Kaiser-Meyer-Olkin (KMO) statistic provides an estimate of this proportion. A higher KMO value indicates that more of the variance among items can be explained by common factors, making the data more suitable for factor analysis. Researchers can interpret the KMO value as follows:\n\nValue | Interpretation\n------|----------------\n0.00 to 0.49 | unacceptable\n0.50 to 0.59 | miserable\n0.60 to 0.69 | mediocre\n0.70 to 0.79 | middling\n0.80 to 0.89 | meritorious\n0.90 to 1.00 | marvelous\n\n\n\n## Rotating Factor Loadings\n\nIn factor analysis, we aim to interpret the underlying structure of observed variables. The pattern of factor loadings is crucial in this process, helping us identify items that load highly on specific factors and potentially naming those factors based on high-loading indicators. In a perfect world, factor loadings would be clear and straightforward, with each item loading highly on only one factor. However, real-life factor loadings are not always so clear-cut, making interpretation more challenging.\n\nTo improve interpretability, we use rotation, which applies a linear transformation to the original factor loadings. Two main types of rotation are orthogonal and oblique rotation. Orthogonal rotation produces uncorrelated factors. The most common technique is VARIMAX rotation, which maximizes the variance of the squared loadings within each factor. Oblique rotation allows factors to correlate; the most common technique is oblimin rotation. In the social sciences, it is often sensible to allow factors to correlate (e.g., different personality dimensions are probably associated).\n\nOne-Factor EFA and One-Factor CFA:\n\nAlthough this course is not about confirmatory factor analysis, it is nevertheless useful to know that a one-factor EFA model is identical to a one-factor CFA model. In other words, if our theory implies a one-factor model, we can use exploratory factor analysis (EFA) with maximum likelihood (ML) estimation to test that model. While EFA aims to identify underlying factors without any preconceived hypotheses about their association with items, CFA tests a hypothesized model - in this case, that one factor explains all item scores. CFA with ML estimation produces a chi-square test that can be used to assess model fit. Note, however, that this test can be sensitive to sample size and may reject good models. Researchers can also use the Root Mean Square Error of Approximation (RMSEA) as an alternative model fit index, where values below 0.08 indicate good fit. RMSEA is calculated from the chi square as:\n\n$$\nRMSEA = \\frac{\\sqrt{\\chi^2 - df}}{\\sqrt{(n - 1)*df}}\n$$\n\nTreating a one-factor EFA as CFA also allows us to estimate latent variable reliability. Recall that Cronbach's alpha assumes that all items are equally important. This means that it assumes that all factor loadings are the same. Factor analysis tests this assumption. Especially when factor loadings differ, it may be useful to compute latent variable reliability instead, using McDonald's Omega (or composite reliability). It allows for different factor loadings, making it more appropriate for cases where items have varying contributions to the latent variable. The formula for McDonald's Omega is:\n\n$$\n\\omega = \\frac{SSL}{SSL+SSR} = \\frac{\\text{Sum of Squared Loadings}}{SSL + \\text{Sum of  Squared Residuals}}\n$$\n\nCalculate SSL as: $SSL = (\\sum_{j=0}^k L_{1,k})^2$ (first sum loadings, then square sum)\n\nCalculate SSR as: $SSR = 1-\\sum_{j=0}^k L_{1,k}^2$ (first square loadings, then sum)\n\n## Estimating Factor Scores\n\nIn many cases, researchers want to conduct further analyses using idividuals' scores on components or latent variables. In previous sections, we learned about two common methods for obtaining scale scores from multiple items: sum scores and mean scores. In sum scores, we add up the responses from each item to create a total score for each individual. Similarly, in mean scores, we take the average of the responses from all items to obtain a score. In both cases, all items contribute equally to the final scale score. However, this approach assumes that all items are equally important, which might not always be the case. PCA and EFA both allow us to determine whether items are indeed equally important. We can also try to compute scale scores that take differences in item loadings into account.\n\nFor PCA, computing such scores is straightforward; these are simply given by multiplying the loadings for one component with the observed item scores. Since this is not a latent variable technique, there is only one possible solution to this calculation. To compute a PCA score for a specific individual, we multiply their standardized item scores by the corresponding factor loadings and then sum the results. For instance, if an individual has standardized item scores of 1, 3, and 2 on items with factor loadings of 0.85, 0.80, and 0.14, respectively, their PCA score would be calculated as $(0.85 * 1 + 0.80 * 3 + 0.14 * 2) / (0.85^2 + 0.80^2 + 0.14^2) = 2.44$. This score represents the individual's relative level on the component.\n\nEstimating latent variable scores in exploratory factor analysis (EFA) is more complex compared to PCA. Unlike PCA, which provides unique factor scores for each individual, EFA does not uniquely determined factor scores. An infinite number of latent variable datasets is consistent with the same EFA model. To estimate factor scores, researchers use methods like the regression method and the Bartlett method. The regression method involves ordinary least squares estimates and aims to maximize the multiple correlation between factor scores and common factors. However, these estimates are biased and the estimated factor scores correlate with one another and with the different latent variables. The Bartlett method produces factor scores that only correlate with their own latent variable but still correlate with estimated scores for other factors. Both methods thus have shortcomings. Some (see references below) have argued that it might be preferable to simply use mean scores instead of factor scores. In cases where factor loadings are approximately equal, this is probably fine.\n\nFurther reading:\n\nEveritt, B. S., & Howell, D. C. (2005). Encyclopedia of Statistics in Behavioral Science. DOI:10.1002/0470013192.bsa726\nDiStefano, C., Zhu, M., & Mindrila, D. (2009). Understanding and Using Factor Scores: Considerations for the Applied Researcher. DOI:10.7275/da8t-4g52\n\n\n\n# Lecture\n\n\n\n\n\n\n{{< video https://www.youtube.com/embed/EF2Jcsh4OqA >}}\n\n\n\n\n\n\n\n\n\n\n# Formative Test\n\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\n\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nWhat is the primary goal of Principal Components Analysis (PCA)? <div class='webex-radiogroup' id='radio_HQJFXNBWUG'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HQJFXNBWUG\" value=\"\"></input> <span>Model confirmation</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HQJFXNBWUG\" value=\"\"></input> <span>Factor extraction</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HQJFXNBWUG\" value=\"answer\"></input> <span>Dimension reduction</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_HQJFXNBWUG\" value=\"\"></input> <span>Data classification</span></label></div>\n\n\n**Question 2**\n\nIn Exploratory Factor Analysis (EFA), what is the role of latent variables? <div class='webex-radiogroup' id='radio_TPGDXBKLBH'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TPGDXBKLBH\" value=\"\"></input> <span>They are directly measured</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TPGDXBKLBH\" value=\"\"></input> <span>They represent linear combinations of original items</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TPGDXBKLBH\" value=\"answer\"></input> <span>They cause people&apos;s responses to the items</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TPGDXBKLBH\" value=\"\"></input> <span>They confirm pre-defined structures</span></label></div>\n\n\n**Question 3**\n\nWhat distinguishes Confirmatory Factor Analysis (CFA) from PCA and EFA? <div class='webex-radiogroup' id='radio_KDGYZLMOQI'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_KDGYZLMOQI\" value=\"answer\"></input> <span>It tests a hypothesized measurement model</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_KDGYZLMOQI\" value=\"\"></input> <span>It is used for dimension reduction</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_KDGYZLMOQI\" value=\"\"></input> <span>It is an exploratory method</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_KDGYZLMOQI\" value=\"\"></input> <span>It assumes that factors cause item responses</span></label></div>\n\n\n**Question 4**\n\nHow are components ordered in PCA? <div class='webex-radiogroup' id='radio_TJTAZIKGEN'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TJTAZIKGEN\" value=\"\"></input> <span>Aligns smallest variance with the first component</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TJTAZIKGEN\" value=\"\"></input> <span>Equally distributes variance across all components</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TJTAZIKGEN\" value=\"answer\"></input> <span>Largest variance with the first component, second-largest with the second, and so on</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TJTAZIKGEN\" value=\"\"></input> <span>Randomly assigns variance to components</span></label></div>\n\n\n**Question 5**\n\nWhat is one way to understand PCA? <div class='webex-radiogroup' id='radio_OWMFPEMNUL'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OWMFPEMNUL\" value=\"answer\"></input> <span>As a method of lossy compression of data</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OWMFPEMNUL\" value=\"\"></input> <span>As a technique for classification of individuals</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OWMFPEMNUL\" value=\"\"></input> <span>As a method for data expansion</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OWMFPEMNUL\" value=\"\"></input> <span>As a process for data duplication</span></label></div>\n\n\n**Question 6**\n\nWhat is the purpose of using orthogonal rotation, like Varimax, in PCA? <div class='webex-radiogroup' id='radio_NHSRRAJPRQ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NHSRRAJPRQ\" value=\"\"></input> <span>To directly interpret loadings as correlations</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NHSRRAJPRQ\" value=\"\"></input> <span>To increase the correlation between items and factors</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NHSRRAJPRQ\" value=\"answer\"></input> <span>To simplify the pattern of loadings and improve interpretability</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NHSRRAJPRQ\" value=\"\"></input> <span>To reduce the number of components retained</span></label></div>\n\n\n**Question 7**\n\nWhat distinguishes Principal Axis Factoring (PAF) from Maximum Likelihood (ML) estimation? <div class='webex-radiogroup' id='radio_CCCFSSEIVX'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_CCCFSSEIVX\" value=\"answer\"></input> <span>PAF provides a solution even in complex models or non-normal data</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_CCCFSSEIVX\" value=\"\"></input> <span>ML is only suitable for CFA</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_CCCFSSEIVX\" value=\"\"></input> <span>ML always results in higher factor loadings</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_CCCFSSEIVX\" value=\"\"></input> <span>PAF allows for a test of model fit</span></label></div>\n\n\n**Question 8**\n\nHow are Eigenvalues computed differently in EFA compared to PCA? <div class='webex-radiogroup' id='radio_XQXIMEHMFL'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XQXIMEHMFL\" value=\"\"></input> <span>EFA does not compute Eigenvalues</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XQXIMEHMFL\" value=\"\"></input> <span>Eigenvalues are the same in both EFA and PCA</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XQXIMEHMFL\" value=\"answer\"></input> <span>Eigenvalues are smaller in EFA as some variance is attributed to error variance</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XQXIMEHMFL\" value=\"\"></input> <span>Eigenvalues are larger in EFA</span></label></div>\n\n\n**Question 9**\n\nWhat indicates a problem with multicollinearity in Exploratory Factor Analysis (EFA)? <div class='webex-radiogroup' id='radio_ASPFSAANAL'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ASPFSAANAL\" value=\"\"></input> <span>No presence of multicollinearity</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ASPFSAANAL\" value=\"\"></input> <span>A determinant value equal to 1</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ASPFSAANAL\" value=\"\"></input> <span>A high Kaiser-Meyer-Olkin (KMO) statistic</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ASPFSAANAL\" value=\"answer\"></input> <span>A determinant value lower than 0.00001</span></label></div>\n\n\n**Question 10**\n\nWhich of these is a method for estimating latent variable scores in EFA? <div class='webex-radiogroup' id='radio_VYAOJMVCOD'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VYAOJMVCOD\" value=\"\"></input> <span>Using mean scores of all items</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VYAOJMVCOD\" value=\"answer\"></input> <span>Regression method</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VYAOJMVCOD\" value=\"\"></input> <span>Sum scores</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VYAOJMVCOD\" value=\"\"></input> <span>Assigning equal weights to all items</span></label></div>\n\n\n:::\n\n\n<div class='webex-solution'><button>Show explanations</button>\n**Question 1**\n\nThe primary goal of PCA is dimension reduction, where a small number of components are used to explain most of the variance in the items.\n\n**Question 2**\n\nIn EFA, latent variables (factors) are assumed to cause people's responses to the items, unlike PCA where components represent linear combinations of original items.\n\n**Question 3**\n\nCFA is a confirmatory approach that tests how well a hypothesized measurement model fits the data, unlike PCA and EFA, which are exploratory.\n\n**Question 4**\n\nPCA aligns the data such that the largest amount of variance is with the first component, and each subsequent component accounts for the next largest variance.\n\n**Question 5**\n\nPCA can be understood as a way to summarize k items using fewer than k components, which can be seen as a form of lossy compression of data.\n\n**Question 6**\n\nOrthogonal rotation, such as Varimax, is employed in PCA to simplify the pattern of loadings, making them easier to interpret.\n\n**Question 7**\n\nPAF is an iterative method suitable for complex models or non-normal data, whereas ML, used for both EFA and CFA, is better for data that are multivariate normal.\n\n**Question 8**\n\nIn EFA, Eigenvalues are always smaller than in PCA because some variance is attributed to error variance, leading to Eigenvalues that are less than the number of indicators and sometimes negative.\n\n**Question 9**\n\nIn EFA, a determinant value lower than 0.00001 indicates excessive multicollinearity, making it difficult to discern the unique contribution of collinear items to the factor model.\n\n**Question 10**\n\nIn EFA, factor scores can be estimated using methods like the regression method or the Bartlett method, each with its own advantages and shortcomings.\n\n\n</div>\n:::\n\n\n\n\n\n\n\n# Tutorial\n\n## PCA\n\nOpen the data file: [`emotions.sav`](data/emotions.sav).\n\nThe data file consists of data from the International College Survey 2001 (Diener and colleagues, 2001). In this survey, data on emotions was collected for 41 countries. The data you’ll analyze in this assignment is about norms for experiencing/expressing 12 emotions in Belgium.\n\n<!-- The questionnaire the participants filled out looked like this: -->\n\n<!-- ![](images/Figure10.png) -->\n\n\nLet’s look at the data. The first two columns contain the number of the participant and the nation, so you don’t need to include them in the analysis.\n\nTrue or false: There are missing data. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value='NA'>FALSE</option></select>\n\nSuppose we are only interested in reducing the number of dimensions of the data, which method would you use? <select class='webex-select'><option value='blank'></option><option value='answer'>Principal Component Analysis</option><option value=''>Path Analysis</option><option value=''>Explanatory Factor Analysis</option><option value=''>Confirmatory Factor Analysis</option></select> \n \n\nNavigate to Analyze → Dimension Reduction → Factor in SPSS\n\nIn the tab Extraction: choose the correct method.\n\nAlso enable the option Scree plot and specify which variables need to be included in the analysis.\n\nCheck the Options tab. Can you determine what method is used to deal with missing data?\n\n<div class='webex-radiogroup' id='radio_SYPIYZVOWQ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_SYPIYZVOWQ\" value=\"answer\"></input> <span>All cases with missing values are removed prior to analysis</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_SYPIYZVOWQ\" value=\"\"></input> <span>No action is taken</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_SYPIYZVOWQ\" value=\"\"></input> <span>All correlations are computed based on available data for that pair of variables</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_SYPIYZVOWQ\" value=\"\"></input> <span>All missing values are removed prior to analysis</span></label></div>\n\n\nPaste the syntax and run the analysis.\n\nTake a look at the output.\n\nWhat number of component have an Eigenvalue greater than 1 (Kaiser's criterion)? <input class='webex-solveme nospaces' size='1' data-answer='[\"3\"]'/>\n\nHow many components does the scree plot suggest?\n\n<input class='webex-solveme nospaces' size='1' data-answer='[\"2\"]'/>\n\nRedo the analysis with the number of components you need to retain according to the scree plot.\n\nYou can specify the number of components in the Extraction menu of the Factor Analysis window.\n\nClick Fixed number of factors and enter the number of components (2).\n\nRun the analysis and look at the loadings in the Component matrix.\n\nTrue or false: This solution is easy to interpret. <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select>\n\nThe two principal components seem to correspond with positive emotions (appropriate and valued), and negative emotions (inappropriate and not valued), but there is not enough simple structure (too many variables have a high loading on both components).\n\nTo aid interpretation, you could rotate the solution. Which type of rotation is most appropriate here? <select class='webex-select'><option value='blank'></option><option value=''>orthogonal</option><option value='answer'>oblique</option></select>\n\n\n<div class='webex-solution'><button>Answer</button>\n\nIt is unlikely that positive and negative emotions are uncorrelated! An oblique rotation seems by far the most sensible choice. \n\n\n</div>\n\n\nRegardless of your previous answer, redo the analysis and choose Direct Oblimin in the Rotation menu.\n\n\nTake a look at the component loadings in the Pattern matrix.\n\nWhich component would you label Positive Emotions? Number.. <input class='webex-solveme nospaces' size='1' data-answer='[\"2\"]'/>\n\nCompare the component loadings in the Pattern Matrix with the loadings in the Component Matrix.\n\nWe now observe that the loadings resemble a simple structure more closely than before the rotation: the low loadings are lower and the high loadings are higher.\n\nNote: Due to the oblique rotation, the loadings are no longer equal to item-component correlations.\n\nWhat is the correlation between the two rotated components? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.143\",\".143\"]'/>\n\n\nRedo the Principal Component Analysis again one last time to save the component scores in the data set. Open Scores in the Factor Analysis window, check the Save as variables checkbox. Have a look at these component scores (now added to your data set): these are the scores for each person on the two components.\n\nAlternatively, add this syntax:\n\n```\n  /SAVE REG(ALL)\n```\n\nWhat is the component score for the first person on the first component? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"1.937\"]'/>\n\nTake a look at the table Total Variance Explained.\n\nHow much of the variance do the two components together account for? <input class='webex-solveme nospaces' data-tol='0.1' size='6' data-answer='[\"51.575\"]'/>%\n\n\nWhat proportion of the variance in the item stress is accounted for by the two components? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.505\",\".505\"]'/>\n\nWhich item has the highest unicity? <select class='webex-select'><option value='blank'></option><option value=''>Anger</option><option value=''>Cheerful</option><option value=''>Happy</option><option value='answer'>Pride</option></select> \n\n## Exploratory Factor Analysis\n\nWe will move on to work with Exploratory Factor Analysis.\n\nFor this second assignment you will perform an Exploratory Factor Analysis (EFA) in SPSS on a set of 18 items. These items measure Tolerance and are part of the European Value Survey (EVS).\n\nDiscuss with your group when we decide to use Exploratory Factor Analysis and when we decide to use Principal Component Analysis.\n\n\n<div class='webex-solution'><button>Explanation</button>\n\nPCA is a data reduction technique. We use it when we want to summarize information in the items.\n\nEFA is used to identify latent variables underlying the measured items. EFA is typically used when a questionnaire has not been validated yet. When we use EFA, we usually do not know exactly which item belongs to which dimension (although we might have an idea based on our theory).\n\n\n</div>\n\n \n\nDiscuss with your group: When do we use Confirmatory Factor Analysis?\n\n\n<div class='webex-solution'><button>Explanation</button>\n\nCFA is used when we DO know which items belong to which dimension. With CFA we can then check whether the model that we have in mind corresponds with what we see in the data.\n \n\n\n</div>\n\n\n<!-- The Tolerance questionnaire is displayed in the figure below. -->\n\n<!-- Take a few minutes to read the items carefully. -->\n\n<!-- ![](images/Figure11.png) -->\n\n \n\nOpen the file [`evs.sav`](data/evs.sav) in SPSS.\n\n<!-- To check whether we are allowed to carry out factor analysis and to know how many factors we should select, we first run another quick PCA. -->\n\nSelect Factor via Analyze -> Dimension Reduction.\n\nWhich extraction method should we use if we want a test of model fit? <select class='webex-select'><option value='blank'></option><option value='answer'>Maximum Likelihood</option><option value=''>Principal Components Analysis</option><option value=''>Principal Axis Factoring</option><option value=''>Unweighted Least Squares</option></select>\n\nDrag all items of the tolerance scale (i.e., V225 - V2242) into the ‘items’ window.\nGo to Descriptives and select the options \"Coefficients\", \"Determinant\", and \"KMO and Bartlett’s test of sphericity\".\nThen, go to extraction and select \"unrotated factor solution\" and \"scree plot\".\nPaste and run the syntax.\n\nWhat is the Determinant? <input class='webex-solveme nospaces' data-tol='0.001' size='5' data-answer='[\"0.004\",\".004\"]'/>\n\nTrue or false: The determinant indicates that multicollinearity might be a problem for these data. <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select>\n\nThe factorability, as determined by the KMO index, is <select class='webex-select'><option value='blank'></option><option value='answer'>Marvelous</option><option value=''>Middling</option><option value=''>Mediocre</option></select>\n\nHow many factors would you want to select based on the scree plot? <input class='webex-solveme nospaces' size='1' data-answer='[\"2\"]'/>\n \nHow many factors would you want to select based on Kaiser’s criterion? <input class='webex-solveme nospaces' size='1' data-answer='[\"3\"]'/>\n\nWhat are the limitations of using these criteria?\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nBoth are based on eigenvalues computed for PCA, but you are performing EFA now.\n\nAlthough you can also compute eigenvalues for EFA, SPSS doesn't use those for the scree plot and Kaiser's criterion - and moreover, eigenvalues for EFA depend on the number of extracted factors, which defeats the purpose of using them to determine how many factors to extract.\n\nFurthermore, EFA is a theory-driven technique; it makes sense to use theory to determine how many factors to retain.\n\n\n</div>\n\n\nAssume that we're extracting two factors for now. Re-do your analysis with the appropriate number of factors.\n\nIn the tab Extraction: choose the number of factors you want to extract.\n\nIn the tab Rotation: Tick the box Direct Oblimin.\n\nIn the tab Options: The interpretation of the pattern matrix is easier if you suppress all coefficients in that table that are small (e.g., values < 0.30). To do so, click on options and ask SPSS to suppress the small coefficients.\n\nIn the tab Descriptives: Ask for the reproduced matrix.\n\nPaste and run the syntax.\n \n\nWhen we interpret the output of the factor analysis, we inspect 4 tables: the pattern matrix, the communalities, the factor correlation matrix, and the reproduced correlation matrix.\n\nWe will start with the pattern matrix.\n\nInspect the factor loadings in the pattern matrix.\n\nWhich item has the highest absolute factor loading on Factor 2? Type the variable label from the table: <input class='webex-solveme nospaces' size='7' data-answer='[\"divorce\"]'/> \n\nDecide for yourself: are the two factors clearly interpretable? Then check your answer.\n\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe solution almost follows a simple structure where each item loads on one factor. Only for the item Having casual sex do we see high factor loadings on both factors.\n\n</div>\n\n\nInspect the communalities table.\n\nHow much of the variance in the item “suicide” do the factors explain? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.317\",\".317\"]'/>\n\nCheck the correlations between the three factors.\n\nHow substantial is the correlation between the factors? <select class='webex-select'><option value='blank'></option><option value=''>weak</option><option value='answer'>moderate</option><option value=''>large</option></select>\n\nInspect the residual correlations.\n\nWhich residual correlation is most concerning? \n\n<div class='webex-radiogroup' id='radio_GELNHMLIKO'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_GELNHMLIKO\" value=\"answer\"></input> <span>Between Cheating on tax and Paying cash</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_GELNHMLIKO\" value=\"\"></input> <span>Between taking soft drugs and joyriding.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_GELNHMLIKO\" value=\"\"></input> <span>Between driving under the influence and claiming state benefits.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_GELNHMLIKO\" value=\"\"></input> <span>Between speeding over the limit and smoking in public places.</span></label></div>\n\n\nTake a look at the pattern matrix again.\n\nCan you think of a meaningful label for each of the factors? (Take into consideration whether the loadings are positive or negative). Then check your answer.\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nThere appears to be a distinction between legal and religious issues.\n\n\n</div>\n\n\n## Exploratory Factor Analysis II\n\nOpen the dataset called [`student_questionnaire.sav`](data/student_questionnaire.sav).\n\nIt contains data on moral judgment in a variety of domains of social life (variables whose names start with MACJ). Note that you need the variable `MACJ13_imputed`, not `MACJ13`.\n\n### Model Selection using the BIC\n\nWhen conducting EFA with ML estimation, we obtain a chi-square test of model fit that allows us to compute the BIC, a comparative fit index that can help us choose the number of factors that best balances model fit and complexity.\n\nRun an EFA analysis for 1-3 and 7-9 factors. Using syntax can help you do this easily - just copy-paste the basic syntax below four times and change the number of classes:\n\n```\nFACTOR\n  /VARIABLES MACJ1 MACJ2 MACJ3 MACJ4 MACJ5 MACJ6 MACJ7 MACJ8 MACJ9 MACJ10 MACJ11 MACJ12 MACJ13_imputed\n    MACJ14 MACJ15 MACJ16 MACJ17 MACJ18 MACJ19 MACJ20 MACJ21\n  /MISSING LISTWISE \n  /CRITERIA FACTORS(1) ITERATE(100)\n  /EXTRACTION ML\n  /ROTATION NOROTATE.\n```\n\nOpen a spreadsheet in Excel or Google Sheets, and copy-paste the chi-square values and degrees of freedom into the first two columns.\nObtain the number of (valid) observations using whatever procedure you want (for example, Descriptives).\n\nAssuming that you have used the first two columns, paste the following formula into the fourth column. Replace \"n\" with the number of valid observations. Drag the formula down to copy it the all cells in its column:\n\n`= A1 - B1 * LOG(n)`\n\nWhat is the BIC for 3 factors? <input class='webex-solveme nospaces' data-tol='0.01' size='11' data-answer='[\"83.39268271\"]'/>\n\nBased on the BIC, out of the set of models compared, which number of factors would you choose? <input class='webex-solveme nospaces' size='1' data-answer='[\"8\"]'/>\n\nTrue or false: This finding corresponds to the conclusion you would draw from the Scree plot. <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select>\n\nTrue or false: This finding corresponds to the conclusion you would draw from Kaiser's criterion. <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select>\n\nTrue or false: KMO suggests that there is insufficient common variance for factor analysis. <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select>\n\nTrue or false: The determinant suggests a potential problem with multicollinearity. This might be because there are so many similar items. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value='NA'>FALSE</option></select>\n\nIf I told you that the theory specified 7 factors, how many factors would you prefer? Explain why, then check your answer.\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nThe BIC for 7 factors is almost identical to the one for 8 factors. If theory dictates 7 factors, you might prefer to stick with 7, as the evidence for 8 factors is not overwhelmingly stronger.\n\n\n</div>\n\n\n### Latent Variable Reliability\n\nRegardless of your previous answer, perform EFA with one factor. Recall that this is equivalent to performing CFA with one factor.\n\nCronbach's alpha assumes that all items have equal factor loadings. Examine the factor loadings matrix.\n\nTrue or false: it looks like the factor loadings are indeed all equivalent. <select class='webex-select'><option value='blank'></option><option value='NA'>TRUE</option><option value='answer'>FALSE</option></select>\n\nCompute Cronbach's alpha for these items, and report the value: <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.931\",\".931\"]'/>\n\nCopy-paste the factor loadings into a spreadsheet.\n\nUse the spreadsheet function `=SUM()` to sum the loadings, then square the sum to get the SSL, $SSL = (\\sum_{j=0}^k L_{1,k})^2$\n\nCreate a new column with the squared factor loadings. Use the function `=A1^2` (assuming that cell A1 contains your first factor loading). Then sum these squared loadings to get the SSR, $SSR = 1-\\sum_{j=0}^k L_{1,k}^2$.\n\nFinally, calculate McDonald's Omega: \n\n$$\n\\omega = \\frac{SSL}{SSL+SSR}\n$$\n\nReport McDonald's Omega: <input class='webex-solveme nospaces' data-tol='0.01' size='11' data-answer='[\"0.952571482\",\".952571482\"]'/>\n\nNote that McDonald's Omega is larger than Cronbach's alpha. This is a rule; Cronbach's alpha underestimates reliability compared to McDonald's omega, and the underestimation becomes worse as the assumption of equal factor loadings is more violated.\n\n### Model Fit\n\nFinally, calculate the RMSEA model fit index for this one-factor model. The cutoff for acceptable fit is RMSEA < .08.\n\n$$\nRMSEA = \\frac{\\sqrt{\\chi^2 - df}}{\\sqrt{(n - 1)*df}}\n$$\n\nTrue or false: The one-factor model has acceptable fit. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value='NA'>FALSE</option></select>\n",
    "supporting": [
      "data_reduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}