{
  "hash": "b8d084dcf74352a827a7293b2d3606f3",
  "result": {
    "markdown": "# Dimension Reduction {#sec-factoranalysis}\n\n\n\n\n\nIn the previous section, we explored how multiple items can be used to measure a single underlying construct. Today, we will delve into three powerful techniques for reducing multiple items to a smaller number of variables: Principal Components Analysis (PCA), Exploratory Factor Analysis (EFA), and a little bit of Confirmatory Factor Analysis (CFA). Our focus will be on PCA and EFA, as they are particularly useful for understanding underlying structures in social science research.\nThis section will primarily discuss Principal Components Analysis (PCA) and Exploratory Factor Analysis (EFA). These techniques help us explore relationships among items and identify latent constructs that explain the observed patterns in data. They serve as effective tools for dimensionality reduction, enabling us to summarize complex datasets with a smaller set of variables. While we do introduce CFA and reflect on its relationship to EFA, a more in-depth discussion of CFA is beyond the scope of this course.\n\nThroughout this lecture, assume that we have k items and n participants. Let's now dive into the details of different data reduction methods.\n\n## Principal Components Analysis (PCA)\n\nPCA is a data rotation technique designed to transform original items into uncorrelated components. These components represent linear combinations of the original items. The primary goal of PCA is dimension reduction, where a small number of components are used to explain most of the variance in the items. This allows us to represent the variance in the items more efficiently. For instance, if ten items measure extraversion, and one component explains most of the variance, we can retain that one component and discard the remaining nine.\n\n## Exploratory Factor Analysis (EFA)\n\nUnlike PCA, EFA is a latent variable method that assumes that latent variables (factors) cause people's responses to the items. For example, extraversion may cause individuals to respond positively to questions about partying and socializing. EFA models the item covariance matrix as a function of a fixed number of factors. It is called \"exploratory\" because all items are allowed to load on (contribute to) all factors, without a predefined structure. In practice, well-constructed questionnaires will exhibit high loadings of items on one factor and low loadings on others.\n\n## Confirmatory Factor Analysis (CFA)\n\nConfirmatory Factor Analysis (CFA) tests a theory about the specific associations between latent variables and observed indicators. Unlike Principal Components Analysis (PCA) and Exploratory Factor Analysis (EFA), which are exploratory, CFA is a confirmatory approach that tests how well a hypothesized measurement model fits the data. In CFA, researchers specify a theoretical model that defines the relationships between observed variables and latent constructs (factors). These latent constructs are not directly measured but are assumed to explain the correlations among the observed variables. The primary goal of CFA is to evaluate whether the data support the hypothesized model. By doing so, researchers can determine if their theoretical model fits the observed data well, providing evidence for the validity of the underlying construct and the measurement instrument. CFA is part of a family of statistical modeling techniques known as \"Structural Equation Modeling\" (SEM).\n\n## Comparing Method\n\nPurpose:\n\n* PCA: Dimensionality reduction.\n* EFA: Exploration of relationships among items and identification of latent constructs.\n* CFA: Testing a predefined theory about which items relate to specific latent constructs.\n\nAssumption:\n\n* PCA: Does not assume latent variables; dropping components assumes they are irrelevant or represent error variance.\n* EFA: Assumes all items are caused by a smaller number of latent variables (factors).\n* CFA: Assumes specific items are caused by specific latent variables.\n\nInterpretation:\n\n* PCA: Components are mathematical constructs with no further meaning.\n* EFA: Factors represent theoretical latent constructs.\n* CFA: Factors represent known theoretical latent constructs.\n\n## Principal Components Analysis\n\nPCA is a data rotation technique that aligns the largest amount of variance with the first component, the second-largest variance with the second component, and so on. These components are uncorrelated by definition, and they serve as linear combinations of the original items. The primary use of PCA is dimension reduction by retaining only components that explain a significant amount of variance, thus providing a lower-dimensional representation of the data.\n\nWe can understand PCA in different ways. Firstly, as rotation of the data. PCA rotates the data so that the first component best reproduces the correlation matrix, and each subsequent component improves the reproduction. Secondly, we can understand PCA as a way to summarize k items using fewer than k components, without significant information loss (lossy compression of data).\n\nSelecting the Number of Components:\n\nVarious strategies exist to determine the number of components to retain, including Kaiser's criterion (Eigenvalue > 1), Cattell's scree plot (inflection point), and Horn's Parallel Analysis (comparison with random data's Eigenvalues). Additionally, theoretical knowledge about the underlying data can guide the choice of components.\n\nInterpreting PCA Loadings:\n\nInterpreting PCA loadings can be challenging, especially in cases where multiple components are correlated. Orthogonal rotation, such as Varimax, can be employed to simplify the pattern of loadings and improve interpretability. However, it is essential to remember that rotated loadings should not be directly interpreted as correlations between items and factors as in PCA.\n\n## Exploratory Factor Analysis\n\nEFA is a model-based approach that assumes the existence of latent variables that cause item responses. It is suitable when you expect clusters of items to be correlated (multicollinear) and seeks to explain correlations between items. EFA assumes that unexplained variance in the items can be attributed to measurement error. This aligns with test theory, where it is assumed that observed items measure latent constructs with error. EFA is particularly suitable when there is a theoretical basis for assuming the existence of latent variables, such as when developing a new questionnaire that has not been validated yet. However, if a theoretical model already exists, Confirmatory Factor Analysis (CFA) may be more appropriate.\n\nTo conduct EFA, we estimate the unknown factor loadings. Two common estimation methods are Principal Axis Factoring (PAF) and Maximum Likelihood (ML). PAF is a default method in SPSS and is based on an iterative procedure involving matrix algebra. It provides a solution even when the model is complex or the data are non-normal. On the other hand, ML is the same estimator used for CFA and works well when the data are multivariate normal. However, ML may not perform well when the model is overly complex (which is not necessarily a bad thing). ML estimation also allows for a test of model fit, which is useful for evaluating the appropriateness of the chosen model.\n\nFactor loadings represent the correlations between each item and the extracted factors. They indicate the strength and direction of the relationship between the observed item and the underlying factor. Factor loadings range from -1 to +1, with values closer to 1 indicating a stronger relationship. In our example, we can see the factor loadings in a factor matrix, where each row corresponds to an item and each column corresponds to a factor. The factor loadings help us identify which items load more strongly on specific factors.\n\nWe can compute Eigenvalues in EFA just as in PCA by taking the column sums of the squared loadings and indicate the amount of variance explained by each factor. Eigenvalues are always smaller than the initial eigenvalues obtained in Principal Component Analysis (PCA) because some variance is now attributed to error variance. Consequently, the sum of the Eigenvalues is also less than the number of indicators, and some Eigenvalues may even be negative.\n\nSimilarly, communalities in EFA are always < 1 because EFA assumes the existence of error variance.\n\n### Selecting the Number of Factors\n\nDetermining the appropriate number of factors to extract is a critical step in EFA.\nResearchers often use eigenvalues as a cue to determine the number of factors to extract, similar to the Kaiser's criterion and Scree plot used in PCA - but note that in EFA, this can be misleading as Eigenvalues now depend on the number of extracted factors. Also, by default, SPSS applies Kaiser's criterion and the Scree plot to PCA Eigenvalues, even if you request EFA!\n\nAn alternative criterion for determining the number of factors is using theoretical knowledge to guide the decision. For example, if emotions are believed to break down into positive and negative emotions, we may choose to extract two factors. Additionally, the chi-square test can be used to evaluate the appropriateness of different factor solutions and assist in selecting the best-fitting model. To directly compare models, one can compute the Bayesian Information Criterion (BIC) - a relative model fit index designed for comparing models, which balances model fit and complexity. It is computed from the chi square as follows:\n\n$$\nBIC = \\chi^2 - df ∗ log(n)\n$$\n\n\n## EFA Assumption Checks\n\nBefore conducting exploratory factor analysis (EFA), it is good practice to perform several assumption checks to ensure the validity and appropriateness of the analysis. One critical aspect to consider is multicollinearity. While factor analysis aims to identify clusters of items that are correlated, excessive multicollinearity can lead to issues. This occurs when multiple items are perfectly linearly dependent, meaning that one item's score can be exactly reproduced using other variables. In such cases, it becomes difficult to discern the unique contribution of collinear items to the underlying factor model. To detect multicollinearity, researchers can examine the determinant, a value between 0 and 1. It has been argued that the determinant should be greater than 0.00001, which indicates multicollinearity is not too high.\n\nAnother assumption check for EFA is the proportion of common variance among items. The Kaiser-Meyer-Olkin (KMO) statistic provides an estimate of this proportion. A higher KMO value indicates that more of the variance among items can be explained by common factors, making the data more suitable for factor analysis. Researchers can interpret the KMO value as follows:\n\nValue | Interpretation\n------|----------------\n0.00 to 0.49 | unacceptable\n0.50 to 0.59 | miserable\n0.60 to 0.69 | mediocre\n0.70 to 0.79 | middling\n0.80 to 0.89 | meritorious\n0.90 to 1.00 | marvelous\n\n\n\n## Rotating Factor Loadings\n\nIn factor analysis, we aim to interpret the underlying structure of observed variables. The pattern of factor loadings is crucial in this process, helping us identify items that load highly on specific factors and potentially naming those factors based on high-loading indicators. In a perfect world, factor loadings would be clear and straightforward, with each item loading highly on only one factor. However, real-life factor loadings are not always so clear-cut, making interpretation more challenging.\n\nTo improve interpretability, we use rotation, which applies a linear transformation to the original factor loadings. Two main types of rotation are orthogonal and oblique rotation. Orthogonal rotation produces uncorrelated factors. The most common technique is VARIMAX rotation, which maximizes the variance of the squared loadings within each factor. Oblique rotation allows factors to correlate; the most common technique is oblimin rotation. In the social sciences, it is often sensible to allow factors to correlate (e.g., different personality dimensions are probably associated).\n\nOne-Factor EFA and One-Factor CFA:\n\nAlthough this course is not about confirmatory factor analysis, it is nevertheless useful to know that a one-factor EFA model is identical to a one-factor CFA model. In other words, if our theory implies a one-factor model, we can use exploratory factor analysis (EFA) with maximum likelihood (ML) estimation to test that model. While EFA aims to identify underlying factors without any preconceived hypotheses about their association with items, CFA tests a hypothesized model - in this case, that one factor explains all item scores. CFA with ML estimation produces a chi-square test that can be used to assess model fit. Note, however, that this test can be sensitive to sample size and may reject good models. Researchers can also use the Root Mean Square Error of Approximation (RMSEA) as an alternative model fit index, where values below 0.08 indicate good fit. RMSEA is calculated from the chi square as:\n\n$$\nRMSEA = \\frac{\\sqrt{\\chi^2 - df}}{\\sqrt{(n - 1)*df}}\n$$\n\nTreating a one-factor EFA as CFA also allows us to estimate latent variable reliability. Recall that Cronbach's alpha assumes that all items are equally important. This means that it assumes that all factor loadings are the same. Factor analysis tests this assumption. Especially when factor loadings differ, it may be useful to compute latent variable reliability instead, using McDonald's Omega (or composite reliability). It allows for different factor loadings, making it more appropriate for cases where items have varying contributions to the latent variable. The formula for McDonald's Omega is:\n\n$$\n\\omega = \\frac{SSL}{SSL+SSR} = \\frac{\\text{Sum of Squared Loadings}}{SSL + \\text{Sum of  Squared Residuals}}\n$$\n\nCalculate SSL as: $SSL = (\\sum_{j=0}^k L_{1,k})^2$ (first sum loadings, then square sum)\n\nCalculate SSR as: $SSR = 1-\\sum_{j=0}^k L_{1,k}^2$ (first square loadings, then sum)\n\n## Estimating Factor Scores\n\nIn many cases, researchers want to conduct further analyses using idividuals' scores on components or latent variables. In previous sections, we learned about two common methods for obtaining scale scores from multiple items: sum scores and mean scores. In sum scores, we add up the responses from each item to create a total score for each individual. Similarly, in mean scores, we take the average of the responses from all items to obtain a score. In both cases, all items contribute equally to the final scale score. However, this approach assumes that all items are equally important, which might not always be the case. PCA and EFA both allow us to determine whether items are indeed equally important. We can also try to compute scale scores that take differences in item loadings into account.\n\nFor PCA, computing such scores is straightforward; these are simply given by multiplying the loadings for one component with the observed item scores. Since this is not a latent variable technique, there is only one possible solution to this calculation. To compute a PCA score for a specific individual, we multiply their standardized item scores by the corresponding factor loadings and then sum the results. For instance, if an individual has standardized item scores of 1, 3, and 2 on items with factor loadings of 0.85, 0.80, and 0.14, respectively, their PCA score would be calculated as $(0.85 * 1 + 0.80 * 3 + 0.14 * 2) / (0.85^2 + 0.80^2 + 0.14^2) = 2.44$. This score represents the individual's relative level on the component.\n\nEstimating latent variable scores in exploratory factor analysis (EFA) is more complex compared to PCA. Unlike PCA, which provides unique factor scores for each individual, EFA does not uniquely determined factor scores. An infinite number of latent variable datasets is consistent with the same EFA model. To estimate factor scores, researchers use methods like the regression method and the Bartlett method. The regression method involves ordinary least squares estimates and aims to maximize the multiple correlation between factor scores and common factors. However, these estimates are biased and the estimated factor scores correlate with one another and with the different latent variables. The Bartlett method produces factor scores that only correlate with their own latent variable but still correlate with estimated scores for other factors. Both methods thus have shortcomings. Some (see references below) have argued that it might be preferable to simply use mean scores instead of factor scores. In cases where factor loadings are approximately equal, this is probably fine.\n\nFurther reading:\n\nEveritt, B. S., & Howell, D. C. (2005). Encyclopedia of Statistics in Behavioral Science. DOI:10.1002/0470013192.bsa726\nDiStefano, C., Zhu, M., & Mindrila, D. (2009). Understanding and Using Factor Scores: Considerations for the Applied Researcher. DOI:10.7275/da8t-4g52\n\n\n\n# Lecture\n\n\n{{< video https://www.youtube.com/embed/EF2Jcsh4OqA >}}\n\n\n\n# Formative Test\n\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\n\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nIntroversion is normally distributed with a mean of 50 and a standard deviation of 10. What is the probability that the mean introversion level of a randomly selected group of 16 people is smaller than 52? Round the answer to 3 decimal places. <div class='webex-radiogroup' id='radio_ACUQXLWYBE'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ACUQXLWYBE\" value=\"\"></input> <span>0.655</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ACUQXLWYBE\" value=\"\"></input> <span>0.345</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ACUQXLWYBE\" value=\"\"></input> <span>0.045</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ACUQXLWYBE\" value=\"answer\"></input> <span>0.788</span></label></div>\n\n\n**Question 2**\n\nVariable X is not normally distributed in the population. Variable X has a population mean of 30 and a population standard deviation of 6. A random sample of N = 36 scores is drawn from the population for variable X. The sample mean is equal to 32. Which of the following statements about the sampling distribution of the sample means for this sample (n = 36) is incorrect? <div class='webex-radiogroup' id='radio_ONGXWNZPCW'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ONGXWNZPCW\" value=\"\"></input> <span>The sampling distribution of sample means is approximately normally distributed.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ONGXWNZPCW\" value=\"answer\"></input> <span>The mean of the sampling distribution of the sample means is equal to 32.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ONGXWNZPCW\" value=\"\"></input> <span>The standard deviation of the sampling distribution of sample means is equal to 1.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ONGXWNZPCW\" value=\"\"></input> <span>The standard error is smaller than the population standard deviation.</span></label></div>\n\n\n**Question 3**\n\nWhat does the sampling distribution represent? <div class='webex-radiogroup' id='radio_XQXDHAXPAJ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XQXDHAXPAJ\" value=\"answer\"></input> <span>A theoretical distribution of sample statistics</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XQXDHAXPAJ\" value=\"\"></input> <span>The distribution of raw data</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XQXDHAXPAJ\" value=\"\"></input> <span>The distribution of sample means</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_XQXDHAXPAJ\" value=\"\"></input> <span>The distribution of population parameters</span></label></div>\n\n\n**Question 4**\n\nWhich of the following statements about the sampling distribution is true? <div class='webex-radiogroup' id='radio_FBDSASFIUZ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_FBDSASFIUZ\" value=\"\"></input> <span>The sampling distribution is identical to the population distribution</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_FBDSASFIUZ\" value=\"answer\"></input> <span>The sampling distribution is centered around the population parameter</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_FBDSASFIUZ\" value=\"\"></input> <span>The sampling distribution is based on a single sample</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_FBDSASFIUZ\" value=\"\"></input> <span>The sampling distribution of a skewed variable is also skewed</span></label></div>\n\n\n**Question 5**\n\nWhat is the standard deviation of the sampling distribution called? <div class='webex-radiogroup' id='radio_TCTGZVJVZL'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TCTGZVJVZL\" value=\"\"></input> <span>Population standard deviation</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TCTGZVJVZL\" value=\"answer\"></input> <span>Standard error</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TCTGZVJVZL\" value=\"\"></input> <span>Variance</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_TCTGZVJVZL\" value=\"\"></input> <span>Bias</span></label></div>\n\n\n**Question 6**\n\nHow does sample size affect the shape of the sampling distribution? <div class='webex-radiogroup' id='radio_VVSYQIBNDJ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VVSYQIBNDJ\" value=\"\"></input> <span>Larger sample sizes increase the variability of sample statistics</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VVSYQIBNDJ\" value=\"answer\"></input> <span>Larger sample sizes result in smaller standard errors</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VVSYQIBNDJ\" value=\"\"></input> <span>Larger sample sizes have no impact on the sampling distribution</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_VVSYQIBNDJ\" value=\"\"></input> <span>Larger sample sizes make the sampling distribution more spread out</span></label></div>\n\n\n**Question 7**\n\nWhat is the probability that a sample mean falls within +/- 1 standard deviation of the population mean, assuming a normal distribution of sample means? <div class='webex-radiogroup' id='radio_OFXOBCCYGH'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OFXOBCCYGH\" value=\"\"></input> <span>0.95</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OFXOBCCYGH\" value=\"\"></input> <span>0.34</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OFXOBCCYGH\" value=\"answer\"></input> <span>0.68</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OFXOBCCYGH\" value=\"\"></input> <span>0.48</span></label></div>\n\n\n**Question 8**\n\nIf the standard deviation of the population is 10 and the sample size is 25, what is the standard error of the sample mean? <div class='webex-radiogroup' id='radio_ZJKKGVUWUQ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ZJKKGVUWUQ\" value=\"\"></input> <span>0.4</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ZJKKGVUWUQ\" value=\"answer\"></input> <span>2</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ZJKKGVUWUQ\" value=\"\"></input> <span>5</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ZJKKGVUWUQ\" value=\"\"></input> <span>0.2</span></label></div>\n\n\n**Question 9**\n\nWhat is the probability that the sample proportion falls within +/- 2 standard deviations of the population proportion, assuming a large sample size? <div class='webex-radiogroup' id='radio_YZKRTAQRNR'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YZKRTAQRNR\" value=\"answer\"></input> <span>0.95</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YZKRTAQRNR\" value=\"\"></input> <span>0.48</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YZKRTAQRNR\" value=\"\"></input> <span>0.68</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_YZKRTAQRNR\" value=\"\"></input> <span>0.34</span></label></div>\n\n\n**Question 10**\n\nIf the standard deviation of the population is 5 and the sample size is 50, what is the standard error of the sample mean? <div class='webex-radiogroup' id='radio_BYHGANYZQX'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_BYHGANYZQX\" value=\"\"></input> <span>0.141</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_BYHGANYZQX\" value=\"answer\"></input> <span>0.707</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_BYHGANYZQX\" value=\"\"></input> <span>0.0707</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_BYHGANYZQX\" value=\"\"></input> <span>0.283</span></label></div>\n\n\n:::\n\n\n<div class='webex-solution'><button>Show explanations</button>\n**Question 1**\n\nCalculate the standard error as 10/sqrt(16). Then, calculate the Z-score as (52-50)/SE. Find the right tailed probability of that Z-score, then calculate 1 minus that probability.\n\n**Question 2**\n\n The sampling distribution will be approximately normal because n >= 30. The SE is indeed 1, because sigma/sqrt(n) = 6 /sqrt(36) = 1. The SE is always smaller than sigma, because it is calculated as sigma divided by square root of n.\n\n**Question 3**\n\nThe sampling distribution represents the distribution of sample statistics, such as sample means or proportions, derived from multiple samples drawn from the same population. It provides insights into the variability and characteristics of these sample statistics.\n\n**Question 4**\n\nThe sampling distribution is centered around the population parameter.\n\n**Question 5**\n\nThe standard deviation of the sampling distribution is known as the standard error. It measures the average variability or spread of sample statistics around the population parameter, reflecting the precision of the estimation.\n\n**Question 6**\n\nLarger sample sizes result in smaller standard errors. As the sample size increases, the sampling distribution becomes more concentrated around the population parameter, leading to a decrease in the standard error. This implies that larger samples provide more precise estimates of the population parameter.\n\n**Question 7**\n\nThe probability that a sample mean falls within +/- 1 standard deviations of the population mean, assuming a normal distribution of sample means, is 68%. This is based on 'the empirical rule'.\n\n**Question 8**\n\nThe standard error of the sample mean is 2. The standard error can be calculated by dividing the standard deviation of the population by the square root of the sample size. In this case, it would be 10 / √25 = 2.\n\n**Question 9**\n\nThe key lesson here is that everything you learned about the sampling distribution also applies to other statistics than the mean, so according to the empirical rule, 95% of sample proportions will fall within +/- 2 standard deviation of the population proportion.\n\n**Question 10**\n\nThe standard error of the sample mean is SD/sqrt(n), so 5/sqrt(50) = .707\n\n\n</div>\n:::\n\n\n\n# Tutorial\n\n## PCA\n\nOpen the data file: emotions.sav.\n\nThe data file consists of data from the International College Survey 2001 (Diener and colleagues, 2001). In this survey, data on emotions was collected for 41 countries. The data you’ll analyze in this assignment is about norms for experiencing/expressing 12 emotions in Belgium.\n\n<!-- The questionnaire the participants filled out looked like this: -->\n\n<!-- ![](images/Figure10.png) -->\n\n\nLet’s look at the data. The first two columns contain the number of the participant and the nation, so you don’t need to include them in the analysis.\n\nTrue or false: There are missing data. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select>\n\nSuppose we are only interested in reducing the number of dimensions of the data, which method would you use? <select class='webex-select'><option value='blank'></option><option value=''>Path Analysis</option><option value=''>Confirmatory Factor Analysis</option><option value='answer'>Principal Component Analysis</option><option value=''>Explanatory Factor Analysis</option></select> \n \n\nNavigate to Analyze → Dimension Reduction → Factor in SPSS\n\nIn the tab Extraction: choose the correct method.\n\nAlso enable the option Scree plot and specify which variables need to be included in the analysis.\n\nCheck the Options tab. Can you determine what method is used to deal with missing data?\n\n<div class='webex-radiogroup' id='radio_OMXANNIABE'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OMXANNIABE\" value=\"\"></input> <span>No action is taken</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OMXANNIABE\" value=\"\"></input> <span>All missing values are removed prior to analysis</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OMXANNIABE\" value=\"\"></input> <span>All correlations are computed based on available data for that pair of variables</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OMXANNIABE\" value=\"answer\"></input> <span>All cases with missing values are removed prior to analysis</span></label></div>\n\n\nPaste the syntax and run the analysis.\n\nTake a look at the output.\n\nWhat number of component have an Eigenvalue greater than 1 (Kaiser's criterion)? <input class='webex-solveme nospaces' size='1' data-answer='[\"3\"]'/>\n\nHow many components does the scree plot suggest?\n\n<input class='webex-solveme nospaces' size='1' data-answer='[\"2\"]'/>\n\nRedo the analysis with the number of components you need to retain according to the scree plot.\n\nYou can specify the number of components in the Extraction menu of the Factor Analysis window.\n\nClick Fixed number of factors and enter the number of components (2).\n\nRun the analysis and look at the loadings in the Component matrix.\n\nTrue or false: This solution is easy to interpret. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n\nThe two principal components seem to correspond with positive emotions (appropriate and valued), and negative emotions (inappropriate and not valued), but there is not enough simple structure (too many variables have a high loading on both components).\n\nTo aid interpretation, you could rotate the solution. Which type of rotation is most appropriate here? <select class='webex-select'><option value='blank'></option><option value=''>orthogonal</option><option value='answer'>oblique</option></select>\n\n\n<div class='webex-solution'><button>Answer</button>\n\nIt is unlikely that positive and negative emotions are uncorrelated! An oblique rotation seems by far the most sensible choice. \n\n\n</div>\n\n\nRegardless of your previous answer, redo the analysis and choose Direct Oblimin in the Rotation menu.\n\n\nTake a look at the component loadings in the Pattern matrix.\n\nWhich component would you label Positive Emotions? Number.. <input class='webex-solveme nospaces' size='1' data-answer='[\"2\"]'/>\n\nCompare the component loadings in the Pattern Matrix with the loadings in the Component Matrix.\n\nWe now observe that the loadings resemble a simple structure more closely than before the rotation: the low loadings are lower and the high loadings are higher.\n\nNote: Due to the oblique rotation, the loadings are no longer equal to item-component correlations.\n\nWhat is the correlation between the two rotated components? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.143\",\".143\"]'/>\n\n\nRedo the Principal Component Analysis again one last time to save the component scores in the data set. Open Scores in the Factor Analysis window, check the Save as variables checkbox. Have a look at these component scores (now added to your data set): these are the scores for each person on the two components.\n\nAlternatively, add this syntax:\n\n```\n  /SAVE REG(ALL)\n```\n\nWhat is the component score for the first person on the first component? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"1.937\"]'/>\n\nTake a look at the table Total Variance Explained.\n\nHow much of the variance do the two components together account for? <input class='webex-solveme nospaces' data-tol='0.1' size='6' data-answer='[\"51.575\"]'/>%\n\n\nWhat proportion of the variance in the item stress is accounted for by the two components? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.505\",\".505\"]'/>\n\nWhich item has the highest unicity? <select class='webex-select'><option value='blank'></option><option value=''>Happy</option><option value=''>Anger</option><option value=''>Cheerful</option><option value='answer'>Pride</option></select> \n\n## Exploratory Factor Analysis\n\nWe will move on to work with Exploratory Factor Analysis.\n\nFor this second assignment you will perform an Exploratory Factor Analysis (EFA) in SPSS on a set of 18 items. These items measure Tolerance and are part of the European Value Survey (EVS).\n\nDiscuss with your group when we decide to use Exploratory Factor Analysis and when we decide to use Principal Component Analysis.\n\n\n<div class='webex-solution'><button>Explanation</button>\n\nPCA is a data reduction technique. We use it when we want to summarize information in the items.\n\nEFA is used to identify latent variables underlying the measured items. EFA is typically used when a questionnaire has not been validated yet. When we use EFA, we usually do not know exactly which item belongs to which dimension (although we might have an idea based on our theory).\n\n\n</div>\n\n \n\nDiscuss with your group: When do we use Confirmatory Factor Analysis?\n\n\n<div class='webex-solution'><button>Explanation</button>\n\nCFA is used when we DO know which items belong to which dimension. With CFA we can then check whether the model that we have in mind corresponds with what we see in the data.\n \n\n\n</div>\n\n\n<!-- The Tolerance questionnaire is displayed in the figure below. -->\n\n<!-- Take a few minutes to read the items carefully. -->\n\n<!-- ![](images/Figure11.png) -->\n\n \n\nOpen the file evs.sav in SPSS.\n\n<!-- To check whether we are allowed to carry out factor analysis and to know how many factors we should select, we first run another quick PCA. -->\n\nSelect Factor via Analyze -> Dimension Reduction.\n\nWhich extraction method should we use if we want a test of model fit? <select class='webex-select'><option value='blank'></option><option value='answer'>Maximum Likelihood</option><option value=''>Principal Components Analysis</option><option value=''>Principal Axis Factoring</option><option value=''>Unweighted Least Squares</option></select>\n\nDrag all items of the tolerance scale (i.e., V225 - V2242) into the ‘items’ window.\nGo to Descriptives and select the options \"Coefficients\", \"Determinant\", and \"KMO and Bartlett’s test of sphericity\".\nThen, go to extraction and select \"unrotated factor solution\" and \"scree plot\".\nPaste and run the syntax.\n\nWhat is the Determinant? <input class='webex-solveme nospaces' data-tol='0.001' size='5' data-answer='[\"0.004\",\".004\"]'/>\n\nTrue or false: The determinant indicates that multicollinearity might be a problem for these data. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n\nThe factorability, as determined by the KMO index, is <select class='webex-select'><option value='blank'></option><option value=''>Mediocre</option><option value='answer'>Marvelous</option><option value=''>Middling</option></select>\n\nHow many factors would you want to select based on the scree plot? <input class='webex-solveme nospaces' size='1' data-answer='[\"2\"]'/>\n \nHow many factors would you want to select based on Kaiser’s criterion? <input class='webex-solveme nospaces' size='1' data-answer='[\"3\"]'/>\n\nWhat are the limitations of using these criteria?\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nBoth are based on eigenvalues computed for PCA, but you are performing EFA now.\n\nAlthough you can also compute eigenvalues for EFA, SPSS doesn't use those for the scree plot and Kaiser's criterion - and moreover, eigenvalues for EFA depend on the number of extracted factors, which defeats the purpose of using them to determine how many factors to extract.\n\nFurthermore, EFA is a theory-driven technique; it makes sense to use theory to determine how many factors to retain.\n\n\n</div>\n\n\nAssume that we're extracting two factors for now. Re-do your analysis with the appropriate number of factors.\n\nIn the tab Extraction: choose the number of factors you want to extract.\n\nIn the tab Rotation: Tick the box Direct Oblimin.\n\nIn the tab Options: The interpretation of the pattern matrix is easier if you suppress all coefficients in that table that are small (e.g., values < 0.30). To do so, click on options and ask SPSS to suppress the small coefficients.\n\nIn the tab Descriptives: Ask for the reproduced matrix.\n\nPaste and run the syntax.\n \n\nWhen we interpret the output of the factor analysis, we inspect 4 tables: the pattern matrix, the communalities, the factor correlation matrix, and the reproduced correlation matrix.\n\nWe will start with the pattern matrix.\n\nInspect the factor loadings in the pattern matrix.\n\nWhich item has the highest absolute factor loading on Factor 2? Type the variable label from the table: <input class='webex-solveme nospaces' size='7' data-answer='[\"divorce\"]'/> \n\nDecide for yourself: are the two factors clearly interpretable? Then check your answer.\n\n\n\n<div class='webex-solution'><button>Answer</button>\n\nThe solution almost follows a simple structure where each item loads on one factor. Only for the item Having casual sex do we see high factor loadings on both factors.\n\n</div>\n\n\nInspect the communalities table.\n\nHow much of the variance in the item “suicide” do the factors explain? <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.317\",\".317\"]'/>\n\nCheck the correlations between the three factors.\n\nHow substantial is the correlation between the factors? <select class='webex-select'><option value='blank'></option><option value=''>weak</option><option value='answer'>moderate</option><option value=''>large</option></select>\n\nInspect the residual correlations.\n\nWhich residual correlation is most concerning? \n\n<div class='webex-radiogroup' id='radio_OXCXHZUOLY'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OXCXHZUOLY\" value=\"\"></input> <span>Between speeding over the limit and smoking in public places.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OXCXHZUOLY\" value=\"\"></input> <span>Between driving under the influence and claiming state benefits.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OXCXHZUOLY\" value=\"answer\"></input> <span>Between Cheating on tax and Paying cash</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OXCXHZUOLY\" value=\"\"></input> <span>Between taking soft drugs and joyriding.</span></label></div>\n\n\nTake a look at the pattern matrix again.\n\nCan you think of a meaningful label for each of the factors? (Take into consideration whether the loadings are positive or negative). Then check your answer.\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nThere appears to be a distinction between legal and religious issues.\n\n\n</div>\n\n\n## Exploratory Factor Analysis II\n\nOpen the dataset called \"student_questionnaire.sav\".\n\nIt contains data on moral judgment in a variety of domains of social life (variables whose names start with MACJ). Note that you need the variable `MACJ13_imputed`, not `MACJ13`.\n\n### Model Selection using the BIC\n\nWhen conducting EFA with ML estimation, we obtain a chi-square test of model fit that allows us to compute the BIC, a comparative fit index that can help us choose the number of factors that best balances model fit and complexity.\n\nRun an EFA analysis for 1-3 and 7-9 factors. Using syntax can help you do this easily - just copy-paste the basic syntax below four times and change the number of classes:\n\n```\nFACTOR\n  /VARIABLES MACJ1 MACJ2 MACJ3 MACJ4 MACJ5 MACJ6 MACJ7 MACJ8 MACJ9 MACJ10 MACJ11 MACJ12 MACJ13_imputed\n    MACJ14 MACJ15 MACJ16 MACJ17 MACJ18 MACJ19 MACJ20 MACJ21\n  /MISSING LISTWISE \n  /CRITERIA FACTORS(1) ITERATE(100)\n  /EXTRACTION ML\n  /ROTATION NOROTATE.\n```\n\nOpen a spreadsheet in Excel or Google Sheets, and copy-paste the chi-square values and degrees of freedom into the first two columns.\nObtain the number of (valid) observations using whatever procedure you want (for example, Descriptives).\n\nAssuming that you have used the first two columns, paste the following formula into the fourth column. Replace \"n\" with the number of valid observations. Drag the formula down to copy it the all cells in its column:\n\n`= A1 - B1 * LOG(n)`\n\nWhat is the BIC for 3 factors? <input class='webex-solveme nospaces' data-tol='0.01' size='11' data-answer='[\"83.39268271\"]'/>\n\nBased on the BIC, out of the set of models compared, which number of factors would you choose? <input class='webex-solveme nospaces' size='1' data-answer='[\"8\"]'/>\n\nTrue or false: This finding corresponds to the conclusion you would draw from the Scree plot. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n\nTrue or false: This finding corresponds to the conclusion you would draw from Kaiser's criterion. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n\nTrue or false: KMO suggests that there is insufficient common variance for factor analysis. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n\nTrue or false: The determinant suggests a potential problem with multicollinearity. This might be because there are so many similar items. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select>\n\nIf I told you that the theory specified 7 factors, how many factors would you prefer? Explain why, then check your answer.\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nThe BIC for 7 factors is almost identical to the one for 8 factors. If theory dictates 7 factors, you might prefer to stick with 7, as the evidence for 8 factors is not overwhelmingly stronger.\n\n\n</div>\n\n\n### Latent Variable Reliability\n\nRegardless of your previous answer, perform EFA with one factor. Recall that this is equivalent to performing CFA with one factor.\n\nCronbach's alpha assumes that all items have equal factor loadings. Examine the factor loadings matrix.\n\nTrue or false: it looks like the factor loadings are indeed all equivalent. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n\nCompute Cronbach's alpha for these items, and report the value: <input class='webex-solveme nospaces' data-tol='0.01' size='5' data-answer='[\"0.931\",\".931\"]'/>\n\nCopy-paste the factor loadings into a spreadsheet.\n\nUse the spreadsheet function `=SUM()` to sum the loadings, then square the sum to get the SSL, $SSL = (\\sum_{j=0}^k L_{1,k})^2$\n\nCreate a new column with the squared factor loadings. Use the function `=A1^2` (assuming that cell A1 contains your first factor loading). Then sum these squared loadings to get the SSR, $SSR = 1-\\sum_{j=0}^k L_{1,k}^2$.\n\nFinally, calculate McDonald's Omega: \n\n$$\n\\omega = \\frac{SSL}{SSL+SSR}\n$$\n\nReport McDonald's Omega: <input class='webex-solveme nospaces' data-tol='0.01' size='11' data-answer='[\"0.952571482\",\".952571482\"]'/>\n\nNote that McDonald's Omega is larger than Cronbach's alpha. This is a rule; Cronbach's alpha underestimates reliability compared to McDonald's omega, and the underestimation becomes worse as the assumption of equal factor loadings is more violated.\n\n### Model Fit\n\nFinally, calculate the RMSEA model fit index for this one-factor model. The cutoff for acceptable fit is RMSEA < .08.\n\n$$\nRMSEA = \\frac{\\sqrt{\\chi^2 - df}}{\\sqrt{(n - 1)*df}}\n$$\n\nTrue or false: The one-factor model has acceptable fit. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select>\n",
    "supporting": [
      "data_reduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}