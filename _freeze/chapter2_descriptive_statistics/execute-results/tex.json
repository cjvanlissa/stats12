{
  "hash": "0abf2b9f7e19b32247daa128a5aae988",
  "result": {
    "engine": "knitr",
    "markdown": "# Descriptive Statistics {#sec-descriptive}\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive statistics describe or summarize properties of data collected in a sample.\nIf you collect data on three variables for five participants, you could imagine that it is still feasible to print the entire dataset as a table, and to maintain the overview.\nAny time you collect data from more than just a handful of participants,\nhowever, this becomes unfeasible.\nInstead, we report descriptive statistics.\nWhile some descriptive statistics are *calculated in the same way* as inferential statistics (which are first introduced in the chapter on hypothesis testing),\nthe distinctive feature of descriptive statistics is that we use them to describe the data observed in a sample, and not to make claims about the larger population.\n\nDescriptive statistics are almost always computed, for a variety of reasons:\n\n1. To describe properties of the sample (e.g., the demographic composition)\n1. To check for mistakes in data entry; e.g., if the maximum value of the variable `age` is 124, the person who entered the data might have made a mistake\n1. To check assumptions of a particular statistical model, which we will cover in later chapters\n1. To answer research questions that do not require hypothesis tests, for example:\n    + In which country are most of our sales conducted?\n    + What is the most common major in my classroom?\n    + Based on data collected from all inhabitants of the Netherlands (i.e.: a census, not a sample), what is the average income?\n\n\n## Measures of Central Tendency {#sec-central}\n\nMeasures of central tendency are statistics that try to capture the \"most common\" value in a sample.\nThe most common measure of central tendency is the \"average\", which statisticians would call the \"mean\".\nAll measures of central tendency summarize the distribution of values of one particular variable as **one representative number**.\n\n### Mean: the \"average\" value\n\nThe most common measure of central tendency is the mean (or average).\nIt is computed by adding all observed scores, and dividing that total by the number of observations.\n\nAs a formula, this looks like:\n\n$\\bar{x} = \\frac{\\sum_{i=1}^{n}x_i}{n} = \\frac{x_1 + x_2 + ... + x_n}{n}$\n\nAn advantage of the mean is that every participant's score contributes to its value equally.\nThis also implies that it is sensitive to extreme values (also called: outliers).\nIf you calculate the mean income in a country where 99% of inhabitants live below the poverty level and 1% are ultra-rich oligarchs, then the mean income will make it look like, on average, people make good money.\nThis sensitivity to extreme values implies that the mean is a good description of the distribution of scores if the distribution is approximately symmetrical (i.e., about 50% of scores are above the mean, and 50% are below it).\n\nTake a look at the figures below; they show different possible distributions of scores in a sample.\nOn the X-axis is the number line.\nExact values are not given here because the important point is the *shape* of the distribution, but you can imagine that the X-axis is a scale of height from 150-210 centimeter, or a self-report questionnaire scale from 1-10.\nOn the Y-axis is the frequency with which each number is reported by the participants; a higher value on this axis means that this response is more common.\nThe red line indicates the location of the mean.\n\nNotice that the distributions labeled a-c are all symmetrical: In distribution a,\nscores cluster around one common value and quickly drop off when you get further away from that common value.\nIn distribution b, scores also cluster around one common value, but they drop off more gradually.\nIn distribution c, every value is exactly equally common.\nFor distributions a-c, the mean would be a good measure of central tendency - it gives you the middle of the distribution.\nHowever, also notice that the mean is a better representation of the \"most common\" value in distributions a-b, but not in distribution c.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(ggpubr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggpubr' was built under R version 4.5.1\n```\n\n\n:::\n\n```{.r .cell-code}\nbase <- ggplot() +\n    xlim(-5, 5) +\n    theme_void() +\n    theme(axis.title.x = element_text(), axis.title.y = element_text(angle = 90))+\n    labs(x = \"Value\", y = \"Frequency\") +\n  geom_vline(xintercept = 0, color = \"red\", linewidth = 2)\np1 <- base + geom_function(fun = dnorm)\np2 <- base + geom_function(fun = function(x){5-abs(x)})\np3 <- base + geom_function(fun = function(x){5})\np4 <- ggplot() +\n    xlim(0, 1) +\n    theme_void() +\n    theme(axis.title.x = element_text(), axis.title.y = element_text(angle = 90))+\n    labs(x = \"Value\", y = \"Frequency\") +\n    geom_vline(xintercept = .5, color = \"red\", linewidth = 2) +\n    geom_function(fun = dbeta, args = list(shape1 = 2, shape2 = 6)) \n\nfigure <- ggarrange(p1, p2, p3, p4,\n                    labels = letters[1:4],\n                    ncol = 4, nrow = 1)\nfigure\n```\n\n::: {.cell-output-display}\n![](chapter2_descriptive_statistics_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\n\n\n### Median: the middle milestone\n\nIf you were to order all scores of your variable from lowest to highest, then the median value is the value that splits your sample in half:\nhalf of the participants score lower than this value, and half score higher.\n\nAnother name for the median is the *50th percentile*. This just means that 50% of participants score lower than the median (and, by extension, 50% score higher).\n\nBased on the explanation of the mean, you might already realize that this value should be equal to that of the mean in a perfectly symmetrical distribution.\nIf there are outliers, though, the median is less strongly affected by them than the mean.\nWe can thus say that the median is a measure of central tendency that is *more robust to outliers* than the mean.\n\nThe median is not really \"calculated\", but it is found by literally sorting all values in order, and then picking the middle value (if you have an odd number of observations), or calculating the mean of the two middle values (if you have an even number of observations).\n\nIf our variable has these values (which are already ordered):\n\n```\n2, 3, 6, 7, 100\n```\n\nThen the median value is the middle value, $Med = 6$. Note that the outlier with value `100` does not really affect it (the mean for this sample would be much higher, $M = 23.6$).\n\nIf our variable has these values (which are already ordered):\n\n```\n1, 2, 3, 6, 7, 100\n```\n\nThen the median would be the mean of the middle two values: $(3+6) / 2 = 4.5$.\n\nBelow is the picture of the means again, but now, the location of the median is indicated with a blue line.\nNote that for the symmetrical distributions a-c, the median is identical to the mean - but for the asymmetrical distribution d, the median is a much better representation of the \"most common value\" than the mean is.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np1 <- p1 + geom_vline(xintercept = 0, color = \"blue\", linewidth = 2)\np2 <- p2 + geom_vline(xintercept = 0, color = \"blue\", linewidth = 2)\np3 <- p3  + geom_vline(xintercept = 0, color = \"blue\", linewidth = 2)\np4 <- p4 + geom_vline(xintercept = .17, color = \"blue\", linewidth = 2)\n\nfigure <- ggarrange(p1, p2, p3, p4,\n                    labels = letters[1:4],\n                    ncol = 4, nrow = 1)\nfigure\n```\n\n::: {.cell-output-display}\n![Location of the median in symmetrical and asymmetrical distributions.](chapter2_descriptive_statistics_files/figure-pdf/figmedians-1.pdf){fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\n\n### Mode: the most common value\n\nThe mode is the most common value in a sample.\nWe can calculate or find it by creating a frequency table, tabulating how often each score is observed in the sample, and then picking the score that occurs most frequently.\n\nWhile the mode can be obtained for variables with any measurement level, it is the only valid measure of \"central tendency\" for *nominal* data (e.g., sex, major, favourite color).\nThe other measures of central tendency are *not* valid for nominal data, because these lack a numerical value.\n\nAgain, note that in a perfectly symmetrical data distribution, the mode will be identical to the mean and the median.\n\nImagine, for example, that I have students from three majors:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Major                  | Frequency|\n|:----------------------|---------:|\n|Social Science         |        43|\n|Cognitive Neuroscience |        22|\n|Business & Economics   |        11|\n\n\n:::\n:::\n\n\n\n\n\n\nThe mode of the variable major, in this case, is \"Social science\". Do you see why we cannot calculate a mean or median for the variable major? Because the majors don't have a numerical value.\n\nHowever, it would be perfectly reasonable for me to say that the mode grade obtained last year was a 6.5. This implies that 6.5 was the most common grade, but it doesn't tell you how many students got that grade, or whether the average grade was above or below the level required to pass.\n\nVisually, the location of the mode is the same as the location of the median in plots a, b, and d in Figure \\@ref(fig:figmedians).\nPlot c does not have a mode; no score is more common than any other score.\n\n## Choosing a Measure of Central Tendency\n\nWhich measure to choose depends, in part, on the measurement level of the variable.\n\n* **Nominal:** Mode\n* **Ordinal:** Mode; if you calculate the mean or median, that means you assume that the distances between all categories are equal (i.e., you're treating your ordinal variable as interval).\n* **Interval/Ratio:** Mode, mean, and median\n\n\n## Measures of Dispersion {#sec-variability}\n\n\nMeasures of central tendency tell us what is a typical score;\nmeasures of dispersion tell *how typical* that score is.\nDispersion simply means variability, so from now on, we will use this term.\n\nHere are several measures of variability:\n\n### Range: full span\n\nThe range is the distance from the smallest to the largest value.\nYou calculate it by subtracting the smallest value from the largest;\nfor example, if your smallest value is 1 and the largest is 5, then the range is $5-1 = 4$.\n\nAs a formula, this looks like:\n\n$R = x_\\text{largest} - x_\\text{smallest}$\n\n\nSometimes, the range is also reported as an interval, $[1, 5]$, or as minimum and maximum values.\nThe range is an intuitive metric, but it is unstable because its value is fully determined by just two observations (the lowest and highest).\nThe variability of all of the other observations does not affect it.\n\n### Sum of Squared Distances to the Mean\n\nA metric of variability that does take all observations into account is the sum of squared distances to the mean - or \"sum of squares\".\n\nTo calculate it, follow these steps:\n\n1. Calculate the mean of all observations, e.g. if our observations are `1,2,3`, then $M = 2$\n1. For each observation, calculate the distance from that mean (subtract the mean), so for our observations `1,2,3`, we get $1-2 = -1$, $2-2 = 0$, and $3-2 = 1$.\n1. Square all these distances to get rid of negative values, so $-1^2 = 1$, $0^2 = 0$, $1^2 = 1$.\n1. Sum the squared distances, in this case $1 + 0 + 1 = 2$\n\nAs a formula, this looks like:\n\n$SS = \\sum_{i=1}^{n}(x_i - \\bar{x})^2$\n\nNote that if we would not square the distances, the sum would always be zero because the mean is mathematically in the middle of all scores, so the negative distances of values below the mean exactly cancel out the positive distances of values above the mean.\n\nThe sum of squares has several important properties.\nFirst, note that its value depends on the sample size: sums of squares of larger samples tend to be larger than those of smaller samples.\nSecond, note that they are not on a very meaningful scale.\nWithout further information, you cannot interpret what it means to say that the sum of squares for the variable age is 6524.\nThird, note that squaring distances does mean that high deviations become (quadratically) more influential:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ndf <- data.frame(Deviation = 1:10,\n                 Squared = c(1:10)^2)\nggplot(df, aes(x = Deviation, y = Squared)) + geom_point() + geom_line() + scale_x_continuous(breaks = 1:10) + theme_bw()\n```\n\n::: {.cell-output-display}\n![When squared, large values are more influential than small values.](chapter2_descriptive_statistics_files/figure-pdf/figsquare-1.pdf){fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\n\n\n### Variance: mean squared distance\n\nOne way to make the sum of squares more interpretable is to divide it by the number of observations.\nThis tells us how far away each observation is from the sample mean, on average.\n\nHere are three formulas, that all describe the calculation of the variance.\nThe first describes how you calculate the variance from the sum of squares (SS);\nthe second includes the formula for the sum of squares, and the third describes how you calculate it by squaring the raw scores and subtracting a sum of $n$ times the squared mean of X, $\\mu_{x}$:\n\n$s^2 = \\frac{SS}{n} = \\frac{\\sum_{i=1}^{n}(x_i - \\mu_{x})^2}{n} = \\frac{\\sum_{i=1}^{n}x_i^2 - n\\mu_{x}^2}{n}$\n\nNote that here, we divide by the sample size $n$.\nWhen using the variance as a descriptive statistic, this is fine.\n\nHowever, in later lectures, we will use sample statistics to make claims about the population (inferential statistics).\nThen, it becomes very important to divide by $n-1$ if the population mean is unknown.\nThe formulas then look like this:\n\n$s^2 = \\frac{SS}{n-1} = \\frac{\\sum_{i=1}^{n-1}(x_i - \\bar{x})^2}{n-1} = \\frac{\\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2}{n-1}$\n\nThe consequence of dividing by $n-1$ is that we get a slightly higher value for the variance.\nWe do this to account for the fact that we don't know the exact value of the population mean; we estimated it from the sample.\nIf we just assume that the sample mean is a perfect representation of the population mean, we will systematically under-estimate the variance.\nBy dividing by $n-1$, we get a slightly larger variance estimate, adjusted for our uncertainty about the value of the population mean.\n\n### Standard Deviation\n\nOne disadvantage of the variance is that it is still on the squared scale we obtained by squaring the distances.\nSo, if your variable measures age in years, than the variance of age is expressed in years squared.\n\nTo restore the variance to the original units of the variable, we can simply take the square root. So if our variables are measured in euros, centimeters, and milliseconds - then the variances will be expressed in euros$^2$, centimeters$^2$, and milliseconds$^2$.\nTaking the square root restores the original units; we call the resulting statistic the *standard deviation*.\n\nYou can think of the standard deviation as the *average deviation* between individual scores and the sample mean.\nWhy don't we just call it the \"average deviation\" then?\nBecause that would be mathematically inaccurate - when we squared the deviations before taking the average, we allowed larger deviations to have a disproportionately larger impact on the value of the variance.\nTaking the square root of the end result, the variance, does not cancel out that disproportionate influence of large deviations.\n\nSo, intuitively it is fine to think of the standard deviation as the \"average\" deviation, as long as you're aware that mathematically, this is not exactly correct, because an average value should assign equal weight to each observation, whereas the standard deviation assigns greater weight to extreme observations.\n\nImagine I tell you that, in one class, the average grade is a 5, with a standard deviation of .5. You would know that most students scored close to a 5, and many of them failed the course. If I told you that the average grade is 5 with a standard deviation of 2, you would know that scores are much more spread out, and a large portion of the students must have passed the course as well.\n\n\n## Effects of Transformations & Outliers {#sec-transform}\n\n| Transformation            | The **mean**... | The **SD**... |\n|---------------------------|---------------------------------|----------------------------|\n| Add / subtract constant   | Shifts by that constant         | Doesn't change                  |\n| Multiply / divide by constant | Scales by that factor        | Scales by that factor |\n| Inject one extreme score  | Pulls center toward outlier     | Increases     |\n\n*Example*: Changing units (e.g., converting centimeters to inches) would rescale both the mean and SD.\n\n## Why Descriptives Matter {#sec-bridge}\n\n* **Data cleaning**: Outliers leap out when you know the usual range.  \n* **Analysis choices**: Skewed or heavy-tailed distributions may call for robust or non-parametric methods.  \n* **Transparency**: Readers can judge your results only if they see the data’s headline features.  \n* **Communication**: \"Participants averaged *8.9 hours of screen time per day* (SD = 1 hr)\" paints an instant picture.\n\n## Context of Discovery VS Justification\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknitr::include_graphics(\"images/creativity_verification.jpg\")\n```\n\n::: {.cell-output-display}\n![An interpretation of De Groot’s empirical cycle, by Wagenmakers, Dutilh, & Sarafoglou, 2018. CC-BY: Artwork by Viktor Beekman, concept by Eric-Jan Wagenmakers](images/creativity_verification.jpg){fig-align='center' fig-pos='H' width=100%}\n:::\n:::\n\n\n\n\n\n\nIn the first chapter, we described De Groot's empirical cycle as a model of cumulative knowledge acquisition through scientific research.\nA crucial assumption of this cycle, highlighted by Wagenmakers and colleagues (see \\@ref(fig:figecwagenmakers)),\nis the distinction between the context of discovery and the context of justification.\nThe context of discovery is exploratory: we peruse data, looking for interesting patterns that might spark a new hypothesis.\nThe context of justification is confirmatory: we test a theory-driven hypothesis.\nIn order to obtain an unbiased test of a hypothesis, the hypothesis cannot be shaped by prior exploration of the data.\nIf we first observe an interesting pattern in data (exploratory), and then conduct a test of that pattern (confirmatory), the test is more likely to confirm the pattern.\nThere are legitimate ways to explore data looking for interesting patterns, and machine learning can be a helpful tool in this search [@vanlissaDevelopmentalDataScience2022].\nHowever, be careful of any cross-contamination between exploration and confirmation.\nAny pattern observed during exploration can introduce bias in subsequent confirmatory tests [@hoijtinkOpenEmpiricalCycle2023].\n\nVery often, the first thing researchers do when collecting or accessing a dataset is to calculate and examine descriptive statistics.\nThis common practice introduces a potential risk of cross-contamination between exploration and confirmation:\nobserving the descriptive statistics may influence other downstream analysis decisions.\nThis is not a problem when conducting purely exploratory analyses,\nand it is also not a problem if the analyses have been preregistered.\nPreregistration means that the analysis plans have been published in a time-stamped archive before collecting or accessing the data, so it's possible for others to check whether the reported analyses were executed as planned [@peikertWhyDoesPreregistration2023], with changes made after seeing the descriptive analyses.\nIn all other cases: be mindful of the risk of introducing bias.\nFor an example of how preregistered hypothesis tests can be combined with rigorous exploration using machine learning, see @vanlissaComplementingPreregisteredConfirmatory2022.\n\n\n<!-- ## Introducing Statistical Models {#sec-model} -->\n\n<!-- Descriptive statistics also represent the first example of a *statistical model*. -->\n<!-- While we will get more deeply into statistical modeling in later chapters, -->\n<!-- let's briefly examine how the topic is relevant here. -->\n\n<!-- Note that there is a difference between  -->\n\n\n\n<!-- as a *story* about how data arise: -->\n\n<!-- > **observed value = systematic part (signal) + random part (noise)** -->\n\n<!-- Descriptive statistics give us the first rough draft of that story: -->\n<!-- * a single value that stands in for the whole data set (signal), and   -->\n<!-- * a measure of how far typical scores wander from that value (noise). -->\n\n\n#Lecture \n\n\n# Formative Test\n\nA formative test helps you gauge how well you’ve grasped the ideas and calculations from **Chapter 2 – Descriptive Statistics**. Try the quiz after working through the lecture slides but **before** our live meeting, so we can focus on any topics that still feel wobbly. If you miss a question you’ll see a hint that points you back to the relevant slide or worked example.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nadd_mcs(\"ch2_formativ.csv\")\n```\n\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nquestion ^[answer]\n\n* (A) D  \n* (B) B  \n* (C) answer  \n* (D) A  \n* (E) C  \n\n\n\n**Question 2**\n\nWhich measure of centre is robust against extreme outliers? ^[Median]\n\n* (A) Mode  \n* (B) Weighted mean  \n* (C) Median  \n* (D) Mean  \n* (E) Median  \n\n\n\n**Question 3**\n\nFor purely nominal data (e.g., eye‑colour), the only valid measure of central tendency is the… ^[Mode]\n\n* (A) Median  \n* (B) Mode  \n* (C) Mode  \n* (D) Geometric mean  \n* (E) Mean  \n\n\n\n**Question 4**\n\nThe range summarises spread by using… ^[Only the minimum and maximum]\n\n* (A) Only the minimum and maximum  \n* (B) Squared deviations  \n* (C) Every score in the set  \n* (D) Only scores below the mean  \n* (E) Only the minimum and maximum  \n\n\n\n**Question 5**\n\nSquaring deviations when computing variance ensures that… ^[All deviations contribute positively to variability]\n\n* (A) Variance must be unbiased  \n* (B) Positive and negative deviations cancel  \n* (C) All deviations contribute positively to variability  \n* (D) Deviations stay in original units  \n* (E) All deviations contribute positively to variability  \n\n\n\n**Question 6**\n\nAdding a constant (e.g., +5) to every score will… ^[Shift the mean up by 5, leave SD unchanged]\n\n* (A) Shift SD up by 5  \n* (B) Shift the mean up by 5, leave SD unchanged  \n* (C) Multiply SD by 5  \n* (D) Shift the mean up by 5, leave SD unchanged  \n* (E) Not affect the mean  \n\n\n\n**Question 7**\n\nMultiplying every score by 3 will… ^[Multiply both mean and SD by 3]\n\n* (A) Multiply both mean and SD by 3  \n* (B) Divide SD by 3  \n* (C) Multiply both mean and SD by 3  \n* (D) Shift the mean by 3  \n* (E) Leave SD unchanged  \n\n\n\n**Question 8**\n\n“Degrees of freedom” for sample variance (n – 1) reflect that… ^[One piece of information (the mean) is already estimated]\n\n* (A) One piece of information (the mean) is already estimated  \n* (B) One piece of information (the mean) is already estimated  \n* (C) The sample size is unknown  \n* (D) Only n – 1 scores are valid  \n* (E) Variance must be unbiased  \n\n\n\n**Question 9**\n\nWhen a distribution is strongly right‑skewed (e.g., income), which centre measure best represents a “typical” observation? ^[Median]\n\n* (A) Mid‑range  \n* (B) Mean  \n* (C) Median  \n* (D) Median  \n* (E) Mode  \n\n\n\n**Question 10**\n\nA histogram with two distinct high peaks is called… ^[Bimodal]\n\n* (A) Bimodal  \n* (B) Bimodal  \n* (C) Unimodal  \n* (D) Skewed  \n* (E) Symmetric  \n\n\n\n**Question 11**\n\nReporting the standard deviation alongside the mean helps readers understand… ^[How tightly scores cluster around the mean]\n\n* (A) The measurement units  \n* (B) How tightly scores cluster around the mean  \n* (C) Whether data are normally distributed  \n* (D) The sample size  \n* (E) How tightly scores cluster around the mean  \n\n\n\n**Question 12**\n\nThe squared correlation (r²) plays a role for dispersion similar to which single‑variable statistic? ^[Variance]\n\n* (A) Inter‑quartile range  \n* (B) Variance  \n* (C) Range  \n* (D) Variance  \n* (E) Mode  \n\n\n\n**Question 13**\n\nThe inter‑quartile range (IQR) is especially useful because it… ^[Ignores extreme tails and focuses on the middle 50 %]\n\n* (A) Uses every score  \n* (B) Is expressed in squared units  \n* (C) Requires the mean first  \n* (D) Ignores extreme tails and focuses on the middle 50 %  \n* (E) Ignores extreme tails and focuses on the middle 50 %  \n\n\n\n:::\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Show explanations]\n\n```\n**Question 1**\n\nexplanation\n\n**Question 2**\n\nThe median depends only on rank order; extreme values cannot pull it up or down.\n\n**Question 3**\n\nNominal categories lack numerical distance, so the most frequent category (mode) is the only appropriate centre.\n\n**Question 4**\n\nRange is computed as X_max – X_min, relying solely on the two extreme scores.\n\n**Question 5**\n\nSquares turn all deviations positive, preventing positive and negative differences from cancelling out.\n\n**Question 6**\n\nA constant shift moves the centre but does not change the spread of scores.\n\n**Question 7**\n\nScaling stretches both centre and dispersion by the same factor.\n\n**Question 8**\n\nAfter fixing the sample mean, only n – 1 deviations can vary independently.\n\n**Question 9**\n\nThe median is unaffected by the long tail of extreme high values.\n\n**Question 10**\n\nTwo modes (peaks) indicate a bimodal distribution.\n\n**Question 11**\n\nSD converts variance back to original units, expressing average distance from the mean.\n\n**Question 12**\n\nJust as variance is the squared SD, r² expresses variance in Y accounted for by X.\n\n**Question 13**\n\nIQR spans Q3 – Q1, removing the influence of extreme outliers.\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n:::\n\n\n\n\n\n\n# Tutorial\n\n\n## Descriptive Statistics\n\n### Step 1\n\nAs explained before, the first step in any statistical analysis involves **inspection of the data**. In the previous assignment we looked at graphical summaries.\n\nThis assignment shows you how to explore data using **descriptive statistics**—values such as the mean, standard deviation, maximum, and minimum.\n\n> **Use the same data file as in the previous tutorial.**\n\n\n### Step 2 – Descriptives for Key Variables\n\nWe will first examine the descriptive statistics for **Optimism, Life Satisfaction,** and **Negative Emotions**.\n\nCompute descriptive statistics as follows:\n\n1. *Analyze > Descriptive Statistics > Descriptives*  \n2. Select **Optimism**, **Life Satisfaction**, **Negative Emotions**  \n3. Click **OK**\n\nSPSS opens a new **Output** window with a table of descriptives for the selected variables.\n\n\n### Step 3 – Frequency Tables\n\nIn the previous step we computed the average value and standard deviations. However, for nominal and ordinal variables, the average value is meaningless. To explore nominal and ordinal variables we may produce **frequency tables**. A frequency table shows the observed percentage for each level of the variable.\n\nGenerate frequencies for **Smoke** and **Relation**:\n\n1. *Analyze > Descriptive Statistics > Frequencies*  \n2. Select **Smoke** and **Relation**  \n3. Click **OK**\n\nSPSS now adds a table with the frequency distributions of the selected variables to the output file.\n\n*Note:* SPSS reports **Percent** and **Valid Percent**. These differ only when missing values are present (none in this dataset).\n\n\n#### Extra – Spotting Multimodality {.smaller}\n\nSometimes a single mean or median masks sub-groups.\n\n1. *Graphs > Legacy Dialogs > Histogram*  \n2. Choose **Life Satisfaction** for *Variable* and click **OK**\n\nIf you notice **two peaks**, colour the bars by **Relation** (single vs. relationship):\n\n1. *Graphs > Chart Builder*  \n2. Drag **Histogram** onto the canvas  \n3. Place **Life Satisfaction** on the *x*-axis  \n4. Drag **Relation** into *Cluster on X*  \n5. Click **OK**\n\n*Take-away:* multiple modes often reveal hidden clusters that may need separate analysis.\n\n\n<!-- variable descriptions kept for reference -->\n<!-- stress: 1 = no stress; 2 = work-related; 3 = personal-life -->\n<!-- smoke: 1 = non-smoker; 2 = smoker -->\n<!-- relation: 1 = single; 2 = relationship -->\n<!-- optim:   1–50 -->\n<!-- satis:   1–50 -->\n<!-- negemo:  1–50 -->\n\n## Quiz 1 – Basic Descriptives\n\n::: {.webex-check .webex-box}\n\nHow many participants are in the sample? ___^[780]\n\nWhat is the mean value of Optimism? _____^[19\\.13]\n \nFor which of the variables is the spread in the scores highest? ^[OPTIM]\n\n* (A) SATIS  \n* (B) OPTIM  \n* (C) NEGEMO  \n\n\n\nThe minimum and maximum observed scores for Negative Emotions were: [_^[3], __^[37]].\n\nWhat percentage of participants is a non-smoker? ____^[48\\.1]\n\nWhat percentage of participants is in a relationship? ____^[47\\.9]\n\n:::\n:::\n\n\n### Weighted Mean {.smaller}\n\nSuppose Class A (*n* = 12, mean = 6) and Class B (*n* = 8, mean = 7) are merged.  \nSPSS effectively multiplies each mean by its *n*, sums those products, and divides by the **total** 20 students, yielding **6.4**.\n\n*Quick SPSS route*  \n\n- Merge the two files if separate (*Data > Merge Files*).  \n- Run *Analyze > Descriptive Statistics > Descriptives* on the combined score column.\n\n\n\n### Step 4 – Finding Erroneous Values\n\nOne reason to inspect descriptives first is to spot **erroneous values** (e.g., age 511 instead of 51).\n\nUse the descriptives to find any out-of-range values, then:\n\n1. *Data > Sort Cases*  \n2. Sort the suspect variable ascending or descending  \n3. Delete rows with invalid values\n\nAt this stage we remove entire cases; later you’ll learn gentler missing-data techniques.\n\n\n\n### Step 5 – Group Comparison with Split File\n\nResearch question: *“Are non-smokers more satisfied with life than smokers?”*\n\n1. *Data > Split File > Compare Groups* → choose **Smoke**  \n2. Run *Analyze > Descriptives* on **Life Satisfaction**\n\nSPSS now outputs separate means for smokers and non-smokers.\n\n## Quiz 2 – Group Means\n\n::: {.webex-check .webex-box}\n\nWas there an erroneous value in the data file? Enter it here: ___^[220]\n\nIf you delete that value, how will the **mean** change? ^[Becomes smaller]\n\n* (A) Stays the same  \n* (B) Becomes larger  \n* (C) Becomes smaller  \n\n\n\nIf you delete that value, how will the **standard deviation** change? ^[Becomes smaller]\n\n* (A) Becomes larger  \n* (B) Stays the same  \n* (C) Becomes smaller  \n\n\n\nWho is more satisfied in this sample? ^[Smokers]\n\n* (A) Non-smokers  \n* (B) Smokers  \n\n\n\nDoes this difference necessarily hold in the population? ^[Can't tell]\n\n* (A) Can't tell  \n* (B) Yes  \n* (C) No  \n\n\n\n:::\n\n\n### Step 6 – Quick Check: How Recoding Affects Spread {.smaller}\n\n*Add 10 points* to every Life-Satisfaction score:\n\n1. *Transform > Compute Variable*  \n2. Target variable: `SATIS_plus10`  \n3. Numeric expression: `SATIS + 10` → **OK**\n\nRun Descriptives on both variables:\n\n- Mean shifts up by 10  \n- **SD is unchanged**\n\n*Multiply by 3* (expression `SATIS * 3`):\n\n- Mean × 3  \n- **SD × 3**\n\n\n\n## More Descriptive Statistics\n\nDescribing the data is an essential first step in any research context.\n\n### Central Tendency by Hand\n\nGrades: 6  3  4  6  7  6  8  9  10  9\n\nCompute **mean, median, mode** by hand.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Remind me how]\n\n```\n\n- Mode = most common value  \n- Median = middle value (or midpoint)  \n- Mean = sum / *n*  \n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\n#### Quiz 3 – Hand Computation\n\n::: {.webex-check .webex-box}\n\nMean ___^[6\\.8]\n\nMedian ___^[6\\.5]\n\nMode _^[6]\n\n:::\n\n\n### Variation by Hand\n\nGrades: 2  7  6  7  8  9\n\nCompute **variance** and **standard deviation**.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Remind me how]\n\n```\n\nVariance = average squared distance from mean  \nSD = √ variance  \n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\n> **Why divide by *n – 1*?**  \n> After the mean is fixed, only *n – 1* deviations are free to vary, so dividing by *n – 1* keeps the sample variance unbiased.\n\n#### Quiz 4 – Hand Computation\n\n::: {.webex-check .webex-box}\n\nVariance ___^[5\\.9]\n\nStandard Deviation ____^[2\\.43]\n\n:::\n\n\n### Verifying in SPSS\n\nEnter the six grades, name the variable, then:\n\n1. *Analyze > Descriptive Statistics > Descriptives*  \n2. **Options… > Variance** → **Continue > OK**\n\nConfirm SPSS matches your hand calculations.\n",
    "supporting": [
      "chapter2_descriptive_statistics_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}