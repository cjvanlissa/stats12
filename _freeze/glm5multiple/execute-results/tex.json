{
  "hash": "6510c8cac7ab47e42b5817f8fedc8f50",
  "result": {
    "engine": "knitr",
    "markdown": "# GLM-V: Multiple regression {#sec-glm5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Multiple regression\n\nMultiple regression is a statistical technique that allows us to examine the relationship between one outcome and multiple predictors. It extends the concept of bivariate linear regression, where we model the relationship between two variables, to include more predictors. In the context of social science research, multiple regression helps us answer the question: What is the unique effect of one predictor, while controlling for the effects of all other predictors?\n\nAs a matter of fact, last week's analyses for categorical variables with more than two categories were already an example of multiple regression. We included two dummy variables to represent a categorical variable with three categories. All that's new today is that we also consider the case where the multiple predictors are continuous variables. An important realization is that a regression model can be expanded to include as many predictors as needed. The general formula for multiple regression is $\\hat{Y} = a + b_1X_1 + b_2X_2 + \\ldots + b_KX_K$, where $\\hat{Y}$ represents the predicted value of the dependent variable Y, $a$ is the intercept, and $b_{1 \\ldots K}$ are the slopes for each predictor.\n\nWhen interpreting the regression coefficients, the intercept (a) represents the expected value of the dependent variable when all predictors are equal to 0. For dummy variables, this is the mean value of the reference category, while for continuous predictors, it represents the expected value for someone who scores 0 on all predictors. The regression coefficients (b1, b2, ..., bK) indicate how many units the dependent variable Y is expected to change when the corresponding predictor X increases by 1 unit, while holding all other predictors constant.\n\nCentering predictors can be useful in multiple regression. By centering, we shift the zero-point of the predictor to a meaningful value, such as the mean value on that predictor. This helps in interpretation, because the intercept now gives us the mean value on the outcome for someone who has an average score on all predictors.\n\nAs previously explained, standardized regression coefficients drop the units of the predictor and outcome variable. They are calculated by transforming the predictors and outcome variable into z-scores with a mean of 0 and a standard deviation of 1, and performing the (multiple) regression analysis on those z-scores. Because the units of the variables are dropped, standardized coefficients make the effects of predictors comparable across different studies or variables with different measurement units. They represent the change in the dependent variable in terms of standard deviations when the corresponding predictor increases by 1 standard deviation.\n\n### Causality\n\nA statistical association between variables does not necessarily imply a causal relationship. Instead, causality is either assumed on theoretical grounds, or established using the experimental method. In an experiment, researchers manipulate an independent variable and observe its effects on the dependent variable. However, in many social science studies, experiments are not feasible or ethical, so researchers rely on observational data. In these cases, establishing causal relationships relies on theory and careful statistical analysis.\n\nOne important concept in causal inference is the direction of effects. While statistical methods can identify associations between variables, determining the direction of causality is a causal assumption that cannot be estimated using statistics alone. The assumed direction of effects is often based on theory and prior knowledge of the subject matter. Researchers make informed assumptions about which variable is likely to have a causal effect on the other based on theoretical reasoning and empirical evidence.\n\nIn the process of analyzing causal relationships, it is essential to consider the presence of confounders, mediators, and colliders. Confounders are variables that are associated with both the independent and dependent variables and can create a spurious association or distort the true causal relationship. Identifying and controlling for confounders is crucial to ensure accurate causal inference.\n\nMediators, on the other hand, are variables that explain the relationship between the independent and dependent variables. They act as intermediate steps or process variables in the causal pathway. Understanding and analyzing mediation effects help us understand the underlying mechanisms through which the independent variable affects the dependent variable.\n\nColliders are variables that are caused by both the independent and dependent variables. Controlling for colliders can lead to spurious statistical relationships between unrelated variables. It is essential to be cautious when including variables in the analysis and consider the causal structure of the variables involved.\n\nOne important take-home message is that, in multiple regression, the distinction between confounders and colliders is crucial. Including confounders as control variables in multiple regression improves our inferences - but accidentally including a collider as control variable (severely) biases our inferences. You therefore have to carefully reason about each variable's role in relation to the other variables in the model.\n\n## Lectures\n\n\n\n\n\n\n{{< video https://www.youtube.com/embed/lT-6YoSfwC4 >}}\n\n\n{{< video https://www.youtube.com/embed/wBB4sed9ku0 >}}\n\n\n\n\n\n\n\n\n\n\n\n## Formative Test\n\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\n\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nWhat is the primary advantage of using multiple regression analysis? ^[To understand the unique effect of each predictor while accounting for others]\n\n* (A) To understand the unique effect of each predictor while accounting for others  \n* (B) To group predictors based on their significance  \n* (C) To identify the most significant predictor  \n* (D) To find a single predictor that explains all the variance  \n\n\n\n**Question 2**\n\nWhat is the purpose of centering in multiple regression analysis? ^[To choose a meaningful zero-point for predictors]\n\n* (A) To ensure the assumption of normality is met  \n* (B) To standardize all predictors  \n* (C) To remove outliers from the data  \n* (D) To choose a meaningful zero-point for predictors  \n\n\n\n**Question 3**\n\nWhat does the intercept (a) in a multiple regression model represent? ^[Expected value when all predictors are equal to 0]\n\n* (A) The effect of the first predictor  \n* (B) The effect of the last predictor  \n* (C) Expected value when all predictors are equal to 0  \n* (D) The difference between the two groups  \n\n\n\n**Question 4**\n\nWhat do the b-coefficients in a multiple regression model represent? ^[The change in the dependent variable for a one-unit change in the predictor while other predictors are held constant]\n\n* (A) The change in the dependent variable for a one-unit change in the predictor while other predictors are held constant  \n* (B) The average of all predictor values  \n* (C) The change in the predictor for a one-unit change in the dependent variable while other predictors are held constant  \n* (D) The change in the dependent variable for a one-unit change in the predictor without considering other predictors  \n\n\n\n**Question 5**\n\nWhen is multicollinearity a concern in multiple regression analysis? ^[When predictor variables are highly correlated with each other]\n\n* (A) When predictor variables are independent  \n* (B) When there are multiple outcome variables  \n* (C) When predictor variables are highly correlated with each other  \n* (D) When there are outliers in the data  \n\n\n\n**Question 6**\n\nWhat is the role of standardized regression coefficients in multiple regression analysis? ^[To compare the effect sizes of predictors on a common scale]\n\n* (A) To compare the effect sizes of predictors on a common scale  \n* (B) To calculate the intercept  \n* (C) To determine the unique effect of each predictor  \n* (D) To convert categorical predictors to continuous ones  \n\n\n\n**Question 7**\n\nWhat is the potential bias introduced when controlling for a collider in multiple regression analysis? ^[It biases the estimate of the association between the two variables that cause the collider]\n\n* (A) It biases the estimate of the association between the two variables that are caused by the collider  \n* (B) It biases the estimate of the association between the two variables that cause the collider  \n* (C) It creates a causal relationship between the variables that cause the collider  \n* (D) It has no effect on the causal inference between the variables that form the collider  \n\n\n\n**Question 8**\n\nGiven the multiple regression equation: Y = 12.5 + 2.3*X1 + 1.8*X2 - 0.5*X3, calculate the predicted value of Y when X1 = 5, X2 = 3, and X3 = 2. ^[28.4]\n\n* (A) 23.1  \n* (B) 20.5  \n* (C) 28.4  \n* (D) 27.6  \n\n\n\n**Question 9**\n\nIn a multiple regression model, if the coefficient of determination (R²) is 0.75 and the SSE is 150, what is the value of SST? ^[600]\n\n* (A) 0.25  \n* (B) 112.5  \n* (C) 450  \n* (D) 600  \n\n\n\n**Question 10**\n\nGiven the following standardized regression equation: Y = 0.6 + 0.35*X1 + 0.25*X2 - 0.15*X3, what is the correct conclusion about the effect of X2? ^[A one SD increase in X2 is associated with a 0.25 SD increase in Y, keeping all other predictors constant.]\n\n* (A) A one unit increase in X2 is associated with a 0.25 increase in Y, keeping all other predictors constant.  \n* (B) X2 is associated with a 0.25 increase in Y, keeping all other predictors constant.  \n* (C) A one SD increase in X2 is associated with a 0.25 SD increase in Y, keeping all other predictors constant.  \n* (D) A one unit increase in X2 is associated with a 0.25 unit increase in Y.  \n\n\n\n:::\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Show explanations]\n\n```\n**Question 1**\n\n Multiple regression analysis helps to understand the unique effect of each predictor while controlling for the effects of other predictors.\n\n**Question 2**\n\n Centering is used in multiple regression analysis to choose a meaningful zero-point for predictors.\n\n**Question 3**\n\n The intercept (a) in a multiple regression model represents the expected value when all predictors are equal to 0.\n\n**Question 4**\n\n The b-coefficients in a multiple regression model represent the change in the dependent variable for a one-unit change in the predictor while other predictors are held constant.\n\n**Question 5**\n\n Multicollinearity is a concern in multiple regression analysis when predictor variables are highly correlated with each other.\n\n**Question 6**\n\n Standardized regression coefficients are used to compare the effect sizes of predictors on a common scale, especially when the units of predictors are not meaningful.\n\n**Question 7**\n\n Controlling for a collider in multiple regression analysis can introduce bias in the estimated association between the variables that form the collider.\n\n**Question 8**\n\n Plug in the values of X1, X2, and X3 into the regression equation: Y = 12.5 + 2.3*5 + 1.8*3 - 0.5*2 = 28.4.\n\n**Question 9**\n\n Use the formula R2 = 1-(SSE/SST) or R2 = SSR/SST\n\n**Question 10**\n\n The correct interpretation is in the original units of the variables, and emphasizing the fact that other predictors were controlled for.\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n:::\n\n\n\n\n\n\n## In SPSS\n\n### Multiple Regression\n\n\n\n\n\n\n{{< video https://www.youtube.com/watch?v=ueNrP5TyZaE >}}\n\n\n\n\n\n\n\n\n\n\n## Tutorial\n\n### Multiple Regression\n\nSocial science students were asked about their opinion towards Tilburg’s nightlife, number of Facebook friends, and some other characteristics. The data are in the [`SocScSurvey.sav`](data/SocScSurvey.sav) file.\n\nIn a previous assignment we predicted Facebook friends by extraversion.\n\nIn this question we will add another predictor, peer pressure.\n\nThe variable peer pressure refers to the tendency to be influenced by close friends. Higher scores reflect higher sensitivity to peer pressure.\n\nBefore we proceed with the regression analysis, we will first look at the correlations between the variables.\n\nAnalyze > correlate > bivariate.\n\nNow choose as variables: Facebook Friends, Extraversion and Peer Pressure, and click OK.\n\nWhat is the correlation between peer pressure and number of Facebook friends? _____^[0\\.145]\n\nSuppose three researchers test the significance of the correlation between peer pressure and Facebook friends. Researcher I tests at the 10% level, researcher II tests at the 5% level, and researcher III at the 1% level.\n\n\nWhich researcher will reject the null hypothesis? ^[Only researcher I]\n\n* (A) All three researchers  \n* (B) Only researcher I  \n* (C) Only researcher III  \n* (D) Only researcher II  \n\n \n\nNow, run the regression analysis in which the number of Facebook friends is regressed on extraversion and peer pressure.\n\nProceed as follows: via analyze > regression > linear.\nChoose Facebook friends as dependent and extraversion and peer pressure as independents.\n\nConsult the output and write down the regression equation.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n$\\text{Friends}_i = -158.012 + 26.560*\\text{Extraversion}_i + 12.056*\\text{Peer}_i + e_i$\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nConsider the second person in the sample. The person had an extraversion score of 11 and a score of 9 on peer pressure.\n\n\nWhat is the predicted number of Facebook friends for this person? _______^[242\\.652]\n\nConsult the output.\n\nResearchers conclude that – in the sample – as peer pressure increases with one unit, the predicted number of Facebook friends increases with 12.056 units. \n\nIs this a valid conclusion? ^[No]\n\n* (A) Yes  \n* (B) No  \n\n\n\nIn multiple regression, the regression coefficients show us the expected changes in the dependent variable, while keeping the other independent variables constant.\n\nWith this in mind, what is the correct conclusion?\n\n^[As peer pressure increases with one unit the predicted number of Facebook friends increases with 12.056 units, while keeping extraversion constant.]\n\n* (A) As peer pressure increases with one unit the predicted number of Facebook friends increases with 12.056 units. This is added to the constant of -158.01.  \n* (B) As peer pressure increases with one unit the predicted number of Facebook friends increases with 12.056 units.  \n* (C) As peer pressure increases with one unit the predicted number of Facebook friends increases with 12.056 units, while keeping extraversion constant.  \n* (D) As peer pressure increases with one unit the predicted number of Facebook friends increases with 12.056 units, while extraversion changes with 26.56 units.  \n\n\n\nConsult the table Coefficients. The table shows the results of t-tests. \n\nWhat are the null hypotheses and alternative hypotheses that are tested here? \n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\nThe t-tests test significance of the individual regression coefficients. In particular, for each coefficient we can use the t-tests to test the following hypotheses:\n\n$H_0: \\beta = 0$, $H_1: \\beta \\ne 0$\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\nWhat is the value of the test-statistic for the significance test for extraversion? _____^[5\\.583]\n\n\nConsider the t-tests for the regression coefficients again.\n\nHow many degrees of freedom do the t-tests have? ___^[131]\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Explanation]\n\n```\n\nNote that:\n\nDegrees of freedom = N - p\nN = number of participants; p = number of parameters in the model (intercept + two regression slopes)\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n \n\nSuppose three researchers test the significance of peer pressure as a predictor of Facebook friends, while controlling for extraversion. Researcher I tests at the 10% level, researcher II tests at the 5% level, and researcher III at the 1% level.\n\n\nWhich researcher(s) will reject the null hypothesis? ^[Only researcher I]\n\n* (A) Only researcher I  \n* (B) Only researcher II  \n* (C) All three researchers  \n* (D) Only researcher III  \n\n \n    \n\n\nWhat *percentage* of the total variance in Facebook friends can be explained by both extraversion and peer pressure? ____^[20\\.9]\n\nCompare the $R^2$ of the regression model with both predictors with the $R^2$ of a model with only extraversion as the predictor.\n\nWhat is the difference? _____^[0\\.017]\n\nIn the previous step we compared the $R^2$ of two so called nested models. \n\nTwo models are nested if the larger model (i.e., the model with the most predictors) contains all predictors of the smaller model.\n\nIn the next lecture we will learn more about nested models, model comparisons, and how useful they are for researchers! \n\n\n### Multiple Regression II\n\nFor this assignment we need the file [`HealthyFood.sav`](data/HealthyFood.sav).\n\nThis file contains hypothetical data on three variables:\n\nEating healthy food (the higher the score, the healthier a person's diet)\nKnowledge about food (the higher the score, the more a person knows about healthy food and risks of unhealthy food)\nIncome (higher scores = more income).\n \nLet's first look at the associations (correlations) between the three variables.\n\nCompute the correlations and summarize the relationships between all pairs of variables. Include in your answer the strength of the relationship (i.e., weak, moderate, or strong), the direction of the relationship (i.e., positive or negative), and generalizability to the population (i.e., is the correlation significant at the 5% level).\n\nCohen’s rules of thumb:\n\n* r = 0.00-0.30 (none to weak)\n* r = 0.30-0.50 (weak to moderate)\n* r = 0.50-0.70 (moderate to strong)\n* r = 0.70-0.90 (strong to very strong)\n* r = 0.90-1.00 (very strong)\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n\n- Income and eating have a weak positive correlation, which is significant at the 5% level.\n- Income and knowledge have a weak to moderate positive correlation, which is significant at the 5% level.\n- Knowledge and eating have a moderate to strong positive correlation, which is significant at the 5% level.\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n \nResearchers may be interested in explaining differences in eating healthy food: in other words, they want to know why some people eat very healthy, while others tend to eat unhealthy. \n\nOne of the hypotheses is that healthy food is on average more expensive than unhealthy food, so one of the explanatory variables may be income.\n\n\nRun a regression analysis using eating as the dependent variable and income as the independent variable. \n\nConsult the output.\n\n\nWhich of the following conclusions is correct?\n\n^[The effect of Income on eating healthy food is positive and significantly different from zero.]\n\n* (A) The effect of Income on eating healthy food is positive and significantly different from zero.  \n* (B) The effect of Income on eating healthy food is positive but not significantly different from zero.  \n\n\n \nSimple regression analysis suggests a positive relationship between income and healthy food.\n\nHowever, other researchers (say Team B) came up with an alternative explanation. They hypothesized that the relationship between income and healthy food can be explained by a confounder; knowledge. People with more knowledge will have better jobs (on average), and, as a result more, income. As the result of their knowledge they also prefer to eat healthy food. I.e., Team B thinks knowledge is a common cause of income and eating healthy food.\n\n\nIn other words, the researchers of Team B hypothesize that the relationship between income and eating healthy food is ^[Spurious]\n\n* (A) Indirect  \n* (B) Spurious  \n\n\n\nDraw (on a piece of paper) the conceptual model that reflects the hypotheses of the researchers.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n\n![](images/y_x_on_z_spurious.png)\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\n\nNow let's see if the data support the hypotheses of the researchers.\n\nRun a multiple regression analysis using eating healthy food as the dependent variable and income and knowledge as independent variables.\n\nConsult the output. Look at the effect of income, controlled for knowledge (both the coefficient and the significance test).\n\n- What happened with the effect of income if you control for knowledge?\n- Does knowledge predict eating healthy food (controlled for income)? \n- Do the data support the hypothesis that the relationship between income and healthy food is confounded by knowledge?\n\nWhat is the p-value of Income when you control for Knowledge? _____^[0\\.778]\n\nWhat is the p-value of Knowledge, controlling for Income? _____^[0\\.001]\n\nDo the data support the hypothesis that the relationship between income and healthy food is confounded by knowledge? ^[Yes]\n\n* (A) Yes  \n* (B) No  \n\n\n\n\nFinally, interpret the output. Write down the answers to the following questions:\n\n1. How well can we predict the variance in healthy eating with the predictors income and knowledge? Interpret R2, report the appropriate test and its significance\n1. Interpret the regression coefficients (size, direction, significance)\n1. Which predictor is the most important predictor of healthy eating behavior? Inspect the standardized regression coefficients\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Answer]\n\n```\n\n\n\nIncome and Knowledge together predict 44.9% of the variance in Eating healthy food, which is significantly different from zero, $R^2 = .45, F(2, 347) = 141.179, p <.001$.\n\nControlling for Knowledge, Income has a positive, but non-significant effect on Eating healthy food, t(347) = .283, p = .778.\n\nControlling for Income, Knowledge has a positive, significant effect on Eating healthy food, t(347) = 15.287, p <.001.\n\nKnowledge is the most important predictor ($\\beta$ = .665) (compare it with $\\beta$ = .012 of Income).\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n",
    "supporting": [
      "glm5multiple_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}