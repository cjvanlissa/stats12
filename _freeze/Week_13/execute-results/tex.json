{
  "hash": "6aca6fc8099979e0758712ace0b52beb",
  "result": {
    "engine": "knitr",
    "markdown": "# Open science and questionable research practices {#sec-testing}\n\n\n\n## Introduction — Open Science and Questionable Research Practices {#week-open-science-intro}\n\nOver the past decade, large-scale replication efforts have shown that many published effects cannot be replicated,\nor finds much weaker effects when reproducing published work with high-powered designs (e.g., replication effects were, on average, about half the size of originals; 36% of replications were statistically significant versus 97% of originals) (Open Science Collaboration, 2015).\nOne reason might be that scientific journals select for \"newsworthy\" (novel and exciting) findings,\nrather than unsurprising but diligently produced facts.\nIn other words, there is a misalignment between what advances careers and what advances knowledge (Nosek et al., 2012).\nScientific claims should earn credibility because they are based on **transparent and reproducible** research,\nnot because results are surprising or the narrative is compelling. \n\nThis week examines how research practices and incentive structures can **inflate false positives** and **distort effect estimates**. Psychology has reported an unusually high share of significant findings (≈96%) despite typical studies being underpowered—conditions that favor publication bias and exaggerated effects (Bakker, van Dijk, & Wicherts, 2012). When samples are small, hypotheses are numerous, and analytic choices are flexible, the **positive predictive value** of a single significant result is low (Ioannidis, 2005). These problems motivate **open-science** reforms that reward accuracy over novelty: preregistration to reduce researcher degrees of freedom and to separate confirmatory from exploratory work; routine sharing of data, code, and materials; and explicit valuing of replication (Nosek et al., 2012; Bakker et al., 2020).\n\n## Scientific Fraud\n\nWhile it is unlikely that scientific fraud is a main cause of the replication crisis in psychology and other fields,\na highly publicized case of fraud did act as a catalyst, drawing attention to potential shortcomings in the way science was being conducted.\nTilburg University professor Diederik Stapel,\na prominent social psychologist, was found by the Levelt Committee to have fabricated and manipulated data across many studies,\noften supplying such fake datasets to PhD students and coauthors,  unwittingly involving them in the fraud.\nThe resulting papers reported striking effects that were considered exemplary, passed peer review, were widely cited, and shaped research agendas - until independent teams failed to reproduce them.\nThe case exposed structural failures of the scientific enterprise, including incentives for newsworthy findings, a lack of auditing of data and analysis code, and tolerance of researcher degrees of freedom that are also common in non-fraudulent questionable research practices (p‑hacking, HARKing, low power).\nThis case motivated the adoption of open science practices that seek to improve academic rigor, such as preregistration of study- and analysis plans prior to data collection, and transparent sharing of data, code, and materials.\n\nMore recently, the Francesca Gino scandal revived this debate.\nIn 2023, [Data Colada](https://datacolada.org/109) research auditors published a report of irregularities in several of Gino's papers on dishonesty (ironically).\nOne striking finding was that the user history of an Excel spreadsheet seemed to indicate that several cases were moved from the control- to the experimental condition - and that this edit led to statistically significant findings.\n\nWhile these cases are thought-provoking, it is unlikely that they are the sole cause for the replication crisis. The main contributing factors might be much more mundane.\n\n## Questionable Research Practices (QRPs) {#sec-qrps}\n\n**Definition:** QRPs are choices made in research and reporting that may be defensible but, when guided by the pursuit of statistical significance (motivated reasoning), end up inflating false-positive findings and effect sizes, thus distorting the published record. In psychology, unusually high rates of “positive” findings alongside typically low power indicate conditions under which QRPs and publication bias can thrive (Bakker et al., 2012).\n\nQuestionable research practices are especially problematic in relation to researcher degrees of freedom, because nearly any dataset can be tortured and sliced until it serves up a significant effect.\nThis raises the proportion of false-positive findings in the literature.\n\n### Examples of QRPs:\n\n- **Optional stopping / sequential testing:** Conducting an analysis, looking at the results, and adding more participants if the result is not (yet) significant ends up inflating the Type I error rate. This is especially problematic if you consider that, when the null hypothesis is true, p-values are uniformly distributed (all values equally likely). Thus, you can keep adding participants until by pure chance you find a significant result.\n- **Outcome Switching / Selective reporting:** Measuring multiple dependent variables but reporting only those that are significant. \n- **Flexible data cleaning:** Post hoc exclusions, outlier rules, and transformations chosen after seeing results and applied such that a hypothesized difference becomes significant, or an inconvenient significant difference disappears.\n- **Researcher degrees of freedom in modeling:** After inspecting the data, trying multiple reasonable model specifications (e.g., control variables, mathematical transformations, exploring interaction effects) and picking the model with the \"most interesting\" results, or the one that supports the researcher's hypothesis.\n- **HARKing (hypothesizing after results are known):** Constructing a hypothesis after seeing a surprising result, and then presenting this post hoc hypothesis as if it was specified before seeing the data. This blurs the distinction between confirmatory and exploratory research. Keep in mind that unexpected things do happen by chance, and creating a hypothesis after observing something unexpected does mean that the hypothesis will be significant, but does not mean that it is likely to be true. For example, Lucia de B. and Lucy Ledby were both convicted after an unusual event was observed (high proportion of infant deaths at their hospitals), and the post-hoc hypothesis was constructed that this must be the result of murder. However, rare events do happen, and they are not always the result of murder.\n- **Publication bias (\"file drawer\" effect):** Studies with significant findings are more likely to be submitted and published than null results. When typical power is low (e.g., 20–40%), only about 20–40% of studies should be significant **even if effects are real**; yet many literatures report mostly significant findings. This indicates publication bias.\n\n## Well-Intended Flawed Practices\n\nEven well-intended practices can misfire when applied uncritically.\nAs briefly mentioned in the chapter on philosophy of science, Null‑Hypothesis Significance Testing (NHST) is a prime example of how deeply entrenched scientific practices can be misleading.\nGerd Gigerenzer called this the \"null ritual\": set up a straw man null hypothesis which states that the effect is zero, adopt $\\alpha = .05$ as default significance level, reject the null hypothesis if $p < .05$ and interpret the result as positive evidence for some finding.\nThis practice encourages dichotomous thinking,\nneglects effect sizes and uncertainty,\nand mixes the incompatible philosophies of testing from Fisher and Neyman–Pearson.\nThis ritual ignores important principles - such as, that even trivial effects become \"significant\" in very large samples,\nand that extreme results (including effects that are significant by pure chance) are more common in small samples.\nWhat is the alternative? To move beyond the ritual and emphasize effect size estimation, uncertainty quantification, power analysis and sample size justification, and open science practices that allow others to replicate and audit findings.\n\n\n## Open Science Practices — Preregistration and Registered Reports {#sec-open-practices}\n\nOpen science practices aim to improve scientific rigor, and the efficiency with which knowledge can accumulate and errors can be corrected,\nby making many aspects of the scientific enterprise *open, accessible, transparent, and reproducible*.\nWe examine several open science practices.\n\n### Data Sharing\n\nIn a way, you have already enjoyed the benefits of data sharing: the exercises and portfolio assignments for this course make use of either real \"open data\", or fake data that was generated based on open data accessed by the author (Caspar van Lissa).\nData sharing helps people reproduce published findings, and makes it possible to reuse existing datasets to test novel research questions.\n\n### Reproducibility\n\nReproducibility means being able to re-perform the same analysis with the same code using a different analyst [@patilVisualToolDefining2019].\nThis can be achieved by creating research archives that include [@vanlissaWORCSWorkflowOpen2021]:\n\n1. Well-documented analysis code; for example, in the form of a dynamic document that combines the written text of a paper or research report (Introduction, Discussion) with the code required to generate the analysis results (e.g., SPSS syntax, R- or Python code). Such dynamic documents can be exported to any format, including PDF, or a website. This GitBook is an example of a dynamic document; both text and analysis results/figures are dynamically generated.\n1. A complete time line of the research archive's historical development. Akin to a lab notebook that documents decisions made during the research process, modern \"version control\" systems make it possible to track, document, and preserve all changes to data and code from the moment a project is conceived, until it is preregistered, and data are collected, and it is ultimately published.\n1. A time capsule of the computer environment (exact versions of software used), because the same analysis code can sometimes give different results on different systems. This is called \"dependency management\"; the most extreme form of dependency management is \"containerization\": creating a virtual computer to run the code.\n\n\n### Preregistration {#sec-prereg}\n\n**Purpose:** Reduce undisclosed flexibility by **specifying key decisions in advance**—hypotheses, primary outcomes, sampling plan and stopping rule, inclusion/exclusion criteria, randomization, and the primary analysis plan. Preregistration **separates confirmatory from exploratory work**; it does not forbid exploration. Deviations are permitted, but they should be **documented and justified**, with the original, time-stamped plan remaining visible. This transparency lets readers follow the study’s evolution, evaluate analytic degrees of freedom, and interpret results accordingly; it also encourages **follow-up confirmation** of exploratory findings with new data (Bakker et al., 2020; Nosek et al., 2012).\n\n**Quality matters.** Effective preregistrations are **specific, precise, and comprehensive**. Structured templates with itemized prompts constrain opportunistic degrees of freedom better than unstructured formats, though **neither eliminates** flexibility entirely (Bakker et al., 2020).\n\n**What preregistration is not.** It is not a ban on creativity. Exploratory analyses remain valuable, preregistration simply makes their status transparent and encourages follow-up confirmation with **new data** (Bakker et al., 2020).\n\n### Registered Reports {#sec-rr}\n\n**Model.** A two-stage publication track in which the **study rationale, design, and analysis plan** are peer-reviewed **before data collection** (Stage 1). Upon **in-principle acceptance**, the journal commits to publish the results regardless of outcome **if** authors follow the approved protocol (Stage 2). This shifts incentives away from “significance” toward **design quality and theoretical contribution**, reducing publication bias and HARKing pressure (Nosek et al., 2012; Bakker et al., 2020).\n\n**Practical payoff.** Registered Reports move the credibility test **upstream**. Peer review and in-principle acceptance occur **before** data collection, which (a) locks key design and analysis decisions, (b) commits publication regardless of outcome, and (c) requires transparent documentation of any deviations. By decoupling publication from statistical significance and limiting undisclosed flexibility, Registered Reports reduce publication bias, HARKing, and selective reporting, complementing preregistration with editorial enforcement at the design stage (Nosek et al., 2012; Bakker et al., 2020).\n\n\n## Lecture\n\nPlease watch this conference presentation by Dr. Amy Orben on questionable research practices:\n\nhttps://www.youtube.com/embed/EIDY6TAy52M?si=SvbSK86hFga3JLq6\n\n# Formative Test\n\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\n\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\n::: {.cell layout-align=\"center\"}\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nA field reports about 90 percent significant findings while typical study power is about 35 percent. What is the most plausible interpretation? ^[Publication bias and selective reporting are likely inflating the visible discovery rate]\n\n* (A) The nominal alpha is lower than 0.05 across studies  \n* (B) Publication bias and selective reporting are likely inflating the visible discovery rate  \n* (C) The field studies exceptionally large true effects  \n* (D) Replications will usually find larger effects  \n\n\n\n**Question 2**\n\nWhich practice best describes collecting an initial sample then checking results and adding participants until p is below 0.05 without a prespecified stopping rule or correction ^[Optional stopping without error control]\n\n* (A) Publication bias  \n* (B) HARKing  \n* (C) Proper sequential design  \n* (D) Optional stopping without error control  \n\n\n\n**Question 3**\n\nA study measures five outcomes but reports only the one that is significant as the primary outcome while omitting the rest. Which QRP is this ^[Selective outcome reporting]\n\n* (A) Selective outcome reporting  \n* (B) Model fishing  \n* (C) Flexible data cleaning  \n* (D) Appropriate parsimony  \n\n\n\n**Question 4**\n\nAfter inspecting results the authors exclude outliers using a post hoc rule that increases the effect size. Which problem is most salient ^[Flexible data cleaning that capitalizes on chance]\n\n* (A) Registered Report compliance  \n* (B) Predefined exclusion criteria  \n* (C) Flexible data cleaning that capitalizes on chance  \n* (D) Correct robustness analysis  \n\n\n\n**Question 5**\n\nAuthors try many plausible models with different covariates and transforms and present only the one that yields p below 0.05. This primarily illustrates ^[Model fishing]\n\n* (A) Randomization failure  \n* (B) Correct multiple testing adjustment  \n* (C) Model fishing  \n* (D) Proper sensitivity analysis  \n\n\n\n**Question 6**\n\nA moderation effect is described as predicted but the interaction was considered only after seeing the data. What is this called ^[HARKing]\n\n* (A) Preregistration  \n* (B) Outcome switching  \n* (C) Optional stopping  \n* (D) HARKing  \n\n\n\n**Question 7**\n\nWith ten independent tests at alpha equals 0.05 and no adjustment what is the approximate probability of at least one false positive ^[About 40 percent]\n\n* (A) About 10 percent  \n* (B) About 95 percent  \n* (C) About 5 percent  \n* (D) About 40 percent  \n\n\n\n**Question 8**\n\nWhich statement best captures the purpose of preregistration in this course ^[Specify key decisions in advance to separate confirmatory from exploratory work and reduce undisclosed flexibility]\n\n* (A) Prevent all deviations from the plan  \n* (B) Guarantee replication success  \n* (C) Ban exploratory analyses  \n* (D) Specify key decisions in advance to separate confirmatory from exploratory work and reduce undisclosed flexibility  \n\n\n\n**Question 9**\n\nWhich preregistration description is strongest ^[Primary outcome PSQI total; hypothesis treatment lowers PSQI versus control; N equals 200 with a fixed stopping rule; exclusions preregistered attention check; analysis linear model with prespecified covariates; alpha equals 0.05; secondary outcomes labeled exploratory]\n\n* (A) We will choose outcomes after data collection to reduce noise  \n* (B) Primary outcome PSQI total; hypothesis treatment lowers PSQI versus control; N equals 200 with a fixed stopping rule; exclusions preregistered attention check; analysis linear model with prespecified covariates; alpha equals 0.05; secondary outcomes labeled exploratory  \n* (C) We will analyze outcomes using appropriate models and stop when effects stabilize  \n* (D) We predict positive effects on several variables and will include covariates as needed  \n\n\n\n**Question 10**\n\nWhich statement about Registered Reports is accurate ^[In principle acceptance before data collection commits publication if the approved protocol is followed regardless of outcome]\n\n* (A) In principle acceptance before data collection commits publication if the approved protocol is followed regardless of outcome  \n* (B) They eliminate the need to share data or code  \n* (C) Acceptance depends on obtaining p below 0.05  \n* (D) Methods are reviewed only after results are known  \n\n\n\n:::\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Show explanations]\n\n```\n**Question 1**\n\nIf average power is about 35 percent then only about 35 percent of studies should be significant even when effects are real. Much higher rates suggest selection on significance and QRPs\n\n**Question 2**\n\nPeeking and topping up until a threshold inflates the Type I error unless a sequential design with error spending is prespecified\n\n**Question 3**\n\nReporting only significant outcomes increases false positives and overstates effects in the visible record\n\n**Question 4**\n\nData dependent exclusion rules raise false positives and bias estimates\n\n**Question 5**\n\nUndisclosed multiple testing across specifications inflates the effective Type I error and effect estimates\n\n**Question 6**\n\nHypothesizing after results are known presents post hoc explanations as if they were a priori predictions\n\n**Question 7**\n\nThe family wise error is 1 minus 0.95 to the power of 10 which is about 0.40\n\n**Question 8**\n\nPreregistration clarifies plans and reduces hidden degrees of freedom. It does not forbid exploration or ensure positive results\n\n**Question 9**\n\nSpecific precise and comprehensive plans reduce undisclosed flexibility and clarify confirmatory versus exploratory analyses\n\n**Question 10**\n\nRegistered Reports decouple publication from results and move review upstream to focus on theory and design quality\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n:::\n\n\n# Tutorial\n\n## Assignment 1: Spot the Practice — QRPs or Good Methods?\n\nBelow are short methods/results fragments from fictional studies. Read each one closely. For each fragment:\n\n1) Identify whether it (potentially) shows a **questionable research practice (QRP)** or **good practice**.  \n2) Name the specific issue (e.g., optional stopping, outcome switching, flexible data cleaning, model fishing, HARKing, or good practice).  \n3) Explain **why** it matters.  \n4) Propose a **minimal fix** (what the authors should have done or reported).  \n5) Decide whether the claim should be labeled **confirmatory** or **exploratory** in the write-up.\n\n**Fragment A**\n\n> Our sample consisted of two cohorts of first-year bachelor's students: 74 students enrolled in 2023, and 82 students enrolled in 2024. We found a statistically significant effect, t(154), p = .047, which we interpret as evidence that the intervention improves well-being.\n\n**Fragment B**\n\n> **Experimental Design** We randomly assigned participants to be in the social support or control condition. The social support condition received daily encouraging messages, supposedly sent by another participant in the study. The control condition received daily informative messages, taken from WikiPedia.\n> **Measurements** We measured participants' stress, mood, sleep, productivity, and affect variability using validated self-report questionnaires. \n> **Results** In line with predictions, participants in the social support condition showed a significant improvement in mood, p = .03.\n\n**Fragment C**\n\n> Participants with unusually fast reaction times (log reaction time < M − 2 SD; n = 4) were considered outliers and were excluded from the study.\n\n**Fragment D**\n\n> We tested several plausible specifications (adding/removing covariates such as age, SES, hours worked; linear vs. log transforms). The model controlling for age and using a log transform yielded a significant effect (p = .041), so we focus on this specification below. Other models are not shown.\n\n**Fragment E**\n\n> **Introduction**: The present study hypothesized that herbal tea would increase sleep quality.\nOur sample consisted of patients who presented with disturbed sleep but were not considered eligible for other medical treatment.\n> **Results**: Consistent with expectations, we found a significant effect of herbal tea on sleep quality for participants who scored high on baseline stress, p = .02.\n\n**Fragment F**\n\n> Before data collection, we preregistered our hypotheses, primary outcome (PSQI total score), stopping rule (N = 200), and analysis plan (linear model with preregistered covariates: age, gender). We also specified exclusion criteria (failed attention check; PSQI missingness > 20%). Deviations: We added one robustness check using a median-split sensitivity analysis (exploratory; reported in Supplement). Data, code, and materials are available on OSF (anonymized).\n\n**Fragment G**\n\n> The study protocol (theory, design, and analysis) received in-principle acceptance prior to data collection. Results were published regardless of outcome, provided fidelity to the approved plan. The primary effect was not significant (p = .21); exploratory analyses are labeled and reported in the Supplement.\n\n\nDiscuss with your group:\n\na. Did you all agree on the label (QRP vs. good practice) and the specific issue?  \na. Which fixes would you prioritize if the authors could change only one thing?  \na. How would preregistration or a Registered Report have changed the design, analysis, or write-up?\n\n\n## Assignment 2: Preregistration Audit — Make It Specific\n\nBelow is an excerpt from a **vague** preregistration. Your task is to (i) spot ambiguities and (ii) rewrite the excerpt so it is **specific, precise, and comprehensive**.\n\n**Vague preregistration excerpt:**\n\n> We will test whether our workshop improves student success. We’ll recruit around 150 students and stop once effects stabilize. We’ll measure several outcomes related to performance and well-being and analyze them using appropriate models. Outliers will be excluded. Demographic control variables will be included.\n\n**Your tasks:**\n\n1) Underline every ambiguity, pinpoint what the risk of QRPs is\n2) Rewrite the preregistration to make it more robust against QRPs. Pinpoint how each change improves the preregistration.\n\n\n```{=latex}\n\n\\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Hints]\n\n```\n\n\nConsider specifying:\n\n- **Primary outcome** (exact variable/scale and scoring); **secondary outcomes** (if any).  \n- **Hypotheses** (directional; one line per test).  \n- **Sampling plan and stopping rule** (target N; any interim looks; conditions for stopping).  \n- **Inclusion/exclusion** and **outlier rules** (exact thresholds, applied blindly if feasible).  \n- **Analysis plan** (model form, covariates, sidedness, alpha, multiple-testing plan).  \n- **Deviations policy** (how you will document any changes).  \n- Distinguishing which analyses are **confirmatory** and which are **exploratory**.\n\n\n```{=latex}\n\n\\end{tcolorbox}\n```\n\n\n\n\n## Psilocybin Liberates the Entrenched Brain?\n\nIn recent years, enthusiasm has grown for psychedelics (like magic mushrooms) as a treatment for depression and other mental health problems.\nHowever, the methodological rigor of studies that find support for psychedelics' efficacy has been severely criticized.\nProfessor [Eiko Fried](https://eiko-fried.com/), one of the main contributors to the methodological critiques, engaged in a real-life exercise, very similar to the one you just completed.\n\nWith your group, pick one of the following sources:\n\n* [Eiko Fried's blog post](https://eiko-fried.com/psilocybin-2022-study-remains-uncorrected/), summarizing the critiques\n* The original [authors' rebuttal of the critiques](https://osf.io/preprints/psyarxiv/pdbf5_v1) (which is separated into seven \"Points\"; pick one point)\n\nDiscuss: Do you find the arguments (for or against the presence of QRPs) persuasive? Which QRPs do you recognize?\n\n## Mini Registered Report Pitch\n\nWith your group, prepare a single presentation slide for an **imaginary study** that you could use for your portfolio assignment (i.e., you can use the same hypothesis as for your portfolio).\n\nDescribe the:\n\n- **Theory** (1–2 sentences) and **confirmatory hypothesis** (directional).  \n- **Study Design** (sampling strategy, assignment or not, sample size, stopping rule, exclusion criteria).  \n- **Primary outcome** (exact measure) and **analysis plan** (model, covariates, sidedness, alpha, multiple-testing plan if applicable).  \n- **Transparency** (data/code/materials; prereg link to be created; planned deviations policy).\n\nIf there is time, groups present and receive peer feedback focusing on **clarity**, **testability**, and **reducing flexibility** at design time.\n",
    "supporting": [
      "Week_13_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}