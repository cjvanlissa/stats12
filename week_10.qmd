# assumptions

```{r, include = FALSE}
source("R/booktem_setup.R")
source("R/my_setup.R")
```

# Why Assumptions Matter

Every time we use a statistical model to describe data,
we make certain simplifying assumptions.
If these assumptions are met, the model is a good representation of the data (descriptive statistics),
and we can make valid inferences about the population based on the model's parameters (inferential statistics).
However, when these assumptions are violated, the model is a bad descriptor of the data, and inferences based on the model can be misleading or difficult to interpret.

To de-mystify assumptions, let's examine one of the simplest statistical models possible: the normal distribution.
The normal distribution is a statistical model to describe the distribution of scores on a variable (or: in the population),
and its two parameters are the mean and standard deviation.
If I draw a random sample of 1000 participants from the population of the Netherlands,
their observed heights might be distributed as in the histogram below.
I could use the normal distribution as a model for these data, and it would do a pretty good job (see the red normal distribution).
In this case, my assumption is that *height is normally distributed around a mean* $\mu$ *and with standard deviation* $\sigma$, or:

$$
\text{Height} \sim N(\mu, \sigma)
$$

If this assumption holds, the mean and standard deviations will be pretty good descriptive statistics of the distribution of data in the sample.
If I assume that height is also normally distributed in the population, and that my sample is representative - then my sample statistics are also pretty good estimators for the population parameters.

```{r}
set.seed(1)
x <- rnorm(1000, mean = 175, sd = 15)
outlrs <- (x < 25 | x > 215)
while(any(outlrs)){
  x[which(outlrs)] <- rnorm(sum(outlrs), mean = 175, sd = 15)
  outlrs <- (x < 25 | x > 215)
}
library(ggplot2)
# parameters that will be passed to ``stat_function``
n = length(x)
mean = mean(x)
sd = sd(x)
binwidth = 10 # passed to geom_histogram and stat_function
set.seed(1)
df <- data.frame(x = x)

ggplot(df, aes(x = x, mean = mean, sd = sd, binwidth = binwidth, n = n)) +
    theme_bw() +
    geom_histogram(binwidth = binwidth, 
        colour = "white", fill = "cornflowerblue", size = 0.1) +
stat_function(fun = function(x) dnorm(x, mean = mean, sd = sd) * n * binwidth,
    color = "darkred", size = 1) + labs(x = "Height (cm)", y = "Frequency")
```

Now imagine that I draw a convenience sample of 200 members of my local basketball association (figure below).
Do you think I can assume that their heights will be normally distributed?
Why (not)?
Do you think these individuals will be representative of the Dutch population?
Will they be representative of the population of Dutch basketball players?
If the assumption that these scores are normally distributed is violated,
then the mean and standard deviation of the normal distribution will be poor descriptive statistics.
Moreover, these sample statistics will be poor estimators of the population parameters.

```{r}
set.seed(2)
x <- rnorm(200, mean = 190, sd = 15)
outlrs <- (x < 185 | x > 240)
while(any(outlrs)){
  x[which(outlrs)] <- rnorm(sum(outlrs), mean = 175, sd = 15)
  outlrs <- (x < 185 | x > 240)
}
# parameters that will be passed to ``stat_function``
n = length(x)
mean = mean(x)
sd = sd(x)
binwidth = 10 # passed to geom_histogram and stat_function
set.seed(1)
df <- data.frame(x = x)

ggplot(df, aes(x = x, mean = mean, sd = sd, binwidth = binwidth, n = n)) +
    theme_bw() +
    geom_histogram(binwidth = binwidth, 
        colour = "white", fill = "cornflowerblue", size = 0.1) +
stat_function(fun = function(x) dnorm(x, mean = mean, sd = sd) * n * binwidth,
    color = "darkred", size = 1) + labs(x = "Height (cm)", y = "Frequency")
```

## Assumptions for Linear Regression

The same principles apply to more complex models than the normal distribution.

whether involving a single predictor or multiple predictors, rests on a set of statistical assumptions. These assumptions ensure that the estimated slopes, standard errors, and p-values behave as expected and can be interpreted with confidence.




This section outlines the key assumptions of linear regression, explains why each one matters, and shows how to check whether they are likely to hold in your data.

### 1 Linearity – the “straight-line” assumption

Linear regression assumes that the relationship between each predictor (*X*) and the outcome (*Y*) is linear, that is, that changes in *X* correspond to proportional changes in *Y*. The estimated slope tells us how much *Y* is expected to increase (or decrease) for a one-unit increase in *X*, but this only holds if the relationship is approximately linear.

**Why it matters**  
When the true relationship is non-linear, fitting a straight line can misrepresent the nature or strength of the association. This can lead to inaccurate slope estimates and misleading conclusions about the predictor’s effect.

**How to check**  
1. Create a scatterplot of *Y* against each *X* variable.  
2. Add a straight trend line (e.g., “fit line at total” in your software).  
3. Visually assess whether the line aligns with the overall pattern of the data points.

**When it is violated**  
Linearity is likely violated if the plotted points form a clear curve, wave, or other systematic pattern that deviates from a straight path.

### 2 Correct Scale of Measurement – keeping variables on appropriate scales

Linear regression requires the outcome variable (*Y*) to be measured on a continuous scale. This means *Y* should represent quantities with meaningful and consistent differences—such as test scores, height, or income. Predictors (*X* variables) can be either continuous or binary (e.g., coded as 0 = control, 1 = treatment).

**Why it matters**  
If *Y* is categorical or ordinal, the assumptions underlying the regression model no longer apply. The model may incorrectly interpret category labels as numeric distances, and key outputs—such as residuals and p-values—lose their interpretability.

**How to check**  
Review the metadata or variable definitions (e.g., via *Variable View* in SPSS) to confirm that:
- The outcome (*Y*) is coded as a continuous numeric variable.
- Categorical predictors are either dummy-coded or otherwise appropriately handled.

**When it is violated**  
The assumption is violated when:
- *Y* is ordinal or nominal (e.g., Likert scales, categories like “red”, “green”, “blue”).
- An ordinal *X* is treated as numeric without justification, leading the model to assume equal spacing between categories.


### 3 Reliable Predictors – measurement consistency of explanatory variables

Linear regression assumes that all predictor variables (*X*) are measured with a reasonable degree of reliability. Inaccurate or inconsistent measurement introduces noise, which can attenuate the estimated relationship between *X* and *Y*. As a result, regression coefficients may be biased toward zero, and the model may attribute true effects to random error.

**Why it matters**  
When predictors are unreliable, the estimated slopes become less trustworthy. Even in large samples, measurement error in *X* can severely compromise the interpretability of regression results, leading to underestimation of effect sizes and increased standard errors.

**How to check**  
- For predictors based on multiple items (e.g., survey scales), compute internal consistency (e.g., Cronbach’s α). Values below 0.70 often indicate problematic measurement.
- If repeated measurements of the same predictor are available, examine the test–retest correlation. High correlation supports reliability.

**When it is violated**  
This assumption is violated when:
- A predictor contains high random measurement error.
- Multi-item scales exhibit low internal consistency.
- Temporal stability of repeated measures is weak (e.g., inconsistent responses across time).

### 4 Homoscedasticity – constant variance of residuals

Linear regression assumes that the variance of the residuals remains approximately constant across all levels of the predicted values. This condition, known as *homoscedasticity*, implies that the model has equal predictive accuracy across the full range of the outcome.

**Why it matters**  
When residuals fan out or contract as the predicted values increase, this indicates *heteroscedasticity*. In such cases, the standard errors may be inaccurate, which undermines the reliability of p-values and confidence intervals.

**How to check**  
1. Save the residuals from the fitted model.  
2. Create a scatter plot of residuals versus predicted values.  
3. Evaluate whether the spread of residuals appears approximately constant across the range of predicted values.

**When it is violated**  
This assumption is violated when:
- The residuals become more dispersed or more concentrated as the predicted value increases.
- The residual-versus-predicted plot reveals a funnel-like or cone-shaped pattern rather than a uniform band.

### 5 Independence of Observations – each case must stand alone

Linear regression assumes that every row in the dataset represents an independent observation. This means that the values in one row should not be systematically related to the values in any other.

**Why it matters**  
When observations are *clustered*—for example, when data come from students in the same classroom, patients treated by the same clinician, or repeated measurements from the same individual—the assumption of independence is violated. In such cases, the residuals are correlated, and the model may underestimate the true variability, leading to overconfident conclusions.

**How to check**  
- Consider the study design: Were the data collected from naturally grouped or repeated units, such as individuals within teams, families, schools, or measured over time?

**When it is violated**  
This assumption is likely to be violated when:
- Observations are nested within a shared context (e.g., students within schools).
- The same individual or unit appears multiple times in the dataset.
- There is a known time-based or spatial structure to the data.

### 6 Normality of Residuals – bell-shaped “leftovers”

Linear regression assumes that the residuals, the differences between the observed and predicted values, are approximately normally distributed. This assumption underpins the validity of *t*-tests and confidence intervals, particularly in small samples.

**Why it matters**  
When the residuals deviate strongly from normality (e.g., they are skewed or have heavy tails), the inference based on the regression model may be misleading. Standard errors, *p*-values, and confidence intervals rely on this assumption when the sample size is modest.

**How to check**  
- Save the residuals and inspect their distribution using a **histogram** or a **Q–Q plot**.  
- For very small datasets (e.g., *N* < 50), formal tests such as the **Shapiro–Wilk test** can be used to assess normality.

**When it is violated**  
This assumption is often violated when:
- The outcome variable is highly skewed or bounded
- There are extreme values in the outcome that disproportionately influence the residuals.  
- The sample size is small and the residual pattern does not resemble a bell-shaped curve.

### 7 No Outliers – prevent one case from dominating the model

Linear regression assumes that no single observation exerts excessive influence on the model. An extreme case—either in terms of its *X*-values (high leverage) or its effect on the line (high influence)—can distort slope estimates, standard errors, and *p*-values.

**Why it matters**  
Outliers can pull the regression line toward themselves, leading to misleading interpretations. Even a single influential point can change the direction, strength, or significance of a predictor's effect.

**How to check**  
- Use **box plots** to identify values that lie beyond 1.5 times the interquartile range.  
- Calculate diagnostic statistics such as **leverage**, **Cook’s distance**, or **DFITS** to detect points with unusual influence.

**When it is violated**  
This assumption may be violated when:
- A case lies far from the bulk of the data on one or more predictors.  
- The residual for a single observation is large relative to others.  
- Diagnostic measures flag a case as both high-leverage and high-influence.



### 8 No Multicollinearity – applies only to multiple regression

Multiple regression assumes that each predictor contributes uniquely to the explanation of the outcome variable. When two or more predictors are highly correlated, the model struggles to distinguish their individual effects. This overlap inflates standard errors, making coefficient estimates unstable and difficult to interpret.

**Why it matters**  
Multicollinearity undermines the precision of regression coefficients. When predictors convey redundant information, the model’s ability to estimate each slope independently deteriorates. This can lead to wide confidence intervals, non-significant p-values, or coefficients with counterintuitive signs.

**How to check**  
- Examine the **Variance Inflation Factor (VIF)** for each predictor. A common guideline is that values above 5 may indicate problematic overlap.  
- Inspect the **condition index**; values above 30 often signal severe collinearity among predictors.

**When it is violated**  
This assumption may be violated when:
- Two or more predictors are strongly correlated (e.g., income and years of education).  
- VIF values are unusually high or the condition index exceeds standard thresholds.  
- The inclusion of additional predictors drastically alters the estimated slopes or increases their standard errors.


### Putting It All Together

Before interpreting regression results, it is essential to verify that the key assumptions are reasonably met. A structured diagnostic workflow helps ensure the validity and interpretability of the estimates:

1. **Visual diagnostics** – Begin with graphical checks for linearity, constant variance, outliers, and normality of residuals. Plots often reveal violations at a glance.  
2. **Statistical diagnostics** – Follow up with numerical checks, such as the Variance Inflation Factor (VIF) for multicollinearity and formal tests for heteroskedasticity or autocorrelation when appropriate.  
3. **Assessment of model structure** – Consider whether the data meet requirements for independence and correct scale of measurement. 

Only when these assumptions are adequately addressed can the reported p-values, confidence intervals, and regression coefficients be interpreted with confidence.

