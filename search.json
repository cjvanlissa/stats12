[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 1 and 2",
    "section": "",
    "text": "1 Overview\nThis GitBook covers the basics of statistics and data analysis. The ability to extract insights from data is an essential skill for both academic and non-academic work, and “data literacy” is increasingly important in a world where data are collected about every aspect of our lives. In this book, you will be able to independently analyze data, interpret and report your findings, and assess the results of analyses performed by others, such as you might find in scientific articles.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Statistics 1 and 2",
    "section": "1.1 Learning goals",
    "text": "1.1 Learning goals\nThis book covers the following learning goals:\n\ncompute and interpret commonly used descriptive statistics such as the sample mean, the median, the mode, variance and standard deviation, the standard error, and the correlation coefficient.\nrecognize different probability distributions such as the normal distribution, and make computations for these probability distributions.\nexplain the essential aspects of null-hypothesis significance testing, including sampling distributions, Type I and Type II errors, one-tailed versus two-tailed testing, and statistical power.\napply different statistical tests such as the Z-test, the one sample t-test, the one way Between Subjects Analysis of Variance test, and statistical tests related to (multiple) linear regression analysis with continuous and categorical predictors; and clarify the statistical and/or methodological assumptions that apply to the techniques that are discussed in this course.\nexplain basic concepts in regression analysis, including: linear association, least-squares estimation, explained variance, Multiple R, multiple correlation, adjusted R-square, raw and standardized regression coefficients, model-comparison tests, predicted scores, residuals and the assumptions;\nchoose the appropriate analysis technique for answering a specific research problem from the range of techniques that are covered in the course.\nuse the software package SPSS to perform several statistical data analyses and be able to correctly interpret and report the output to an informed audience (e.g., Liberal arts students, researchers from the social sciences/business and economics/cognitive neuroscience).\ndraw valid conclusions from the results of empirical data analyses given specific research questions envisaged.\napply statistical tests in the context of multiple linear regression models with interaction terms and logistic regression models; interpret the corresponding output.\ndescribe the concepts of probabilities, odds and logits; describe the relationship between the three scales; transform one into another (formulae are provided).\napply statistical tests in the context of factorial ANOVA, ANCOVA and Analysis of Repeated measures; interpret the corresponding output; and calculate and interpret effect size estimates relevant for these statistical techniques (e.g., (partial) eta squared)\napply statistical tests in the context of multiple linear regression models with interaction terms and interpret the corresponding output.\ngauge the reliability of measurements from questionnaires and identify problematic items.\nexplore the dimensionality of questionnaire data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#using-this-gitbook",
    "href": "index.html#using-this-gitbook",
    "title": "Statistics 1 and 2",
    "section": "1.2 Using this GitBook",
    "text": "1.2 Using this GitBook\nThis GitBook is “Open Educational Material”. It is intended to replace conventional statistics textbooks, for free.\nAll essential information is contained within this GitBook. However, it is recommended that you download several materials to your local hard drive, and use them from your your local computer throughout the course:\n\nDownload all data files for the tutorials\n\nSave the ZIP archive to your drive\nRight-click the downloaded file, and select “Extract here” (or similar option, depending on your operating system)\nYou should now have a folder with the data files.\n\nDownload all lecture PDFs\nOptional: Download the PDF of the book",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Statistics 1 and 2",
    "section": "1.3 Software",
    "text": "1.3 Software\nBy default, it is assumed that you will be making the exercises in this book using the commercial program ‘SPSS’.\nIt is important to note that there are free alternatives to SPSS; you might consider using these on your own computer, instead of buying a license for SPSS:\n\nPSPP, which is designed to be nearly identical to SPSS with all the same basic functionality: https://www.gnu.org/software/pspp/pspp.html\n\nIf the PSPP website is down, the Windows installer is here: https://sourceforge.net/projects/pspp4windows/\n\nJASP, which is more modern, looks nicer and is very easy to use – but looks less similar to SPSS: https://jasp-stats.org/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#contributing-fixing-errors",
    "href": "index.html#contributing-fixing-errors",
    "title": "Statistics 1 and 2",
    "section": "1.4 Contributing / Fixing Errors",
    "text": "1.4 Contributing / Fixing Errors\nThis book is a work in progress, so you might find errors. Please help me fix them! The best way is to open an issue on github that describes the error. You are also welcome to suggest fixes directly by opening a pull request, if you know how to.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#credit",
    "href": "index.html#credit",
    "title": "Statistics 1 and 2",
    "section": "1.5 Credit",
    "text": "1.5 Credit\nThis book was authored by Caspar J. Van Lissa. Its code and layout are derived from Lisa DeBruine’s “booktem” (DeBruine & Lakens, n.d.).\nAlso see: https://psyteachr.github.io/\n\n\n\n\nDeBruine, L. M., & Lakens, D. (n.d.). Methods Book Template. Retrieved June 4, 2025, from https://debruine.github.io/booktem/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "statistics1.html",
    "href": "statistics1.html",
    "title": "\n2  Statistics 1\n",
    "section": "",
    "text": "2.1 Course Description\nThe course Statistics 1 covers the basics of statistics and data analysis. This GitBook contains all relevant information about this course. It is assumed that every student reads it carefully. If you have any questions, first consult this GitBook, then ask a fellow student, and only if your question is still not answered, then contact the course coordinator.\nCommunication about the course occurs through Canvas (Login with your student ID and password).\nIn the course, the following techniques will be discussed:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "statistics1.html#course-description",
    "href": "statistics1.html#course-description",
    "title": "\n2  Statistics 1\n",
    "section": "",
    "text": "Information on the use of SPSS and interpretation of the output.\nDescriptive statistics;\nNormal distribution; standard scores;\nSampling distributions; Z and t distributions;\nHypothesis tests and confidence intervals for the mean.\nThe power of a statistical test.\nOne way Between Subjects Analysis of Variance.\nLinear regression analysis",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "statistics1.html#learning-goals",
    "href": "statistics1.html#learning-goals",
    "title": "\n2  Statistics 1\n",
    "section": "\n2.2 Learning goals",
    "text": "2.2 Learning goals\n After taking this course, students will be able to…\n\ncompute and interpret commonly used descriptive statistics such as the sample mean, the median, the mode, variance and standard deviation, the standard error, and the correlation coefficient.\nrecognize different probability distributions such as the normal distribution, and make computations for these probability distributions.\nexplain the essential aspects of null-hypothesis significance testing, including sampling distributions, Type I and Type II errors, one-tailed versus two-tailed testing, and statistical power.\napply different statistical tests such as the Z-test, the one sample t-test, the one way Between Subjects Analysis of Variance test, and statistical tests related to (multiple) linear regression analysis with continuous and categorical predictors; and clarify the statistical and/or methodological assumptions that apply to the techniques that are discussed in this course.\nexplain basic concepts in regression analysis, including: linear association, least-squares estimation, explained variance, Multiple R, multiple correlation, adjusted R-square, raw and standardized regression coefficients, model-comparison tests, predicted scores, residuals and the assumptions;\nchoose the appropriate analysis technique for answering a specific research problem from the range of techniques that are covered in the course.\nuse the software package SPSS to perform several statistical data analyses and be able to correctly interpret and report the output to an informed audience (e.g., Liberal arts students, researchers from the social sciences/business and economics/cognitive neuroscience).\ndraw valid conclusions from the results of empirical data analyses given specific research questions envisaged.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "statistics1.html#course-schedule",
    "href": "statistics1.html#course-schedule",
    "title": "\n2  Statistics 1\n",
    "section": "\n2.3 Course Schedule",
    "text": "2.3 Course Schedule\nThe official course schedule is available on TimeEdit. The information below might go out of date. For a general overview of the content, see below:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "statistics1.html#attendance",
    "href": "statistics1.html#attendance",
    "title": "\n2  Statistics 1\n",
    "section": "\n2.4 Attendance",
    "text": "2.4 Attendance\nAttendance is mandatory based on our experience that students who actively participate tend to pass the course, whereas those who do not tend to drop out or fail. All lectures and practicals ‘build’ on each other, so if you have to miss either one, absolutely make sure you have caught up with the materials before the next session.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "statistics1.html#study-load",
    "href": "statistics1.html#study-load",
    "title": "\n2  Statistics 1\n",
    "section": "\n2.5 Study Load",
    "text": "2.5 Study Load\nBelow is a breakdown of the expected study load:\n\n\n\n\nActivity\nDuration\nTimes\nTotal\n\n\n\nSynchronous\n\n\n\n\n\nLectures\n2.0\n14\n28\n\n\nTutorials\n2.0\n14\n28\n\n\nAsynchronous\n\n\n\n\n\nKnowledge clips\n1.0\n14\n14\n\n\nFormative tests\n1.0\n14\n14\n\n\nReading time\n1.0\n14\n14\n\n\nStudying\n1.0\n1\n37\n\n\nAssessment type(s)\n\n\n\n\n\nPortfolio\n15.0\n2\n30\n\n\nExam\n1.5\n2\n3\n\n\nTotal\n\n\n168\n\n\nECTS\n\n\n6",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "statistics1.html#staff",
    "href": "statistics1.html#staff",
    "title": "\n2  Statistics 1\n",
    "section": "\n2.6 Staff",
    "text": "2.6 Staff\nCoordinator:\ndr. Caspar J. van Lissa\nLab sessions\nAmirali Rezazadeh",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "statistics1.html#teaching-philosophy",
    "href": "statistics1.html#teaching-philosophy",
    "title": "\n2  Statistics 1\n",
    "section": "\n2.7 Teaching Philosophy",
    "text": "2.7 Teaching Philosophy\n\nStudent-paced learning: instead of having traditional lectures where you sit and listen for two hours, you will watch relatively short (~45 minutes) lecture videos to prepare for class. In class, we use the material from these videos to guide discussions, make exam questions, and work on your portfolios.\nChallenge-based learning: a substantial part of your grade is based on your ability to apply the techniques you’ve learned to a real research question in several portfolio assignments. You can choose your own research question, can find your own dataset (or use a default dataset), and work on a topic that actually interests you.\nThroughout the course, you will be working in small learning teams to promote interaction among students, peer support, and accountability. Learning to work effectively in groups is an important skill; we will focus on group skills in the first lecture.\n\n\n2.7.1 Why group assignments?\nContact with fellow students is a key aspect of the university experience. We want to stimulate you to engage with the material and with one another. Therefore, the portfolio assignments are made in groups. There are also aspects of learning in groups that can really improve your knowledge, like peer feedback. To ensure that every group member pulls their weight, the final exam tests each student’s individual comprehension of all material covered in the portfolios.\nGroups comprise 3-5 members and are assigned randomly when the course starts. However, it is allowed to switch with a consenting member of another group, or to join/merge with another small group if your group has become smaller than 3 members. There are three portfolio registration deadlines. Before these deadlines, one group member must submit the definitive group composition via a Google form.\n\n2.7.2 Why use portfolio assessment?\nPortfolio assignments are well-suited for a skills-based course like Statistics 1. They also take a lot of the pressure off because you can work at your own pace, and keep improving the work until it is good enough. We entrust you with the responsibility of making these portfolio assignments in good faith, without instrumental assistance from outside your group or plagiarism, so I kindly ask you to make good on this trust, and hand in original work to show what you’ve learned.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "statistics1.html#grading",
    "href": "statistics1.html#grading",
    "title": "\n2  Statistics 1\n",
    "section": "\n2.8 Grading",
    "text": "2.8 Grading\n Your grade is based on two components:\n\n\nA portfolio composed of three assignments made in groups, and\n\nAn individual exam, split into three sessions, to test comprehension of the material covered in the portfolios.\n\nA grade of 5.5 or higher is required for both components to pass the course.\nThe first occasion for the exam is split into three sessions, administered throughout the semester, for the following reasons:\n\nTo reduce study load by administering small tests shortly after the material is taught\nTo ensure continued engagement with the course\nTo give students feedback on their current level of understanding\n\nWhile you do receive an informal grade for each session, the final grade is simply calculated based on your correct answers in all sessions. If that grade falls below 5.5, you can take a resit which covers the material of the entire exam (all 3 sessions).\n\n2.8.1 Portfolios 40% (2 x 20%)\nYou work on the portfolio assignments with your group, both during the lab sessions and outside of class. For each assignment, register your group membership before the set deadline at http://tiny.cc/stats12_portfolio. You hand in your group’s portfolio assignment before the set deadline, at which point it is graded. If your grade is below the passing level of 5.5, your group will have the opportunity to revise the portfolio based on teacher feedback to receive a maximum grade of 6.\nGroups should equally distribute the work load for the portfolio assignments. In case doubts are raised about the equal distribution of labor in a particular group, the portfolio assignment in question will be supplemented with individual oral examination and an individual grade, which can not exceed the original grade for the group assignment. In other words, failing to distribute the work properly can not have positive effects, but it can have negative effects on your grade. To prevent this, make clear agreements about the distribution of work with your group mates.\n\n2.8.2 Exam 60%\nTo make sure that all students are equally involved in the making of the portfolio assignments, an individual exam assesses comprehension of the material covered therein. It is a digital multiple choice exam, split into three sessions, to test comprehension of the material covered in the portfolios.\n\n2.8.2.1 Exam 1\nCovers Week 35 (Introduction to Statistics) up to Week 41 (Hypothesis Testing)\n\n2.8.2.2 Exam 2\nCovers Week 43 (General Linear Model (GLM) I: Bivariate regression) up to Week 47 (Open science and questionable research practices)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "statistics1.html#assignments",
    "href": "statistics1.html#assignments",
    "title": "\n2  Statistics 1\n",
    "section": "\n2.9 Assignments",
    "text": "2.9 Assignments\nBelow is a description of the assignments. For each assignment, every element labeled with a lower case letter is graded fail (0 points), pass (1 point), or excellent (1.5 points). Grades are summed for each assignment, and rescaled from 1-10. The final grade is the average across assignments of the rescaled grades. Note the stated word limit for each section. If you can write a good report with fewer words, that’s fine. If you exceed the word limit however, your grade for that section cannot exceed a pass (1 point).\nThe focus of the assignments should me on motivating, reporting, interpreting, and discussing your analyses. You will get a good grade for well-reasoned and discussed analyses.\nSee the Appendices section to access data sources for the assignment.\n\n2.9.1 Assignment 1\nDescriptive statistics and statistical inference\n\nSelect at least three variables for further analysis, and motivate your selection based on theory, using at least one reference to explain why are you interested in the properties of the selected variables (150 words)\n\nInclude one continuous variable\nInclude one nominal variable\nInclude one ordinal variable\n\n\nDescribe the dataset (200 words + tables/figures)\n\nUse appropriate univariate descriptive statistics for all variables\nPlot data using appropriate plots\nInclude at least one frequency- or crosstable\n\n\nFor a continuous variable:\n\nSelect one or more values with clinical/societal/statistical relevance (i.e., provide some justification for the choice of value)\nUsing probability calculus, calculate and report the probability of observing values that fall below/between/exceed the chosen value(s)\n\n\nFor a continuous variable:\n\nFormulate a specific null- and alternative hypothesis\nReport a one-sample t-test or Z-test for the specific null-hypothesis\nCalculate the probability of comitting a Type II error\n\n\nDiscuss your analyses (300 words)\n\nExplain your rationale for important modeling decisions\nMotivate your choice for the type of statistics and analyses\nDiscuss assumptions\nDiscuss what you have learned from it and how you might improve it\n\n\nUse APA style throughout your report\nReflect on the group process (300 words). Note: I will grade your reflection, not your process. So: if your group’s process is not working well, but you reflect on it properly, you can still get full marks for this component. Use Gibbs’ Reflective Cycle:\n\nDescribe what happened during the group work\nExplain how you felt during the group work\nLook at the good and bad aspects of the group work\nWhat were the obstacles you experienced? What factors contributed to success?\nWhat could you have done differently to improve the situation?\nWhat are your intentions to make the next group assignment work (even) better?\n\n\n\n\n2.9.2 Assignment 2\nGeneral linear model\n\nSelect at least three variables for further analysis, and using at least one reference, explain what research questions you will investigate and what hypotheses you will test (150 words)\n\nInclude one continuous outcome variable\nInclude one continuous predictor\nInclude one nominal or ordinal predictor\n\n\nConstruct a model with only the continuous predictor (200 words)\n\nReport and interpret the different sums of squares\nReport and interpret the explained variance\nConduct a separate correlation analysis. Compare the results with the regression analysis.\n\n\nConstruct a model with only the categorical predictor (200 words)\n\nReport and interpret the model results\nConduct a separate ANOVA or t-test with the same variables, whichever one is suitable. Compare the results with the regression analysis.\n\n\nConstruct a model with both the continuous and categorical predictor (200 words)\n\nReport and interpret the model results\nConduct and report a nested model test\n\n\nDiscuss your analyses (300 words)\n\nExplain your rationale for important modeling decisions\nMotivate your choice for the type of statistics and analyses\nDiscuss assumptions\nDiscuss what you have learned from it and how you might improve it\n\n\nUse APA style throughout your report\nReflect on the group process (300 words). Note: I will grade your reflection, not your process. So: if your group’s process is not working well, but you reflect on it properly, you can still get full marks for this component. Use Gibbs’ Reflective Cycle:\n\nDescribe what happened during the group work\nExplain how you felt during the group work\nLook at the good and bad aspects of the group work\nWhat were the obstacles you experienced? What factors contributed to success?\nWhat could you have done differently to improve the situation?\nWhat are your intentions to make the next group assignment work (even) better?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "statistics1.html#use-of-large-language-models-llms",
    "href": "statistics1.html#use-of-large-language-models-llms",
    "title": "\n2  Statistics 1\n",
    "section": "\n2.10 Use of Large Language Models (LLMs)",
    "text": "2.10 Use of Large Language Models (LLMs)\nHonestly: I advise against using ChatGPT and similar LLMs for this course. Here’s why: LLMs learn from all text on the internet, which includes a lot of text posted by people who do not understand statistics. As a result, my experience is that LLMs produce a lot of plausible sounding nonsense for statistics assignments.\nIf you’re worried about the quality of your writing: that is not graded here. I’d rather have a simple and clear report in imperfect English than a beautifully written AI-fluff piece full of hallucinated nonsense.\nIf you decide to use LLMs, it is your responsibility to thoroughly check its output for logical consistency and correctness. You may not yet have the level of expertise required to know when ChatGPT generates irrelevant nonsense - but the teacher who grades your work does. Consider this carefully when deciding what makes more sense: doing your work manually, making sure each step is correct - or outsourcing it to AI, and then checking its work before submitting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics 1</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "3  Introduction to Statistics",
    "section": "",
    "text": "3.0.1 Population and Sample\nStatistics are more relevant than ever in this digital age, where data about our entire lives is readily available, and software to analyze such data has become extremely user-friendly and freely available. We live in a world where organizations large and small collect data to tailor products and services, and being data literate is becoming increasingly important across industries.\nStatistics allows us to make sense of data and gain valuable insights. It helps us better understand social phenomena, predict sales and optimize marketing strategies, and even explore the relationship between brain activity and behavior. Data analysis is one of the most marketable skills taught at universities.\nBefore we delve deeper into statistics, it’s crucial to distinguish between methods and statistics. Methods refer to the procedures used in research, such as data collection, participant selection, and study design. Statistics, on the other hand, focuses on analyzing the data obtained from these methods.\nTwo fundamental branches of statistics covered in this course are descriptive statistics and inferential statistics. Descriptive statistics involves summarizing and describing the characteristics of a dataset, while inferential statistics allows us to make educated guesses about a larger population based on a smaller sample.\nStatistical modeling is another aspect of statistics where theories are represented mathematically. This enables us to predict important outcomes, such as sales figures, well-being, or the likelihood of neurological disorders. Statistical modeling also allows us to explore data for interesting patterns or to perform tests to answer theoretically driven research questions.\nIn scientific research, statistics can help us test theories. The process of scientific knowledge acquisition is described by the empirical cycle: We start with a theory, from which we derive testable hypotheses. A theory is an abstract system of assumptions about the relationships between constructs. A hypothesis is a concrete statement, derived from the theory, about expected quantitative relationships between measured variables. We then collect data and test the hypothesis. If the hypothesis is refuted, we re-examine the theory and possibly amend it.\nTo lay a foundation for understanding statistics, it’s essential to be familiar with some basic concepts. First, data in the social sciences often come in tabular format (e.g., spreadsheets), where each row represents an individual observation, and each column represents the individuals’ scores on various variables.\nA crucial distinction is the one between population and sample. The population refers to the complete set of objects of interest, such as all people in a country or all students in a class. However, due to practical limitations, we usually do not have access to the population. Instead, we draw a sample from it, which is a subset of the population. Sampling theory establishes the rationale for drawing inferences about a population based on samples. Sample statistics serve as our best estimate of population parameters. If the sample is representative, those estimates will be unbiased. Moreover, we can estimate our uncertainty about the sample statistics as estimates of population parameters. The best way to ensure a representative sample is to use random sampling, where each individual in the population has an equal chance of being included—though in practice, constraints often lead researchers to rely on convenience or stratified sampling, which can limit the strength of our inferences.\nThe distinction between constructs and variables is also important. Constructs are abstract features of interest within a population, like short-term memory, intelligence, or education. Variables, on the other hand, are placeholders that represent specific values associated with these constructs—much like column headers in a spreadsheet. Data then refer to the specific values of a variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "introduction.html#lecture",
    "href": "introduction.html#lecture",
    "title": "3  Introduction to Statistics",
    "section": "\n3.1 Lecture",
    "text": "3.1 Lecture",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "introduction.html#formative-test",
    "href": "introduction.html#formative-test",
    "title": "3  Introduction to Statistics",
    "section": "\n3.2 Formative Test",
    "text": "3.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat is the primary purpose of statistics?\n\nTo create data visualizationsTo collect and store dataTo design research studiesTo summarize and analyze data\n\nQuestion 2\nWhat is the difference between descriptive statistics and inferential statistics?\n\nDescriptive statistics is used in social sciences, while inferential statistics is used in natural sciences.Descriptive statistics involves analyzing data, while inferential statistics involves collecting data.Descriptive statistics summarize data, while inferential statistics involves making informed guesses about parameters in a larger population.Descriptive statistics deals with nominal variables, while inferential statistics deals with ratio variables.\n\nQuestion 3\nWhat is the purpose of statistical modeling?\n\nTo describe the characteristics of a datasetTo summarize data using graphs and chartsTo represent a theory as a testable statistical model.To make educated guesses about a larger population based on a smaller sampleTo predict outcomes and explore patterns in data\n\nQuestion 4\nWhat is the empirical cycle in scientific research?\n\nThe process of summarizing and describing data using statisticsThe process of repeatedly collecting and analyzing dataThe process of formulating a theory, deriving hypotheses, testing these with data, and reflecting on theoryThe process of designing research studies and selecting participants\n\nQuestion 5\nWhat is the distinction between population and sample in statistics?\n\nPopulation refers to inferential statistics, while sample refers to descriptive statistics.Population refers to the complete set of potential participants, of which the sample is a subset.Population refers to all participants in a study, while sample refers all participants who provided complete answers.\n\nQuestion 6\nWhat is the variance?\n\nThe measure of dispersion for scores on a continuous variable.The average distance of observations to the mean.The average squared distance of observations to the mean.The distance between the lowest and highest value on a continuous variable.\n\nQuestion 7\nSix students work on a Statistics exam. They obtain the following grades: 8, 9, 5, 6, 7 and 8. The teacher calculates a measure of central tendency, which is equal to 7.5. Which statistic did the teacher calculate?\n\nStandard deviationMedianMeanMode\n\nQuestion 8\nFor which of the three scatterplots below is the correlation coefficient strongest?\n\nBAC\n\n\n\n\nShow explanations\n\nQuestion 1\nStatistics is the science concerned with developing and studying methods for analyzing data.\nQuestion 2\nDescriptive statistics are calculated based on sample data; inferential statistics involves using those sample statistics to make best guesses about population parameters and quantify uncertainty about those guesses.\nQuestion 3\nStatistical modeling in particular refers to the process of translating a theoretical model into a statistical model whose coefficients can be estimated using data.\nQuestion 4\nThe empirical cycle is a theoretical cyclical model of knowledge production through scientific research, whereby theory gives rise to hypotheses, which are tested in data, after which the theory is revisited based on the results.\nQuestion 5\nPopulation refers to the complete set of potential participants, of which the sample is a subset.\nQuestion 6\nThe variance is the sum of squared distances of observations to the mean, divided by the number of observations minus one. So calculate:\\(S_{X}^2= \\frac{\\sum_{i=1}^nX_i}{n} = \\frac{(7 + 6 + 8 + 6 + 8)}{5} = 7\\)\nQuestion 7\nFirst rule out improbable answers; the variance is not a measure of central tendency, and all grades are pretty close to each other, so it would be impossible for the variance to be that high. We can see what the mode (most common value) is: it’s 8. So we only choose between mean or median. Mean: calculate \\(\\bar{X}= \\frac{\\sum_{i=1}^nX_i}{n} = \\frac{8 + 9 + 5 + 6 + 7 + 8}{6} = 7.17\\)Median: order the numbers, note that there is an odd number, take the average of the two middle numbers. 5, 6, 7, 8, 8, 9 -&gt; 7.5\nQuestion 8\nCorrelation measures linear association, so eliminate option C. Option B shows a very small correlation - probably 0 or maybe .1. So the correct answer is A, which shows a moderate negative correlation.\n\n\n\n\n\n\n{width = 30%}\n\n\n{width = 30%}\n\n\n{width = 30%}\n\n\n\n\nFigure 3.1: Scatterplots",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "introduction.html#in-spss",
    "href": "introduction.html#in-spss",
    "title": "3  Introduction to Statistics",
    "section": "\n3.3 In SPSS",
    "text": "3.3 In SPSS\n\n3.3.1 Instruction Video",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "introduction.html#tutorial",
    "href": "introduction.html#tutorial",
    "title": "3  Introduction to Statistics",
    "section": "\n3.4 Tutorial",
    "text": "3.4 Tutorial\n\n3.4.1 Introducing SPSS\nWelcome everyone to your first lab session for Statistics 1 and 2. Today, we start working with an introduction to SPSS and we calculate a few basic descriptive statistics.\nEach lab session consists of several assignments and includes explanations on how to carry out the analyses in SPSS.\nYou can work at your own pace. If you experience any problems, or if you have any questions, feel free to ask your teacher.\nYou will receive feedback to your answers after you have submitted the practical.\nGood luck!\n\n3.4.1.1 Step 1\nHi there!\nDuring the lab sessions of this course you will learn how to work with the statistics program IBM SPSS (SPSS for short).\nBackground information is given throughout the exercises. We will occasionally refer to additional reading materials for this course, or other sources (e.g., youtube videos).\nIf you’re working from a student workplace, SPSS is already installed. If you’re working from your own computer, you either have to purchase SPSS, or you can use a free alternative (see ?sec-software) - but note that, at this point, the instruction text is still focused on SPSS so if there are any differences it will be your responsibility to figure out how to use your software.\nYour first task is to start the SPSS program. You can easily find SPSS via the Windows Start Menu. SPSS may ask about the coding: use Unicode (button to the left). Then there may be another window open that you can close. In the end you should see an empty spread sheet.\n\n3.4.1.2 Step 2\nNow you’ve got SPSS running, we’re ready to go!\nWe will start with a number of introductory exercises using the data file stressLAS.sav. To obtain this and other tutorial data files, download the GitBook, and open the data folder to find all files.\nOpen the file in SPSS. Proceed as follows: via the op menu follow the route: File -&gt; open -&gt; data. SPSS now opens a new window. Search for the file stressLAS.sav and open the file in SPSS.\n\n3.4.1.3 Step 3\nThe file contains data about a study on - you guessed it - stress.\nMore precisely, it contains data on the following variables:\n\n\nstress: Measures whether the participant experiences stress, and where the stress comes from.\n\nsmoke: Measures the smoking behavior of the participant.\n\nrelation: Whether or not the participant is involved in a long-term romantic relationship.\n\noptim: Measures how optimistic the participant is on a scale of 0 to 50.\n\nsatis: Measures of life satisfaction of the participant on a scale of 0 to 50.\n\nnegemo: The amount of negative emotions on a scale of 0 to 50.\n\n3.4.1.4 Step 4\nAfter opening the data file, you will see the tabs Data View and Variable View at the bottom of your screen.\nMake sure the tab Data View is selected.\nLook at the Data View and describe the data file. What do the rows represent, and what do the columns represent?\n\n3.4.1.5 Step 5\nNow switch to the Variable View tab.\nThe Variable View lists the variables and their properties. We will not discuss all the columns in detail, but focus on the most important ones, which includes: name, label, values, and measure.\nExplain for each of the columns name, label, values, and measure what aspect of the variable it describes. Also explain the difference between variable name and variable label.\n\n3.4.1.6 Step 6\nValue Labels\nFor nominal and ordinal variables we have to indicate what the scores represent; that is, we have to assign so called value labels. Value labels are specified under Values.\nIf you click on values for the variable of interest, and then on the blue button with the three dots on the right, SPSS opens a new window that allows you to view, define, or modify the value labels.\nWhat are the value labels for the Stress and what are they for Smoke?\n\n3.4.1.7 Step 6a\nYou may have noticed that the value labels are missing for the nominal variable Relation.\nAdd the value labels yourself in SPSS such that a score 1 represents “Single” and 2 represents “In a relationship”.\n\n3.4.1.8 Step 7\nEvery variable has a so-called Measurement Level.\nFirst, summarize the measurement levels in your own words (as if you have to explain it to a fellow student). Then, indicate the measurement level for each of the variables of interest (Stress, Smoking, etc.).\n\n3.4.1.9 Step 8\nCongratulations, you have completed your first assignment!\nBefore we proceed make sure that you save the data file (via file &gt; save). Because you changed the data, it is important to save the file under a different name. This way, you don’t risk losing the original data.\nIn the next assignment we will generate descriptive statistics for this data.\n\n3.4.2 Plotting data\n\n3.4.2.1 Step 1\nThe first step in any statistical analysis involves inspection of the data at hand by means of descriptive statistics and/or graphical summaries. Descriptive statistics include the mean, standard deviation, minimum and maximum value. Examples of graphical summaries are bar charts, histograms, and scatter plots.\nIn this assignment we will look at graphical summaries. In particular, we will look at three: bar charts, histogram, and scatter plots.\nYou may use the same data file as for the previous assignments.\n\n3.4.2.2 Step 2\nFirst, we will create a bar chart for Stress.\nProceed as follows:\nGraphs &gt; legacy dialogs &gt; bar Select Simple and click on define Select Stress under Category Axis (i.e., the variable at the x-axis) Then Click on OK and consult the graph in the output\n\n3.4.2.3 Step 3\nYou may have noticed that SPSS by default creates a bar chart with the observed frequency depicted on the y-axis. We will now create a new bar chart and instead ask SPSS to show the percentages on the y-axis.\nProceed as follows:\nGraphs &gt; Legacy Dialogs &gt; Bar Again choose Simple and click on Define Under “Bars represent” choose “% of cases” Click on OK. SPSS will now create a bar chart, where the heights of the bars represent percentages.\n\n3.4.2.4 Step 4\nNext, we will create a histogram for Negative Emotions.\nProceed as follows:\nGraph &gt; Legacy Dialogs &gt; Histogram Select Negative Emotions under variable, and ask SPSS to Display normal curve (check the box). Click on OK.\nInvestigate the histogram; What is shown on the x-axis and what is shown on the y-axis?\nHow to read the histogram:\n\nx-axis: the scores on the negative emotions (here numbers between 0 and 50). bars represent score ranges; the more respondents with a score in that range, the higher the bar.\ny-axis: the observed number of respondents per score range.\n\n3.4.2.5 Step 5\nFinally, we will create a scatter plot for Negative Emotions and Life Satisfaction. Scatter plots are very useful to get a first impression of whether variables are associated.\nCreate a scatter plot as follows:\nGraphs &gt; legacy dialogs &gt; scatter/dot Choose Simple Scatter Select Negative Emotions on the x-axis, and Life Satisfaction on the y-axis Click OK\nConsult the output. Look at the scatter plot and see if you understand the graph.\nHow to read a scatter plot:\n\nx-axis represents the scores on Negative Emotions.\ny-axis represents the scores on Life Satisfaction.\nEach dot in the graph is a case, representing how the case scores on both Negative Emotions and Life Satisfaction.\n\n3.4.3 Quiz\n\nDescribe the first bar chart; What is shown on the x-axis? \nFrequency of Responses\nNumeric scores of Stress\nPercentages of Responses\nCategories of Stress\nIn the first bar chart, what is shown on the y-axis? \nCategories of Stress\nNumeric scores of Stress\nPercentages of Responses\nFrequency of Responses\nWhat’s the approximate proportion of people experiencing work-related stress? \n66%\n70%\n33%\nBased on the bar charts, what can you say about differences in stress levels in the sample? Are most people stressed or not? In other words: How is stress distributed across the three categories? \nEvenly distributed\nMost people report no stress\nMost people report life stress\nMost people report work stress\nDescribe the distribution of Negative Emotions. Are the scores normally distributed (i.e., like a bell-shape)? Really consider why this is / is not the case before checking your answer. \nNot normal\nNormal\nBased on the scatter plot from Step 5, would you expect an association between Negative Emotions and Life Satisfaction? \nSmall positive\nStrong positive\nSmall negative\nNo association\n\n\n3.4.4 Descriptive Statistics\n\n3.4.4.1 Step 1\nAs explained before, the first step in any statistical analysis involves inspection of the data. In the previous assignment we looked at graphical summaries.\nThis assignment shows you how to explore data using descriptive statistics. Descriptive statistics include values such as the mean, standard deviation, the maximum value and the minimum value.\nUse the same data file as for the previous assignments.\n\n3.4.4.2 Step 2\nWe will first take a look at the descriptive statistics for Optimism, Life Satisfaction, and Negative Emotions.\nCompute descriptive statistics as follows:\nAnalyze &gt; Descriptive Statistics &gt; Descriptives\nSelect the variables Optimism, Life Satisfaction and Negative Emotions Now click on OK SPSS will open a new window - the output window - including a table with the descriptives for the selected variables.\n\n3.4.4.3 Step 3\nIn the previous step we computed the average value and standard deviations. However, for nominal and ordinal variables, the average value is meaningless. To explore nominal and ordinal variables we may produce Frequency tables. A frequency table shows the observed percentage for each level of the variable.\nLet’s generate a frequency table for variables Smoke and Relation.\nAnalyze &gt; Descriptive Statistics &gt; Frequencies\nSelect the variables for which you want to have the frequency distribution (i.e., Smoke and Relation) Click OK. SPSS now adds a table with the frequency distributions of the selected variables to the output file.\nNote: SPSS reports percentages and valid percentages. Percentages differ when there are missing values. Because we don’t have missing values here, the numbers are the same. Missing values will be discussed in the next assignment.\n\n\n\n\n\n\n\n\n\n\n3.4.5 Quiz\n\nHow many participants are in the sample? \nWhat is the mean value of Optimism? \nFor which of the variables is the spread in the scores highest? \nSATIS\nNEGEMO\nOPTIM\nThe minimum and maximum observed scores for Negative Emotions were: [ , ].\nWhat percentage of participants is a non-smoker? \nWhat percentage of participants is in a relationship? \n\n\n3.4.5.1 Step 4\nOne of the reasons to first inspect descriptive statistics is to have a first check if there are erroneous values in the data file. Erroneous values are values that are out of range, or impossible given the variable envisaged. For example, a person may have mistyped his/her age (e.g., 511 instead of 51).\nNow it’s your task to check for each variable whether there are erroneous values (out of range values) in the file using descriptive statistics and/or graphs.\nUse the descriptive statistics to find any erroneous values.\nOne way to deal with missing values is by removing the entire case. This is not a recommended practice; however, at this point, it is the only method you have learned.\nTo find the cases that have missing values you may sort the data file on a variable with suspect values from high to low (or low to high).\nThis can be done as follows:\nData &gt; Sort Cases Select the variable on which cases should be sorted Select the cases in descending order Click on OK Go the data view and verify that the cases are now ordered.\nRemove the case(s) (i.e., delete the row from the data file) with invalid values.\n\n3.4.5.2 Step 5\nNow that we’ve “cleaned” the data file it’s time to answer our first research question!\nThe question is: “Are non-smokers in our sample on average more satisfied with their life than smokers?”\nTo answer this question, we need the mean of life satisfaction per smoking group. In order to generate those, we will use the Split File option in SPSS. This is an option in SPSS that allows us to get results for separate groups.\nData &gt; Split File &gt; Compare Groups Select the groups based on the variable Smoke Click OK\nNotice that you don’t see any changes in the data file or anything in the output file yet (!). However, after running the Split file command, SPSS from now will do the analyses per group, as we will see next.\nCompute the mean of Life Satisfaction (via descriptive statistics) and consult the output.\nYou may notice that SPSS provides the means of the non-smoking and smoking group separately. Compare the means for both groups to answer the following questions.\n\n3.4.6 Quiz\n\nWas there an erroneous value in the data file? If so, type the value of that erroneous value here: \nTo answer this question, only use reasoning. If you delete that value, how do you think the mean of that variable will be affected? \nStays the same\nBecomes larger\nBecomes smaller\nTo answer this question, only use reasoning. If you delete that value, how do you think the standard deviation of that variable will be affected? \nStays the same\nBecomes smaller\nBecomes larger\nTo answer this question, only use reasoning. If you delete that value, how do you think the standard deviation of that variable will be affected? \nStays the same\nBecomes larger\nBecomes smaller\nIn this sample, who are more satisfied with life? \nSmokers\nNon-smokers\nDo you think this also holds for the population of all persons? \nNo\nYes\nCan’t tell\n\n\n3.4.7 Missing Values\nThis is a short assignment about missing values.\nMissing values are ‘holes in the data matrix’. Missing data is a common issue in empirical research. Respondents may forget to fill in questions or refuse to answer questions (if the latter is the case, we are in trouble). It is important that missing data are adequately handled in data analysis.\nUse the same data file as for the previous assignments.\nIn the previous assignment we activated the split file option. However, we don’t need this split file in the remaining questions, therefore we have to undo the split file option.\nData &gt; split file Choose “Analyze all cases, do not create groups”\nCompute the frequency distribution of stress. Consult the output, and answer the following questions\n\n3.4.8 Quiz\n\nWhat is the percentage of respondents who experience No Stress? \nWhich type of stress is most common in the sample? \nWork stress\nNo stress\nLife stress\nFor educational purposes only, we will now create missing values in the data file.\nNavigate to the Data view and delete the value for Stress for the first 10 cases. Notice that you only have to delete the scores for the variable Stress, and not the complete case.\nCompute the frequency distribution for Stress again and compare the new table with the previous one.\nExplain what has changed and why.\n\n\nAnswer\n\nWe can see that the values of Percent and Valid Percent have changed and that a ‘missing’ row has been added to the table. It makes sense that the percentages have changed, as there are now missing values. You may have noticed that the values for Percent and Valid Percent differ. Percentage is obtained by dividing the observed frequency by the total (including respondents with a missing value). Valid Percentage is obtained by dividing the observed frequency by the number of respondents with a valid score (thus, not counting the respondents who had a missing value).\n\nImagine I have a sample of 65 participants, with 3 missing value. Of these participants, 15 reported no stress. What is the percentage of no stress, calculated by hand? \nWhat is the percentage out of valid responses (i.e., valid percent), calculated by hand? \n\nBy deleting the values we created empty cells in the data file. SPSS sees these empty cells as system missing. Some researchers instead use specific values to indicate missing values. For example, we may code missing values by 999 if the respondent refused to answer, and 998 if the respondent accidentally skipped the question. These are examples of user missing values, and we have to specify the values to be coded as missing in the Variable view.\nLet’s try this!\nGo to the Data View, and fill in 999 in the cells that have no value on the variable Stress. Then go to the variable view, look for the column ‘Missing’ and click on Missing for Stress. A new window opens. Specify 999 as a discrete missing value. SPSS now knows that the value 999 stands for “missing observation”. Click OK.\nRe-compute the frequency distribution for Stress.\nExamine how the table changed compared to the previous ones.\n\n3.4.9 More Descriptive Statistics\nIn this final assignment, we will continue with descriptive statistics.\nAs mentioned in the lecture, describing the data is an important first step in any research situation.\nFor didactic reasons, we will do some computations by hand, but this is not something you have to do on the exam. However, it is good to experience at least once how the computations work and that the numbers in SPSS are not the result of magic.\nLet’s first look at measures of central tendency:\nConsider the following grades for 10 students: 6, 3, 4, 6, 7, 6, 8, 9, 10, 9.\nCompute (by hand) the mean, median, and mode.\n\n\nRemind me how\n\n\nThe mode is the most common value.\nThe median is the middle value (or mean of two middle values for an equal number)\nThe mean is calculated as the sum of all values, divided by the number of values: \\(\\frac{\\sum_{i=1}^nX_i}{n}\\)\n\n\n\n\n3.4.10 Quiz\n\nWhat is the mean? \nWhat is the median? \nWhat is the mode? \n\nMeasures of variation\nNext we will look at a measure of variation (i.e., indicating the amount of spread in the observations).\nConsider the grades of 6 students: 2, 7, 6, 7, 8, 9.\nCompute the variance and standard deviation by hand.\n\n\nRemind me how\n\nThe variance is the “average squared distance between observations and the mean”: \\(\\frac{\\sum_{i=1}^n(X_i-\\bar{X})^2}{n-1}\\)\nThe SD is the square root of the variance\nFollow these steps:\n\nCompute the mean, e.g., \\(\\bar{X} = 5\\)\n\nFor each observation, calculate the distance from the mean; e.g., \\(3-5 = -2\\)\n\nSquare these distances, e.g.: \\((-2)^2 = 4\\)\n\nAdd these distances for all observations\nDivide by number of observations minus 1\n\n\n\n3.4.11 Quiz\n\nWhat is the variance? \nWhat is the standard deviation? \n\nWe now will verify the answer to the question in the previous step using SPSS!\nFirst, we have to enter the data in SPSS. Proceed as follows:\nOpen SPSS (use Unicode, and close the opening windows)\nMake sure that you have the data view on the screen\nType in the grades in SPSS (i.e.: 2, 7, 6, 7, 8, 9):\nGo to variable view and change the name of the variable and provide a meaningful label\nSecond, we can compute the variance and standard deviation in SPSS.\nProceed as follows:\nAnalyze &gt; Descriptive statistics &gt; Descriptives\nSelect the variable you just defined Now click op Options. A new window opens which shows many more descriptive options Enable the variance Click Continue and OK\nConsult the table descriptive statistics in the output window.\nWere your computations correct?\n\n3.4.12 Correlation\nFor the next few questions we need the data file LAS_SocSc_DataLab2.sav. Open the file in SPSS. You will see that the file contains data for six variables, named X1 through X6. We will inspect the associations between pairs of variables (so called bivariate relationships).\nFirst, generate a scatter plot for X1 and X2. Proceed as follows: Graphs &gt; Legacy dialogs &gt; Scatter/dot. Then ask for a Simple scatter. Put X1 on the X-Axis and X2 on the Y-Axis. Describe the association. Take into account whether the relationship follows a straight line (i.e., linearity), is positive or negative (i.e., direction), and whether the relationship seems to be weak, moderate or strong (i.e., strength).\nSecond, generate a scatter plot for X3 and X4. Make sure that X3 is shown on the X-axis and X4 on the Y-axis. Describe the association in terms of linearity, direction and strength.\nThird, generate a scatter plot for X5 and X6. Describe the association in terms of linearity, direction and strength.\n\nIs the relationship between X1-X2 positive? \nTRUE\nFALSE\nIs the relationship between X5-X6 positive? \nTRUE\nFALSE\nIs the relationship between X1-X2 linear? \nTRUE\nFALSE\nIs the relationship between X3-X4 linear? \nTRUE\nFALSE\nGive an indication of the strength of the relationship between X1-X2: \nweak\nzero\nmoderate\nstrong\nGive an indication of the strength of the relationship between X3-X4: \nzero\nweak\nmoderate\nstrong\nGive an indication of the strength of the relationship between X5-X6: \nstrong\nzero\nweak\nmoderate\n\nConsider the relationship between X3 and X4, can you think of an example of two variables that would be associated in this way?\n\n\nShow answer\n\nAny cyclical process;\n\nTime in the day and how far the water reaches up the beach (ebb and flow)\nLocation of the sun in the sky\n\n\n\n3.4.12.1 Correlation Coefficient\nIn this step we will look at the correlation coefficient as numerical description of linear association.\nNotice that in the previous step we found a non-linear association. The correlation coefficient would not be a valid measure to describe such an association, but nevertheless it is instructive to see why caution should be exercised in drawing conclusions about association from the correlation coefficient alone.\nWe will use SPSS to compute the correlation coefficient.\nAnalyze &gt; Correlate &gt; Bivariate Select X1, X2, … X6 as the variables Click OK\nConsult the table Correlations in the output.\nThere are several values in the table, but we are looking for the Pearson Correlation. The other numbers are the so called significance level, a concept we discuss soon, and the sample size.\n\n3.4.13 Quiz\n\nWhat is the correlation coefficient for the variables X1 and X2? \nWhat is the correlation coefficient for the variables X2 and X6? \nWhat is the correlation coefficient for the variables X3 and X4? \nCan we interpret this correlation coefficient? \nYes, otherwise SPSS would give an error\nNo, assumption of normality violated\nNo, assumption of linearity violated\nNo, assumption of association violated\nInterpret the correlation between X5 and X6? \nModerate negative\nWeak negative\nModerate positive\nWeak positive",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Statistics</span>"
    ]
  },
  {
    "objectID": "chapter2_descriptive_statistics.html",
    "href": "chapter2_descriptive_statistics.html",
    "title": "4  Descriptive Statistics",
    "section": "",
    "text": "4.0.1 Measures of Central Tendency\nDescriptive statistics describe or summarize properties of data collected in a sample. If you collect data on three variables for five participants, you can still print the entire dataset as a table and maintain the overview:\nAny time you collect data from more than just a handful of participants, however, this becomes unfeasible. Instead, we report descriptive statistics.\nDescriptive statistics are almost always computed when data are collected, for a variety of reasons:\nMeasures of central tendency are statistics that try to capture the “most common” value in a sample. The most common measure of central tendency is the “average”, which statisticians would call the “mean”. All measures of central tendency summarize the distribution of values of one particular variable as one representative number.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter2_descriptive_statistics.html#lecture",
    "href": "chapter2_descriptive_statistics.html#lecture",
    "title": "4  Descriptive Statistics",
    "section": "\n4.1 Lecture",
    "text": "4.1 Lecture",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter2_descriptive_statistics.html#formative-test",
    "href": "chapter2_descriptive_statistics.html#formative-test",
    "title": "4  Descriptive Statistics",
    "section": "\n4.2 Formative Test",
    "text": "4.2 Formative Test\nA formative test helps you gauge how well you’ve grasped the ideas and calculations from Chapter 2 – Descriptive Statistics. Try the quiz after working through the lecture slides but before our live meeting, so we can focus on any topics that still feel wobbly. If you miss a question you’ll see a hint that points you back to the relevant slide or worked example.\n\n\nQuestion 1\nWhich measure of central tendency is robust against extreme outliers?\n\nModeMeanWeighted meanMedian\n\nQuestion 2\nFor purely nominal data (e.g., eye color), the only valid measure of central tendency is the:\n\nMedianMeanModeGeometric mean\n\nQuestion 3\nThe range summarises spread by using:\n\nOnly the minimum and maximumSquared deviationsEvery score in the setOnly scores below the mean\n\nQuestion 4\nSquaring deviations when computing variance ensures that:\n\nAll deviations contribute positively to variabilityDeviations stay in original unitsVariance must be unbiasedPositive and negative deviations cancel\n\nQuestion 5\nAdding a constant (e.g., +5) to every score will:\n\nShift SD up by 5Multiply SD by 5Not affect the meanShift the mean up by 5, leave SD unchanged\n\nQuestion 6\nMultiplying every score by 3 will:\n\nLeave SD unchangedShift the mean by 3Multiply both mean and SD by 3Divide SD by 3\n\nQuestion 7\nThe “degrees of freedom” for the sample variance (n-1) reflect that:\n\nThe sample size is unknownOne piece of information is already used to estimate the meanOne piece of information is used to estimate the varianceOnly n-1 scores are valid\n\nQuestion 8\nWhen a distribution is strongly right-skewed (e.g., income), which measure of central tendency best represents a typical observation?\n\nMeanMedianModeRange\n\nQuestion 9\nA distribution with two distinct peaks is called:\n\nSymmetricUnimodalBimodalSkewed\n\nQuestion 10\nReporting the standard deviation alongside the mean helps readers understand:\n\nThe measurement unitsHow tightly scores cluster around the meanIf there are outliersThe sample size\n\n\n\n\nShow explanations\n\nQuestion 1\nThe median depends only on rank order; extreme values cannot pull it up or down.\nQuestion 2\nNominal categories lack numerical distance, so the most frequent category (mode) is the only appropriate centre.\nQuestion 3\nRange is computed as X_max and X_min, relying solely on the two extreme scores.\nQuestion 4\nSquares turn all deviations positive, preventing positive and negative differences from cancelling out.\nQuestion 5\nA constant shift moves the centre but does not change the spread of scores.\nQuestion 6\nScaling stretches both centre and dispersion by the same factor.\nQuestion 7\nAfter fixing the sample mean, only n-1 unique pieces of information remain.\nQuestion 8\nThe median is unaffected by the long tail of extreme high values.\nQuestion 9\nTwo modes (peaks) indicate a bimodal distribution.\nQuestion 10\nSD converts variance back to original units, expressing average distance from the mean.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter2_descriptive_statistics.html#tutorial",
    "href": "chapter2_descriptive_statistics.html#tutorial",
    "title": "4  Descriptive Statistics",
    "section": "\n4.3 Tutorial",
    "text": "4.3 Tutorial\n\n4.3.1 Descriptive Statistics\n\n4.3.1.1 Step 1\nAs explained before, the first step in any statistical analysis involves inspection of the data. In the previous assignment we looked at graphical summaries.\nThis assignment shows you how to explore data using descriptive statistics—values such as the mean, standard deviation, maximum, and minimum.\nUse the data file stressLAS.sav, as in the previous chapter.\n\n4.3.1.2 Step 2 – Descriptives for Key Variables\nWe will first examine the descriptive statistics for Optimism, Life Satisfaction, and Negative Emotions.\nCompute descriptive statistics as follows:\n\n\nAnalyze &gt; Descriptive Statistics &gt; Descriptives\n\nSelect Optimism, Life Satisfaction, Negative Emotions\n\nClick OK\n\n\nSPSS opens a new Output window with a table of descriptives for the selected variables.\n\n4.3.1.3 Step 3 – Frequency Tables\nIn the previous step we computed the average value and standard deviations. However, for nominal and ordinal variables, the average value is meaningless. To explore nominal and ordinal variables we may produce frequency tables. A frequency table shows the observed percentage for each level of the variable.\nGenerate frequencies for Smoke and Relation:\n\n\nAnalyze &gt; Descriptive Statistics &gt; Frequencies\n\nSelect Smoke and Relation\n\nClick OK\n\n\nSPSS now adds a table with the frequency distributions of the selected variables to the output file.\nNote: SPSS reports Percent and Valid Percent. These differ only when missing values are present (none in this dataset).\n\n4.3.1.3.1 Extra – Spotting Multimodality\nSometimes a single mean or median masks sub-groups.\n\n\nGraphs &gt; Legacy Dialogs &gt; Histogram\n\nChoose Life Satisfaction for Variable and click OK\n\n\nIf you notice two peaks, color the bars by Relation (single vs. relationship):\n\n\nGraphs &gt; Chart Builder\n\nDrag Histogram onto the canvas\n\nPlace Life Satisfaction on the x-axis\n\nDrag Relation into Cluster on X\n\nClick OK\n\n\nTake-away: multiple modes often reveal hidden clusters that may need separate analysis.\n\n\n\n\n\n\n\n\n4.3.2 Quiz 1 – Basic Descriptives\n\nHow many participants are in the sample? \nWhat is the mean value of Optimism? \nFor which of the variables is the spread in the scores highest? \nSATIS\nOPTIM\nNEGEMO\nThe minimum and maximum observed scores for Negative Emotions were: [, ].\nWhat percentage of participants is a non-smoker? \nWhat percentage of participants is in a relationship? \n\n\n4.3.2.1 Weighted Mean\nSuppose Class A (n = 12, mean = 6) and Class B (n = 8, mean = 7) are merged.\nSPSS effectively multiplies each mean by its n, sums those products, and divides by the total 20 students, yielding 6.4.\nQuick SPSS route\n\nMerge the two files if separate (Data &gt; Merge Files).\n\nRun Analyze &gt; Descriptive Statistics &gt; Descriptives on the combined score column.\n\n4.3.2.2 Step 4 – Finding Erroneous Values\nOne reason to inspect descriptives first is to spot erroneous values (e.g., age 511 instead of 51).\nUse the descriptives to find any out-of-range values, then:\n\n\nData &gt; Sort Cases\n\nSort the suspect variable ascending or descending\n\nDelete rows with invalid values\n\nAt this stage we remove entire cases; later you’ll learn gentler missing-data techniques.\n\n4.3.2.3 Step 5 – Group Comparison with Split File\nResearch question: “Are non-smokers more satisfied with life than smokers?”\n\n\nData &gt; Split File &gt; Compare Groups → choose Smoke\n\nRun Analyze &gt; Descriptives on Life Satisfaction\n\n\nSPSS now outputs separate means for smokers and non-smokers.\n\n4.3.3 Quiz 2 – Group Means\n\nWas there an erroneous value in the data file? Enter it here: \nIf you delete that value, how will the mean change? \nStays the same\nBecomes larger\nBecomes smaller\nIf you delete that value, how will the standard deviation change? \nBecomes smaller\nBecomes larger\nStays the same\nWho is more satisfied in this sample? \nSmokers\nNon-smokers\nDoes this difference necessarily hold in the population? \nNo\nYes\nCan’t tell\n\n\n4.3.3.1 Step 6 – Quick Check: How Recoding Affects Spread\nAdd 10 points to every Life-Satisfaction score:\n\n\nTransform &gt; Compute Variable\n\nTarget variable: SATIS_plus10\n\nNumeric expression: SATIS + 10 → OK\n\n\nRun Descriptives on both variables:\n\nMean shifts up by 10\n\nSD is unchanged\n\nMultiply by 3 (expression SATIS * 3):\n\nMean × 3\n\nSD × 3\n\n4.3.4 More Descriptive Statistics\nDescribing the data is an essential first step in any research context.\n\n4.3.4.1 Central Tendency by Hand\nGrades: 6 3 4 6 7 6 8 9 10 9\nCompute mean, median, mode by hand.\n\n\nRemind me how\n\n\nMode = most common value\n\nMedian = middle value (or midpoint)\n\nMean = sum / n\n\n\n\n\n4.3.4.1.1 Quiz 3 – Hand Computation\n\nMean \nMedian \nMode \n\n\n4.3.4.2 Variation by Hand\nGrades: 2 7 6 7 8 9\nCompute variance and standard deviation.\n\n\nRemind me how\n\nVariance = average squared distance from mean\nSD = √ variance\n\n\nWhy divide by n – 1?\nAfter the mean is fixed, only n – 1 deviations are free to vary, so dividing by n – 1 keeps the sample variance unbiased.\n\n\n4.3.4.2.1 Quiz 4 – Hand Computation\n\nVariance \nStandard Deviation \n\n\n4.3.4.3 Verifying in SPSS\nEnter the six grades, name the variable, then:\n\n\nAnalyze &gt; Descriptive Statistics &gt; Descriptives\n\n\nOptions… &gt; Variance → Continue &gt; OK\n\n\nConfirm SPSS matches your hand calculations.\n\n\n\n\nHoijtink, H., Bruin, J. de, Duken, S. B., Flores, J., Frankenhuis, W., & Lissa, C. J. van. (2023). The Open Empirical Cycle for Hypothesis Evaluation in Psychology. https://doi.org/10.31234/osf.io/wsxbh\n\n\nPeikert, A., Ernst, M. S., & Brandmaier, A. M. (2023). Why does preregistration increase the persuasiveness of evidence? A Bayesian rationalization [Preprint]. https://osf.io/cs8wb. https://doi.org/10.31234/osf.io/cs8wb\n\n\nVan Lissa, C. J. (2022a). Developmental data science: How machine learning can advance theory formation in Developmental Psychology. Infant and Child Development, 32(6), 1–12. https://doi.org/10.1002/icd.2370\n\n\nVan Lissa, C. J. (2022b). Complementing preregistered confirmatory analyses with rigorous, reproducible exploration using machine learning. Religion, Brain & Behavior, 0(0), 1–5. https://doi.org/10.1080/2153599X.2022.2070254",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "Chapter3_Correlation.html",
    "href": "Chapter3_Correlation.html",
    "title": "\n5  Bivariate Descriptive Statistics\n",
    "section": "",
    "text": "5.0.1 Covariance\nIn the previous chapter, we covered univariate (= single variable) descriptive statistics. In this chapter, we introduce the first bivariate (= two variables) descriptive statistics. Let’s take stock of the road so far, and set a goal for where we want to go. Last week, we ended with the variance. The variance is a statistic that tells us, on average, how much people’s scores deviate from the mean. Today we will move into bivariate descriptive statistics. We will learn about the covariance, which tells us: If someone’s score on one variable deviates positively from the mean, is their score on another variable also likely to deviate positively from the mean? We will also learn about the correlation, which tells us: How strong is the association between two variables, and is it positive or negative?\nIn this chapter, we will talk about two hypothetical variables, X and Y. In your mind, you can substitute any two variables you like; for example, X = hours studied, Y = grade obtained, or X = extraversion, Y = number of friends.\nBefore arriving at the correlation coefficient, statisticians often begin with covariance—a preliminary measure of how two variables vary together. Covariance reflects direction: it is positive when high values of X accompany high values of Y, and negative when they move in opposite directions. However, its numerical value is not directly interpretable because it is tied to the units of the measurement. A covariance expressed in centimeters and kilograms will differ from one computed in meters and pounds, even if the underlying association remains unchanged. As a result, covariance cannot meaningfully convey the strength of a relationship—only whether the variables tend to move in the same or opposite directions.\nIt provides a concise summary of the association between pairs of scores across individuals. For example, a researcher might retrieve each student’s high school GPA (a measure of academic performance) and pair it with their family’s annual income. The goal is to determine whether higher grades tend to correspond with higher income. In correlational studies, each individual contributes two measurements, commonly referred to as X and Y forming the foundation for analysis.\nThe correlation coefficient is a statistic that quantifies the strength and direction of association between two variables. It tells us the degree to which two variables move together. One way to think of the correlation coefficient is as a bivariate (= two variables) descriptive statistic.\nTo explore this relationship visually, researchers often rely on scatter plots. In a scatter plot, X values appear along the horizontal axis and Y values along the vertical. Each point on the plot corresponds to one participant’s pair of scores. These plots allow immediate detection of linear trends, and outliers—patterns that may remain obscured when examining data in purely numerical or tabular form.\nThe word “covariance” means: varying, or moving, together. Let’s have a look at mock data from five students on hours studied and final grade obtained:\nset.seed(2)\ngrads &lt;- data.frame(\n  Hours = round(runif(5, 2, 20))\n)\ngrads$Grade &lt;- round(scales::rescale(.7*grads$Hours + rnorm(5), to = c(1, 10)))\nknitr::kable(grads)\n\n\n\nHours\nGrade\n\n\n\n5\n3\n\n\n15\n9\n\n\n12\n6\n\n\n5\n1\n\n\n19\n10\nWe can visualize these data using a “scatterplot”; a simple graph where each observation is shown as a dot with X-coordinate determined by their value on the X variable (Hours), and Y-coordinate determined by the Y variable (Grade):\nlibrary(ggplot2)\nggplot(grads, aes(x = Hours, y = Grade)) + geom_point() + theme_bw()\nNotice that, if you squint, it appears like there might be some pattern in the data: more hours studied tends to go hand in hand with a higher grade. There might be a positive association between these variables! In the next sections, we go about quantifying this association numerically, step by step.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "Chapter3_Correlation.html#lecture",
    "href": "Chapter3_Correlation.html#lecture",
    "title": "\n5  Bivariate Descriptive Statistics\n",
    "section": "\n5.1 Lecture",
    "text": "5.1 Lecture",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "Chapter3_Correlation.html#formative-test",
    "href": "Chapter3_Correlation.html#formative-test",
    "title": "\n5  Bivariate Descriptive Statistics\n",
    "section": "\n5.2 Formative Test",
    "text": "5.2 Formative Test\nThis short quiz checks your grasp of Chapter 3 – Covariance & Correlation.\nWork through it after you’ve studied the lecture slides (and before the next live session) so we can focus on anything that still feels uncertain. Each incorrect answer reveals a hint that sends you back to the exact slide or numerical example you need.\n\nadd_mcs(\"questions_correlation.csv\")\n\n::: {.webex-check .webex-box}\n\n**Question 1**\n\nA covariance of +120 cmkg tells you... &lt;div class='webex-radiogroup' id='radio_RSWHGAVIPD'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_RSWHGAVIPD\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;120 % of Y variance explained&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_RSWHGAVIPD\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;A perfect linear trend&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_RSWHGAVIPD\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;The units have been standardised&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_RSWHGAVIPD\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;X and Y tend to rise together&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 2**\n\nIf the covariance between study hours and stress level is negative, what does that imply? &lt;div class='webex-radiogroup' id='radio_HPMNPFSIWH'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_HPMNPFSIWH\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;No relationship&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_HPMNPFSIWH\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Longer study -&gt; higher stress&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_HPMNPFSIWH\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;Longer study -&gt; lower stress&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_HPMNPFSIWH\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Units are incomparable&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 3**\n\nWhich statement about covariance magnitude is true? &lt;div class='webex-radiogroup' id='radio_BWRCTCAWPI'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_BWRCTCAWPI\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;Its size depends on measurement units&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_BWRCTCAWPI\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;It equals the regression slope&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_BWRCTCAWPI\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;It ranges only from -1 to +1&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_BWRCTCAWPI\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;A larger value always means a stronger relationship&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 4**\n\nConverting temperatures from Celsius to Fahrenheit will make the covariance between temperature and ice-cream sales... &lt;div class='webex-radiogroup' id='radio_HDMEBUVMGR'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_HDMEBUVMGR\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;Increase by a constant factor&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_HDMEBUVMGR\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Switch sign&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_HDMEBUVMGR\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Become unitfree&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_HDMEBUVMGR\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Stay exactly the same&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 5**\n\nPearson's r is best described as... &lt;div class='webex-radiogroup' id='radio_DIJORFVSOM'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_DIJORFVSOM\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Raw measure of joint variability&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_DIJORFVSOM\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Mean of X and Y combined&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_DIJORFVSOM\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Ratio of two variances&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_DIJORFVSOM\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;Standardised (unit-free) covariance&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 6**\n\nIf r = 0, we can conclude that... &lt;div class='webex-radiogroup' id='radio_MQEFQZIUXO'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_MQEFQZIUXO\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;X causes Y&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_MQEFQZIUXO\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;X and Y are unrelated in every way&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_MQEFQZIUXO\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;No linear relationship is present&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_MQEFQZIUXO\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;The data contain no outliers&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 7**\n\nA positive covariance but r ~= 0.05 usually indicates that... &lt;div class='webex-radiogroup' id='radio_PUKIOJNEKQ'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_PUKIOJNEKQ\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Data range is restricted to zero&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_PUKIOJNEKQ\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;The variables move together only slightly&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_PUKIOJNEKQ\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;The relationship is strong&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_PUKIOJNEKQ\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Units have been standardised&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 8**\n\nWhich scatterplot feature primarily determines the sign of covariance (and r)? &lt;div class='webex-radiogroup' id='radio_YWTADVXYFW'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_YWTADVXYFW\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Sample size&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_YWTADVXYFW\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;Overall slope direction&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_YWTADVXYFW\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Point density&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_YWTADVXYFW\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Presence of a mode&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 9**\n\nYou multiply every X score by 10 but leave Y unchanged. What happens? &lt;div class='webex-radiogroup' id='radio_SETUOMIEZC'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_SETUOMIEZC\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Both covariance and r unchanged&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_SETUOMIEZC\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Covariance unchanged; r x 10&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_SETUOMIEZC\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;Covariance x 10; r unchanged&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_SETUOMIEZC\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Covariance x 10; r x 10&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 10**\n\nA covariance of 0 implies that... &lt;div class='webex-radiogroup' id='radio_NYKECCFALB'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_NYKECCFALB\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;They have opposite scales&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_NYKECCFALB\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;Their linear relationship (r) is 0&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_NYKECCFALB\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;X causes Z&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_NYKECCFALB\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;X and Y are unrelated in every way&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 11**\n\nTwo variables show r = 0.85. Which conclusion is justified? &lt;div class='webex-radiogroup' id='radio_IQWSEHDBHM'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_IQWSEHDBHM\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;X causes Y&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_IQWSEHDBHM\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;X and Y are associated; causal direction unknown&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_IQWSEHDBHM\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;A third variable is impossible&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_IQWSEHDBHM\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Y causes X&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 12**\n\nAnalysing data with a restricted range typically makes r... &lt;div class='webex-radiogroup' id='radio_GXSHZAQTJR'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_GXSHZAQTJR\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;Smaller in magnitude&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_GXSHZAQTJR\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Exactly zero&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_GXSHZAQTJR\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Larger in magnitude&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_GXSHZAQTJR\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Change sign&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 13**\n\nAn extreme outlier that follows the overall trend will most likely... &lt;div class='webex-radiogroup' id='radio_VPVNLCUEOR'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_VPVNLCUEOR\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Remove measurement error&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_VPVNLCUEOR\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;Inflate the magnitude of r&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_VPVNLCUEOR\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Drive r toward zero&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_VPVNLCUEOR\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Make covariance negative&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 14**\n\nA coefficient of determination (r2) of 0.49 means that... &lt;div class='webex-radiogroup' id='radio_CURTKBWNBU'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_CURTKBWNBU\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;49 % of X variance is explained by Y&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_CURTKBWNBU\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;The correlation is -0.70&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_CURTKBWNBU\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Covariance is unitfree&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_CURTKBWNBU\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;49 % of Y variance is explained by X&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n**Question 15**\n\nAfter converting both X and Y to zscores, the covariance of those zvariables equals... &lt;div class='webex-radiogroup' id='radio_KCVYNXFPYV'&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_KCVYNXFPYV\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Their geometric mean&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_KCVYNXFPYV\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Always zero&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_KCVYNXFPYV\" value=\"\"&gt;&lt;/input&gt; &lt;span&gt;Sample size&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=\"radio\" autocomplete=\"off\" name=\"radio_KCVYNXFPYV\" value=\"answer\"&gt;&lt;/input&gt; &lt;span&gt;Pearson&apos;s r&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\n\n\n:::\n\n\n&lt;div class='webex-solution'&gt;&lt;button&gt;Show explanations&lt;/button&gt;\n**Question 1**\n\nPositive sign = same-direction movement; magnitude is unit-dependent so strength not directly interpretable.\n\n**Question 2**\n\nNegative covariance means high X pairs with low Y and viceversa.\n\n**Question 3**\n\nRescaling either variable rescales covariance; therefore magnitude alone is not comparable across units.\n\n**Question 4**\n\nMultiplying Celsius by 1.8 and adding 32 rescales covariance by 1.8; adding a constant does not affect it.\n\n**Question 5**\n\nDividing covariance by the product of SDs removes units and bounds the result between -1 and +1.\n\n**Question 6**\n\nr only detects linear association; other patterns may still exist.\n\n**Question 7**\n\nSmall r means weak linear association despite positive sign.\n\n**Question 8**\n\nPositive slope -&gt; positive sign; negative slope -&gt; negative sign.\n\n**Question 9**\n\nScaling one variable scales covariance by that factor but leaves r (unitfree) unchanged.\n\n**Question 10**\n\nZero covariance means no linear comovement; nonlinear links could still exist.\n\n**Question 11**\n\nCorrelation quantifies association but cannot establish causality without experimental control.\n\n**Question 12**\n\nLess variability reduces covariance relative to the SDs, shrinking r.\n\n**Question 13**\n\nTrendconsistent outliers add leverage, increasing |r|.\n\n**Question 14**\n\nr2 translates correlation into varianceexplained terms.\n\n**Question 15**\n\nStandardising divides by SDs, so covariance in zspace equals r.\n\n\n&lt;/div&gt;",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "Chapter3_Correlation.html#tutorial",
    "href": "Chapter3_Correlation.html#tutorial",
    "title": "\n5  Bivariate Descriptive Statistics\n",
    "section": "\n5.3 Tutorial",
    "text": "5.3 Tutorial\n\n5.3.1 Load Data\nOpen LAS_SocSc_DataLab2.sav.\nThe file contains six variables (X1 … X6). You’ll inspect three bivariate relationships.\n\n5.3.1.1 Plot the pairs\nGenerate three simple scatterplots:\n\n\nGraphs › Legacy Dialogs › Scatter/Dot ► Simple Scatter\n\nPairings & axis order\n\n\nX1 (X-axis) vs X2 (Y-axis)\n\n\nX3 (X-axis) vs X4 (Y-axis)\n\n\nX5 (X-axis) vs X6 (Y-axis)\n\n\n\n\nPaste and Run each syntax block.\n\nDescribe linearity, direction, and strength for each plot.\n\n“The relationship between X1 and X2 is positive.” \nTRUE\nFALSE\n“The relationship between X5 and X6 is positive.” \nTRUE\nFALSE\n“The relationship between X1 and X2 is linear.” \nTRUE\nFALSE\n“The relationship between X3 and X4 is linear.” \nTRUE\nFALSE\nStrength of X1–X2:\nzero\nstrong\nweak\nmoderate\nStrength of X3–X4:\nstrong\nweak\nmoderate\nzero\nStrength of X5–X6:\nweak\nzero\nstrong\nmoderate\n\n\n5.3.1.2 Correlation coefficients\nEven when the pattern is non-linear it’s useful to see why Pearson r can mislead.\nAnalyze › Correlate › Bivariate\nSelect all six variables → OK.\n\nX1–X2 correlation: \nX2–X6 correlation: \nX3–X4 correlation: \nCan we interpret X3–X4’s r at face value?\nNo, assumption of normality violated\nYes, otherwise SPSS would give an error\nNo, assumption of linearity violated\nNo, assumption of association violated\nInterpret X5–X6:\nModerate positive\nWeak positive\nWeak negative\nModerate negative\n\nTake-away: Pearson’s r is good at detecting linear patterns (like X1–X2), but it may be close to zero even when the variables have a strong curved pattern (like X3–X4).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "Chapter3_Correlation.html#correlation-work-dataset-work.sav",
    "href": "Chapter3_Correlation.html#correlation-work-dataset-work.sav",
    "title": "\n5  Bivariate Descriptive Statistics\n",
    "section": "\n5.4 Correlation – Work Dataset (Work.sav)",
    "text": "5.4 Correlation – Work Dataset (Work.sav)\nHaving practiced on simulated data, let’s now apply the same workflow to a real dataset related to the workplace.\nData File: Work.sav\n\n5.4.0.1 Why inspect the plot first?\nBefore trusting Pearson r we check for\n\nan approximately linear pattern, and\n\n\nextreme values that could distort the statistic.\n\n\nSelect the correct reason:\n\nTo check if the relationship is strong enough\nTo check if the relationship is positive\nTo check if the relationship is linear\n\n\n5.4.0.2 Create the scatter-plot\nGraphs › Legacy Dialogs › Scatter/Dot → Simple Scatter\n\nX-axis =scmental (Mental Pressure)\n\nY-axis =scemoti (Emotional Pressure)\n\nPaste and Run.\n\nThe cloud of data points is roughly linear: \nTRUE\nFALSE\nThere are obvious outliers: \nTRUE\nFALSE\nApproximate strength:\n\nModerately weak and negative\nModerately weak and positive\nModerately strong and positive\nModerately strong and negative\n\n\n5.4.0.3 Compute Pearson r\n\nAnalyze › Correlate › Bivariate → (scmental, scemoti) → OK\nThe correlation coefficient is (2 decimals): \nInterpretation:\n\nThere is no relationship between mental and emotional pressure.\nThere is a relationship between mental and emotional pressure.\nWe cannot draw a conclusion on whether or not there is a relationship.\nTake-away: Mental and emotional pressure show a moderately strong, significant positive relationship—employees who feel more mentally pressured also tend to feel more emotionally pressured.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "6  Probability Distributions",
    "section": "",
    "text": "6.1 Lecture\nProbability refers to the likelihood or chance of an outcome occurring in a random experiment. It is defined as the proportion of times that a particular outcome is expected to occur if the experiment is repeated an infinite number of times.\nA random experiment is a process with multiple potential outcomes that could theoretically be repeated under similar conditions. For example, flipping a coin is a random experiment, and before flipping the coin, the outcome is a random experiment with a probability of getting heads or tails of 50% each. Once the coin is flipped, the outcome becomes fixed (the opposite of random), resulting in either heads or tails.\nIn a way, when you draw samples from a population and observe the values of particular variables (e.g., country of origin, height, age), you are performing random experiments. That means that, like with any random experiment, the values you are likely to observe also follow certain probability distributions. Discrete random variables have a finite or countable number of possible outcomes, such as the outcome of a coin toss. On the other hand, continuous random variables, such as the height of individuals, have an infinite number of possible outcomes.\nFor discrete (categorical) variables, we use discrete frequency and probability distributions, which summarize the observations and probabilities of each possible outcome, respectively. These distributions can be represented using frequency distributions, contingency tables, or bar charts.\nFrequency distributions summarize observed outcomes in a sample. For example, a frequency distribution can tell us the proportion of Dutch students in a class or the number of times a particular number was rolled on a die.\nContingency tables (also called crosstables) are used to describe the join frequency distribution, and possibly relationship, between two categorical variables. They show the frequencies of different combinations of values for the two variables.\nWe can use frequency distributions to estimate the probabilities of observing those outcomes in the future. To calculate probabilities from frequencies, we can use different approaches depending on the type of probability distribution we want. In general, dividing frequencies by the total number of observations (grand total) gives us probabilities. In contingency tables, marginal probability distributions are obtained by dividing the marginal totals (row sums or column sums) by the grand total, which provides us with a probability distribution for each separate variable. Conditional probability distributions are derived by dividing a specific row or column by the row- or column total (marginal total), and tells us the probabilities of one variable given a specific value of the other variable.\nIn continuous probability distributions, the possible outcomes are infinite and described by a continuous function. One common example is the normal distribution, also known as the bell curve. It is a symmetric distribution that extends from negative infinity to positive infinity, and it is characterized by two parameters: its mean (average) and standard deviation (measure of dispersion). The square of the standard deviation is called the variance.\nThe standard normal distribution, also known as the Z-distribution, is a standardized version of the normal distribution, rescaled to have a mean of 0 and a standard deviation of 1. Standardizing normal distributions allows us to calculate probabilities more easily using standard normal distribution tables or calculators. We can then convert these probabilities back to the original units if needed.\nProbability distributions can be used as models to describe/approximate the distribution of real data. Behind the scenes, we do this any time we describe the distribution of scores on a variable using its mean and standard deviation. While we often assumpe that variables are normally distributed, that assumption is not always accurate. For example, depression symptoms do not follow a normal distribution: Most people score near-zero on depression symptoms, and few people have higher scores (but these are also not normally distributed). In such cases of violations of the assumption of normality, the mean and standard deviation are not very informative. You may use other descriptive statistics, consider different probability distributions (outside the scope of this course), or discuss the limitations of the assumption of normality.\nIn conclusion, probability distributions provide a way to represent the probabilities associated with different outcomes of a random variable, whether discrete or continuous. By using probability distributions, we can report descriptive statistics, calculate probabilities, and make predictions about future observations.\nVIDEO ERRATA: from 10:10 - 10:50 I talk about the probability of Being Dutch and Having a Tattoo, but I’m calculating the probability of Being Dutch and Not Having a Tattoo (I misread the column labels).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#formative-test",
    "href": "probability.html#formative-test",
    "title": "6  Probability Distributions",
    "section": "\n6.2 Formative Test",
    "text": "6.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nAn HR advisor is looking for new employees for LEGO. He thinks it is important for them to be creative. Creativity is normally distributed in the population, with a mean of 180 and a standard deviation of 25. A higher score indicates higher creativity. The advisor only wants to select applicants that belong to the 0.015 proportion of most creative people. What cut-off/boundary score for creativity should the HR advisor use?\n\n180.38125.75234.25206.13\n\nQuestion 2\nWhat is probability?\n\nA measure of association between two categorical variables.The proportion of times an outcome was observed in a sample.The proportion of times an outcome would be observed in a random experiment if it were repeated many times.The subjective chance of observing a specific outcome in a single random experiment.\n\nQuestion 3\nWhat is a random experiment?\n\nAny process with multiple potential outcomes where the probability of each outcome is unknown.A naturally occurring experiment; for example comparing participants who grow up in an area where the drinking water is rich in a particular mineral with control participants from another area.A process with multiple potential outcomes that could be repeated under similar conditions.An example of the experimental method where people are assigned to a treatment- or control group.\n\nQuestion 4\nWhat are discrete random variables?\n\nVariables with a normal distribution.Variables with a finite or countable number of possible outcomes.Variables with a probability distribution.Variables with an infinite number of possible outcomes.\n\nQuestion 5\nWhat information is contained in a frequency distribution?\n\nA summary of the observed counts of discrete outcomes in the population.A summary of observed probabilities of discrete outcomes in a sample.A measure of association between two categorical variables.A summary of the observed counts of discrete outcomes in a sample.\n\nQuestion 6\nWhat is the standard normal distribution?\n\nA standardized version of the normal distribution with a mean of 0 and a standard deviation of 1.A contingency table summarizing two categorical variables.A probability distribution where the total probability sums to 1.Any continuous distribution with infinite possible outcomes.\n\nQuestion 7\nA researcher is interested in the relationship between movie watched and popcorn consumption. She counts the number of people who consume popcorn during a movie, and whether they have watched Mean Girls or not. The results are presented in the table below this quiz. What is P(Popcorn|Mean Girls), rounded to 3 decimal places?\n\n0.080.040.060.63\n\n\n\n\nShow explanations\n\nQuestion 1\nFind the Z-score that matches a right tail probability of .015 and calculate \\((Z*25) + 180\\)\nQuestion 2\n(The frequentist definition of) probability builds upon the idea that you could theoretically repeat the random experiment many times and calculate the proportion of times a given outcome is observed\nQuestion 3\nA random experiment is a process with multiple potential outcomes that could theoretically be repeated many times under similar conditions.\nQuestion 4\nDiscrete random variables have a finite (=discrete) or countable number of possible outcomes.\nQuestion 5\nFrequency distributions are used to summarize observed outcomes in a sample.\nQuestion 6\nThe standard normal distribution is a standardized version of the normal distribution with a mean of 0 and a standard deviation of 1.\nQuestion 7\nThe question is about the conditional probability of having popcorn given that (|) someone watched mean girls; divide the 151 pp who match this description by the total number of people who watched mean girls.\n\n\n\n6.2.0.1 Popcorn consumption Table\n\n\n \n \nPopcorn consumption\n\n \n\n\n\n \n \nYes\nNo\nTotal\n\n\nMovie type\nMean Girls\n151\n90\n241\n\n\n\nOther\n1678\n2130\n3808\n\n\n \nTotal\n1829\n2220\n4049",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#tutorial",
    "href": "probability.html#tutorial",
    "title": "6  Probability Distributions",
    "section": "\n6.3 Tutorial",
    "text": "6.3 Tutorial\n\n6.3.1 Normal Distribution\nIn this assignment you will practice with the normal distribution.\nThe normal distribution deserves special attention as it is commonly used in statistics for the social sciences. The normal distribution was already derived in the 18 century by DeMoivre, Adrian, and also Gauss*, and since then it played a central role in statistics. More importantly, many attributes in the social sciences are by close approximation normally distributed, as was first discovered by Quetelet. Hence, the normal distribution has great empirical relevance, which comes in handy for our research!\n\nFor that reason, the normal distribution is sometimes referred to as a Gaussian curve.\n\nBefore we start, the concept of a random variable needs to be introduced first. A random variable is a numerical outcome of a chance experiment.\nFor example, a random variable is the number of pips when throwing two dice (here the chance experiment is throwing two dice). Also, the proportion of girls in a random sample of 10 children is a random variable (here the chance experiment is the random selection of 10 children).\nWe also distinguish between continuous and discrete (categorical) random variables.\nA continuous variable can take on infinitely many values. For example, the height of a person is a continuous variable. Take any two persons of different height, and we can always find a third person in between. Discrete variables can take on only particular values. For example, the number of correct of correct answers when a person blindly guesses all the answers is a discrete random variable.\n\nif you want to know, see for example Wikipedia\n\nNormal distributions are distributions for continuous random variables.\nLet’s first have another look at different examples of normal distributions:\n\n\n\nDist.\n\\(\\mu\\)\n\\(\\sigma\\)\n\n\n\n1\n–1.0\n0.5\n\n\n2\n0.5\n0.5\n\n\n3\n0.5\n1.5\n\n\n4\n2.0\n1.0\n\n\n\nInspection of the graphs of the bell-shape distributions shows that it is a symmetric distribution. We also see that the distributions differ in location (the point on the x-axis where it reaches its maximum) and the spread. In other words, the distribution is characterized by two parameters: the mean and standard deviation. The mean is denoted by the Greek letter \\(\\mu\\) (pronounced as “moo”) and standard deviation is denoted by the Greek letter \\(\\sigma\\) (pronounced as “sigma”).\nCompare the distributions and see how the parameters determine the location (mean) and spread (standard deviation).\n\n6.3.1.1 Quiz\n\nCorrectly complete the sentence below by filling in the gaps:\nGiven the example “A student guesses the correct answer to 10 multiple choice questions with four answer categories each” the random experiment is \nthe correct answer\nthe number of correct answers\nthe 4 answer categories\nguessing the correct answer and the random variable is \nthe 4 answer categories\nthe correct answer\nthe number of correct answers\nguessing the correct answer.\nAre the following random variables discrete or continuous?\nNumber of heads in 10 throws with a fair coin is continuous. \nTRUE\nFALSE\nTime by train from Tilburg to Eindhoven is continuous. \nTRUE\nFALSE\nThe number of correct answers on a test is continuous. \nTRUE\nFALSE\nThe mean height in a random sample is continuous. \nTRUE\nFALSE\nThe average number of correct answers in a random sample of 100 students is continuous. \nTRUE\nFALSE\nIf the standard deviation (SD) increases, the distribution becomes \nnarrower\nwider\n\n\n6.3.1.2 “The Empirical Rules”\nAs a first step we may consider some practical rules for working with the normal distribution, the so called “empirical rules”. In particular, if a random variable is normally distributed the following empirical rules apply:\n68% of the values lie within one standard deviation from the mean. 95% of the values lie within two standard deviations from the mean.\nIf we know that a variable is normally distributed, we can also compute probabilities of certain outcomes, so called events. For example, if we know that the scores on a test are normally distributed, we may want to know the probability that a randomly selected person has a score above a certain cut-off (i.e., satisfies a certain selection criterion).\nIn the next few steps, you will practice on how to compute probabilities under the normal distribution. To do so, we have to be able to work with the standard normal distribution (Z-distribution) and accompanying tables.\n\n6.3.1.3 Quiz\n\nComplete the following sentences:\nIQ scores are normally distributed with mean 100 and an SD of 15. This means that 95% of the persons in the population has an IQ in between \n85\n70\n55\n15 and \n100\n130\n145\n115.\nStudents loan after completing the bachelor is normally distributed with mean 1500 Euros and an SD of 150. This means that 68% of the students ends up with a loan between \n1200\n1350\n1485\n1000 and \n1450\n1515\n1650\n1800.\nWhat is the mean of the standard normal distribution? \nWhat is the SD of the standard normal distribution? \n\n\n6.3.1.4 Calculating probabilities\nFor the next series of exercises you need to use a Z-table or calculator (e.g., Excel, Google Sheets, R online, or the Z-table in this GitBook, Appendix B).\nWe have seen that probabilities are related to the area under the curve. This means that for continuous variables we can only find the probability that the outcome falls within a certain interval. For example, the time to complete a task is more than 10 minutes; the IQ is in between 70 and 90.\nNote that there may be different ways to get to the correct answer.\nNumerical Examples\n\n6.3.1.5 Quiz\n\nConsider a continuous variable X, which is normally distributed with \\(X \\sim(\\mu = 30, \\sigma = 4)\\).\nCompute the following probabilities:\nP(X&gt;36.8): \nP(X&lt;24): \nP(X&lt;35): \nP(28&lt;X&lt;34): \n\n\nExplanation\n\nP(X&gt;36.8):\n\nTransform X into Z: (36.8 − 30)/4 = 1.7\nRead Upper Tail Area for Z = 1.7, which equals 0.0446\nConclusion: P(X&gt;36.8)= 0.0446.\n\nP(X&lt;24):\n\nTransform X into Z: (24 − 30)/4 = -1.5\nBecause the Z-distribution is symmetric, we know that P(Z&lt;−1.5) is equal to P(Z&gt;1.5). The latter probability can be found in the Z-table, which is 0.0668. This is also the probability we are looking for.\nConclusion: P(X&lt;24) = 0.0668.\n\nP(X&lt;35):\n\nThe probability we are looking for equals 1− P(X&gt;35). Thus, we first need P(X&gt;35).\nCompute corresponding Z-value: Z = = 1.25.\nLook for the upper tail area in the Z-table: P(Z&gt;1.25) = .1056.\nThus, the area we are looking for is 1 − .1056 = .8944\nConclusion: P(Z&lt;35) = 0.89444\n\nP(28&lt;X&lt;34):\n\nRemark: there are different ways to come the answer. So the answer below is just one of few possibilities.\nWe need the area under curve between 28 and 34. This area equals 1 minus the tail areas; that is, P(28&lt;X&lt;34) = 1 − P(X&lt;28) − P(X&gt;34)\nLets start with P(X&gt;34). First compute Z= = 1. Via the Z-table we find P(Z&gt;1)= .1587.\nNow determine P(X&lt;28). First compute Z= = −0.5. Via the Z-table we find P(Z&lt;−0.5) = P(Z&gt;0.5) = .3085.\nHence, we have 1 − 0.1587 − 0.3085 = 0.5328\nConclusion: P(28&lt;X&lt;34) = 0.5328.\n\n\nStudents are looking for a new roommate. They read in an article that the time people spend under the shower is in the population normally distributed with mean of 10 and SD of 8 (measured in minutes). Suppose they randomly select a person as their new roommate.\nWhat is the probability that this randomly selected person will spend more than 20 minutes under the shower? \nSuppose “confidence in society” is measured on a continuous scale from 0 (no confidence at all) to 100 (highly confident). Also suppose that confidence is normally distributed in the population with mean 52.6 and SD of 12. One speaks of low confidence if the score falls below 43.\nWhat percentage of the population has low confidence? \nThe scores on a test for aggressive behavior are normally distributed with mean 50 and SD of 10. The test is used to select police officers. In particular, only police officers with scores between 42 and 62 qualify for the job as they are not too aggressive and not too friendly either.\nWhat percentage of the population qualifies as police officer? \n\n\nExplanations\n\nLet X denote time people spend under the shower. We want to know the probability that the person showers for more than 20 minutes; that is, P(X&gt;20) given \\(\\mu\\) = 10 and \\(\\sigma\\) = 8.\nCompute Z value: (20 −10)/8 =1.25. Hence, we need P(Z&gt;1.25); that is, the area beyond Z = 1.25.\nVia the Z-table (look in the column labelled C) we find: P(Z&gt;1.25) = .1056.\nConclusion: the probability that a random selected person will shower more than 20 minutes is 0.106 (about 11%).\nLet X stands for the confidence level. X is normally distributed with mean = 52.6 and SD = 12. We need P(X&lt;43). That is, we need the area under the curve to the left of 43.\nFirst, transform to Z-scores: X=43 =&gt; Z = (43 −52.6)/12 = −0.8. Thus, we need P(Z&lt;−.8).\nThe left-tail areas are not shown in the Z-table. Therefore, to find the area, we will first look for the area in the other tail; that is, we will look for P(Z&gt;0.8) in the Z-table. The probability equals 0.2119. Because the distribution is symmetric, the left tail area is also 0.2119. This gives us the answer.\nConclusion: about 21.2% in the population has low confidence in society.\nLet X be the test scores. We need to compute P(42&lt;X&lt;62). This area can not be directly found in the Z-table, so we have to take some additional steps. First, because the whole area under the curve is 1, we can say that the area we are looking for is equal to 1 minus the areas in the tail; thus, 1 − P(X&lt;42) − P(X&gt;62). These latter probabilities can be read from the Z-table!\nCompute Z-values: 1 − P(Z&lt;−0.8) − P(Z&gt;1.2). Remember that \\(Z = \\frac{Observed-mean}{SD}\\).\nDetermine the tail areas: because the distribution is symmetric, we have P(Z&lt;−0.8)=P(Z&gt;0.8). The latter can found in Z-table, which equals .2119. Probability P(Z&gt;1.2) can be directly read from the Z-table, which is .1151.\nHence, 1 − .2119 − .1151 = 1−.327 = .673.\nConclusion: 67.3% of the population qualifies as police officer.\n\nSuppose the time to complete a certain task is normally distributed with mean 8.6 and standard deviation (SD) of 3.5.\nWhat is the probability that a randomly selected person needs more than 11.4 minutes to complete the task? \nAgain, suppose the time to complete a task is normally distributed with mean 8.6 and standard deviation of 3.5.\nComplete the sentence:\n“95% of the participants completes the task within 1.6 and  minutes.”\nScores on a selection test are normally distributed with a mean of 500 and a standard deviation of 50. A person qualifies for the job if they score between 480 and 580.\nWhat is the probability that a randomly selected person will qualify for the job? \nAn IT company is looking for new programmers. To qualify for the job the programmers need to score high on conscientiousness. Therefore, the job applicants need to complete the Conscientiousness scale from the NEO-PI-R (a popular personality inventory for the Big Five personality traits*) as part of the selection procedure. Research has shown that in the population the scores on the scale are normally distributed with a mean of 133.4 and SD of 18.3. To qualify for the job, the conscientiousness of the programmer needs to be among the highest 20% in the population.\nWhat cut-off should the company use to select new personnel? Round to a whole number. \n\nThe Big Five is a popular model for personality; see Wikipedia for more info on the Big Five.\n\n\n\nExplanations\n\nQuestion 1:\n\nTransform X into Z: (11.4 -8.6)/3.5 = 0.8\nRead Upper Tail Area for Z = 0.8, which equals 0.2119\nConclusion: P(X&gt;36.8)= 0.212\n\nQuestion 2:\n95% of the participants completes the task within 1.6 and 15.6 minutes. You can use the empirical rule that 95% of the observations falls within 2SDs from the mean. Thus, 95% of the observations lies within 8.6−2×3.5 and 8.6+2×3.5\nAnd 8.6+2×3.5 = 15.6\nQuestion 3:\nWe need P(480&lt;X&lt;580). This probability equals 1 − P(X&lt;480) − P(X&gt;580).\nP(X&lt;480) = P(Z&lt;−0.4) = P(Z&gt;0.4) = 0.3446 (via Z-table)\nP(X&gt;580) = P(Z&gt;1.6) = P(Z&gt;1.6) = 0.0548 (via Z-table)\nSo the final answer equals: 1−0.3446−0.0548=0.6006 (0.601 when rounded at three decimal places).\nQuestion 4:\nTo get to the correct answer, these are the steps: - First find the Z-value that marks the highest 20%. This value equals 0.84. - Then compute the corresponding cut-off on the X-scale: X = 0.84×18.3 + 133.4 = 148.772 - Rounded to the nearest integer equals 149.\n\n\n\n6.3.2 Missing Values\nFor this assignment, and also later assignments, we will use a (real) data set on Type D personality and several background characteristics (age, gender, and education level (7 ordered levels)).\nType D personality is defined as the tendency towards negative affectivity (NA) (e.g., worry, irritability, gloom) and social inhibition (SI) (e.g., reticence and a lack of self-assurance). Theory suggests that Type D individuals have poorer health outcomes.\nType D is measured with the DS14 scale. The DS14 consists of 14 items, 7 measuring NA and 7 items measuring SI. Answers are given on a five point scale (scored 0 through 4).\n\nsee also Type D personality on Wikipedia.\n\nOpen the data file TypeDDataSSC.sav in SPSS. The data file contains the scores on the DS14 items measuring Type D as well as the background variables for 80 respondents.\nGo to the variable view. The content of the items are given under labels and it is indicated whether the item measures NA or SI.\n\n6.3.2.1 Quiz\n\nIs the first item in the DS14 an indicator of NA or SI? \nSI\nNA\nGo to the data view in SPSS and inspect the data.\nDo you see any missings? \nNo\nYes\nWhat is the valid N of the variable age? \nHow many system missings do we have on gender? \n\n\n6.3.2.2 Recoding missing values\nRemember that Empty cells are called system missings. There are reasons to use user-specified missing codes instead; for example, this allows you to keep track of reasons for missingness (which enables you to report more comprehensively on your missing data).\nSo, for this exercise we will define a user-missing code for the missing values. Missing code is number that a researcher uses to designate that the value is missing. The code must be chosen such that it cannot be confused with actual scores. For example, for age the missing code can be 999, because 999 is an impossible age.\nNow, we will first define missings for age.\nGo to the data view; look at the values of age and whenever the value is missing fill in 999. (In total three persons had a missing on age; so you have to fill in 999 three times).\nGo to the variable view. We have to define the missing code under missing. Click on the cell and […], and SPSS opens a new window. Define 999 as missing code.\nIn the previous step we filled in the missing codes manually. For a small data set this is okay, but for large data sets (say thousands of persons on many variables) this would be problematic.\nIn the next few steps we will see how we can define the missings more easily.\nTo do so, we will use the function recode in SPSS. We will first apply the recoding to gender.\nBefore going into the recoding, let’s first look at the frequency table for gender.\nYou may already have this output from answering the Quiz.\n\n6.3.2.3 Recode into Same Variables\nReplacing user missing values with a missing code using the recode option works as follows:\nNavigate to Transform &gt; Recode into the same variables. SPSS opens a new window.\nSelect gender as the variable to be recoded.\nClick on Old and New Values. SPSS opens a new window. In this window we can specify the recoding. In our case we want to recode System Missing into 999. So, choose “system missing” as old Value, and specify 999 as new value, and click on Add below. (See the more info section below for the SPSS specifications.)\nClick on Continue, and click on OK. SPSS now replaced system missing by 999. Go to the data view and verify that SPSS filled in 999 at the empty places.\nGo to the variable view and specify 999 as the user missing code for gender. (In the same way as you did for age).\nCompute the frequency table for gender again.\nVerify that all “system missings” are now reported as “user missings” instead.\nIn the previous step we only recoded the missings for gender, but we could do that for all variables. It is most convenient to use a code that can be used for all variables. In this case we can use 999 as the missing code for all variables, it’s easy because we can apply this recoding to all variables at once. Just follow the same steps as before, but now select all variables to recode.\nRun the recode command for all variables.\nVerify in the data file that SPSS replaced all system missings by 999.\nNow we also have to specify in the variable view that 999 is the missing codes for all variables. We already changed it for age and gender. To do the same thing for other variables is easy; you can just use copy-paste! Click on Missing for gender, click on the right mouse button, choose copy. Go to the next variable, click on the right mouse button, and with paste you can define the missings for other variables.\nTip: If you like shortcuts: you can also click on missing, type Ctrl C, and then use Ctrl V to copy the information about the missings.\nSo, we specified the missing codes, but we also want to know for each respondent how many missings values he or she had. In other words, for each participant we want to count the number of missings. Participants with too many missings may be excluded from further analysis. Counting the number of missings per person can also be easily accomplished in SPSS.\nTransform &gt; Count Values within Cases. SPSS opens a new window. Specify the name of the target variable (e.g., CountMiss); this is the name of a new variable that gives the number of missings. You may also give the variable a label, say: “Number of Missings”.\nSelect all variables.\nClick on Define Values. SPSS opens a new window. Select System or User Missing at the left and click on add. Click on continue than OK.\nSPSS will now create a new variable that shows how many missings there are for each person on the variables selected in the list.\nGo to the data view and verify that SPSS added a new variable (i.e., a new column with values) named CountMiss.\n\n6.3.2.4 Quiz\n\nCompute the frequency table for the third DS14 item. How many missing values do we have on this item? \nHow many missings does person 8 have, using the variable CountMiss? \nCompute the frequency table for CountMiss.\nWhat is the maximum number of missings for the participants in this dataset? \nHow many participants have this many missings? \nHow many participants have at least one missing value? \n\nMake sure you save the data file including the variable with the number of missings. We will use it in the next assignment. `\n\n6.3.3 Select Cases and Split File\nIn this assignment we will take a closer look at selecting cases and how to do analyses for subgroups.\n\n6.3.3.1 Selecting Cases\nIn the previous assignment we have seen that some respondents had one or more missing values. Suppose we want to discard these persons in the analysis, which means that for all the remaining analyses we only want to include participants with no missings. This method of handling missing data is called listwise deletion, and it is generally considered bad practice - but it’s also easy, so we will teach it in this course. More advanced courses cover expert methods of handling missing data.\nProceed as follows:\nData &gt; Select cases. SPSS opens a new window.\nChose “If condition is satisfied” and chose ‘if’. SPSS opens a new window again.\nSpecify the condition CountMiss = 0 to select cases with no missings.\nMake sure that the output is specified as “Filter out unselected cases”. Click on OK.\nGo to the data view.\nVerify that SPSS crossed out cases with one or more missings.\nVerify that SPSS added a new variable labelled ‘filter_$’. This is filter variable indicating who is included in the analyses (value = 1) or not (value is 0). If you remove the filter variable, SPSS will use all cases again.\n\n6.3.3.2 Quiz\n\nWhat is the mean for the variable age of the selected group? \nWhat is the valid sample size for that mean? \n\nNow, run the selection procedue again, but remove the incomplete cases from the data file.\nProceed as follows:\nData &gt; Select cases Choose “Delete Unselected Cases” under output. Click OK. Verify that the incomplete cases are removed.\nBecause you have modified the data, it is prudent to save the new file under a different name (e.g., TypeD_selection.sav). Use this file with only the complete cases (i.e., TypeD_selection.sav) for the remaining steps.\n\n6.3.3.3 Split File\nSometimes we want to do analyses for subgroups, especially when exploring the data for the first time. For example, we may want to have the descriptive statistics for males and females separately. One way to do this is to use the Split File option. With this option you can tell SPSS that you want to have tables for each subgroup separately.\nLet’s see how it works!\nProceed as follows:\nVia menu follow Data &gt; split file. SPSS opens a new window.\nChoose ‘Compare Groups’ and choose gender as the variable for Groups Based on.\nClick OK.\nImportant: Notice no output appears and no changes are made to the data. This makes sense because we haven’t asked SPSS to generate any output nor to make changes in the data. Yet, SPSS now knows that he has to produce tables for males and females separately once we ask to generate output.\nTo undo the split file, proceed as follows.\nData &gt; Split file Choose Analyze all cases, do not create groups. Click OK. SPSS now no longer produces the output per group.\n\n6.3.3.4 Quiz\n\nHave SPSS compute the mean and SD for age.\nHow many women are there in the sample?  What is the mean age of men in the sample?  What is the mean age of women in the sample? \nOne of the variables is Education Level. It is an ordinal variable with 7 levels, score 1 represents the lowest level of education, and score 7 the highest.\nCompute the mean age per level of education.\nFor which education level was the mean age highest? \nWhat was the value of the mean age for this education level? \n\n\n6.3.4 Recode and Compute\nFor this assignment we will continue with the data on Type D and the selection of complete cases.\nBefore you start, make sure that the Split File option is disabled.\nIn practice, you often have to do some data handling before you can actually start doing analyses. For example by coding missing values, or you may have questionnaire data for which some of the questions are formulated contra-indicative and therefore should be reverse coded. Another reason would be that you may have to compute the total score (e.g., sum or average) for a set of questions.\nIn this assignment we will practice some basic data handling skills.\n\n6.3.4.1 Reverse Scoring Contra Indicative Items\nIf you read the item labels, you will see that the first two SI items (items DS14_1 and DS14_3 in the scale) are contra indicative. This means that for these items higher scores reflect low SI, while for other items higher scores reflect high SI. Therefore, the responses to these items should be recoded first to make sure that all items are scored in the same direction. To do so, we will create new variables that reflect the recoded items. Proceed as follows:\nTransform &gt; Recode into different Variables\nChoose DS14_1 as the Numerical Variable to be recoded\nSpecify a name for the output variable (say DS14_1R)\nGive a label, say: “SI item 1 (recoded)”\nClick on change\nDo not close the window yet, but continue to the next step…\nTo recode, we have to specify the Old Values and New values. Reverse scoring of the DS14 items means that\nold value 0 -&gt; new value 4\nold value 1 -&gt; new value 3\nold value 2 -&gt; new value 2 (*)\nold value 3 -&gt; new value 1\nold value 4 -&gt; new value 0\n(*) You may think this line is superfluous but for the recoding in SPSS you need to specify for every possible value a recoded new value, even if the values remains the same.\nSpecify the old and new values. Each time you specified old and new values click on ADD such that the recoding scheme appears in the little dialog.\n\n6.3.5 Using Syntax\nTo ensure that others can see exactly how you got from raw data to the final dataset used for analysis, it is essential to keep a complete record of any changes made to the data. This is also why we previously argued that you should not overwrite a datafile after altering it.\nUp until now, we ran the analyses by “click-and-point” via the menu. This is a good starting point to explore the software SPSS, but it is not good practice for professional use because there’s no record of what you did to the data in order to get your result.\nNOTE: For all your portfolio assignments, providing syntax is mandatory so I can grade what you DID, not just what you reported!\nTo keep a record of changes made to your data, you can prepare a script that contains all instructions for the analysis instead. By evaluating this script on the data, you should consistently get the same results.\n\n6.3.5.1 Why syntax?\nUsing syntax is important for several reasons:\n\nFirst, efficiency: it makes life easier. Once you have the syntax, you can easily redo the whole analysis without going through all the points-and-clicks again.\nSecond, communication: When you work together on research projects, it is important that you exactly understand the analyses that were done, even if you didn’t do the analyses yourself. By using syntax, all team members can see what has been done and how.\nThird, documentation & data management: As a researcher you are responsible for data storage and management (!). This not only includes storage of data, but also documentation of the all the steps and analyses you did to come to your results (e.g., handling missing values, detection of outlying values). Ideally, you should provide the materials such that other researchers can easily replicate your analyses starting from the raw data file. Working with SPSS syntax is a great way to do so.\nFourth, necessity: some statistical procedures are only available via SPSS syntax (e.g., simple effects analysis in MANOVA).\n\n\n6.3.5.2 Help!\nYou don’t need to memorize the commands by heart. SPSS offers help functions. If you highlight the command (e.g., statistics) and click on the button with the paper and the question mark in the top menu. SPSS opens a help file.\nUse the help function to modify the syntax such that SPSS produces a table that also reports the range (e.g., the difference between the largest and smallest value).\n\n6.3.5.3 How to use syntax\nThere are two ways to use syntax. The first is to create an empty syntax script via File -&gt; New -&gt; Syntax, and then start adding the code from scratch. You can either write the code as text, or create it via SPSS’ various dialog windows. For this course, we recommend using the dialogs:\n\nIn any dialog window, click “Paste” instead of “OK”.\nA new window opens (or existing window comes into focus) with a script file (or “syntax” file). The instructions for the requested analysis are added at the bottom of this file.\nSelect all instructions you wish to execute, and press the green “Play” button.\nYou can re-organize this script file, adding, or removing operations or changing their order. Keep it nice and organized!\n\n\n\nSyntax for recoding\n\nSelect the highlighted lines and press the green “Play” button. Verify in the data view that SPSS added a new variable (i.e., new column) with the recoded values for Item 1.\nSyntax is also useful because it may be more straightforward than the graphical interface. For example, the recode syntax above took a lot of pointing and clicking to get:\nRECODE DS14_1 (0=4) (1=3) (3=1) (4=0) INTO DS14_1R.\nEXECUTE.\nFor recoding more variables, we can copy-paste these instructions and change the variable names.\nFor example, to recode DS14_3 in the same way as you did the recoding for DS14_1 we would write:\nRECODE DS14_1 (0=4) (1=3) (3=1) (4=0) INTO DS14_1R.\nRECODE DS14_3 (0=4) (1=3) (3=1) (4=0) INTO DS14_3R.\nEXECUTE.\nAlternatively, we could simply say:\nCOMPUTE DS14_1R = 4-DS14_1.\nCOMPUTE DS14_3R = 4-DS14_3.\nEXECUTE.\nVerify that this calculation also works.\n\n6.3.5.4 Commenting\nWhen working with syntax, it is highly recommended to add comments as reminders to your future self of the purpose of each step in the script. These comments should clarify the syntax and give general information (e.g., when was the syntax last modified, who did the modifications, etc.).\nComments are text lines that start with an * and ends with a dot. Comments are printed in grey.\nREMARK: the dot at the end of your text line is really important. If you do not add it, your syntax will run work properly!\nAdd comments including the following information:\n\nWhen was the syntax made?\nWho made syntax?\nWhat does the syntax do?\n\nFor example (CJ is short for Caspar J. Van Lissa):\n* CJ: This script also recodes Likert scales with integer values.\nCOMPUTE DS14_1R = 4-DS14_1.\nCOMPUTE DS14_3R = 4-DS14_3.\nEXECUTE.\nAfter including the comments, run the complete syntax including the comment line (by selecting and running it, or via top menu Run &gt; All).\nIf the syntax runs correctly, the comments were correctly included.\n\n6.3.5.5 Compute Variable\nFor the analyses we are not interested in the single item scores but in the summed scores. Because the DS14 contains two subscales (consult the item labels to see to which subscale the item belongs), we want to compute the summed scores for the NA and SI items separately.\nLet’s first do this for NA. Proceed as follows:\nTransform &gt; Compute variable. SPSS opens a new window.\nChoose a name for the target variable, say: NAtotal.\nUnder numerical expression you have to say what you want to compute. In this case the sum of the NA items, which is DS14_2 + DS14_4 + … etc. So, select the first item to be summed from the list at the left, type +, select the next item you want to add, and so on. Make sure that you only add up the NA items (in the variable view you can see which items measure NA and which measure SI).\nClick Paste, and run the code.\nAlternatively, copy-paste this syntax, complete it and run it:\nCOMPUTE NAtotal = DS14_2 + DS14_4 + .\nCOMPUTE SItotal = .\nEXECUTE.\n\n6.3.5.6 Quiz\n\nCompute the frequency table for the recoded DS14_3 item.\nWhat percentage of the respondents had the highest score on this item? \nWhat is the mean of NAtotal? \nCompute the mean of NAtotal for men and women separately. For which group is the mean highest? (Hint: Use the skills you’ve learned in the previous assignments). \nWomen\nMen\nCompute the total scores for SI. Make sure you use the reverse scored items for items 1 and 3 to compute the total score (this implies you have to leave the original item 1 and 3 out).\nWhat is the mean of SI in the total sample? \nWhat is the mean for the subsample of men?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "samplingdistribution.html",
    "href": "samplingdistribution.html",
    "title": "7  The Sampling Distribution",
    "section": "",
    "text": "7.1 Lecture\nAs explained in lecture 1, a sample is an observed subset of a larger population. We typically calculate statistics based on sample data, and use these as best guesses of the values of population parameters. This process is called statistical inference. A crucial insight is that sample statistics are not perfect estimates of population parameters. The discrepancy between the sample statistic and population parameter is known as sampling error.\nWe have some theoretical insight into theoretical behavior of sample statistics. For example, we can imagine constructing a probability distribution of the values we might see for a sample statistic, such as the mean, if we were to draw very many random samples from an identical population. This theoretical distribution of means is called the sampling distribution. The central limit theorem tells us that, regardless of the shape of the distribution of the data in the population, as the sample size increases, the sampling distribution of the mean approaches a normal distribution. This is an important realization, because it means that we can use probability calculus using the normal distribution to draw inferences about population parameters based on sample statistics.\nThe standard deviation of the sampling distribution plays a central role in inferential statistics. It is so important that we give it a unique name: we call this particular standard deviation the standard error (SE). The standard error quantifies the average, or expected, amount of sampling error when we use a sample statistic to estimate the population parameter. If the standard error is small, our estimates based on the sample are likely to be accurate, whereas a large standard error indicates greater uncertainty.\nWith the help of the normal distribution, and given a particular (hypothesized or known) population mean and standard error, we can calculate how likely it is to observe specific sample means. For example, if we want to determine the probability that the mean of a random sample exceeds a certain value, we can standardize the sample mean using the formula \\(Z = \\frac{M - \\mu}{SE_M}\\), where M is the sample mean, \\(\\mu\\) is the known or hypothesized population mean, and SE is the standard error. By looking up the corresponding probability on the standard normal distribution table or using statistical software, we can assess the likelihood of observing a specific sample mean (or greater, or smaller).\nConfidence intervals are a way to express our uncertainty about the sample statistic as estimator of the population parameter. A confidence interval is a range of values - a window - within which we expect the true population parameter to fall with a certain level of confidence. Typically, we select a 95% confidence interval, which means that if we could repeat the sampling process many times and calculated confidence intervals each time, 95% of those intervals would contain the true population parameter. The width of the confidence interval is determined by the standard error and is proportional to the level of confidence desired. The formula for a confidence interval is often written as: \\(M \\pm Z_{95\\%} * SE_M\\). In practice, this comes down to approximately: \\(M \\pm 2 * SE_M\\).\nVIDEO ERRATA: from 19:40 - 19:50 I incorrectly report the probability of P(Z &gt; 1) as .025, but it is .16.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "samplingdistribution.html#formative-test",
    "href": "samplingdistribution.html#formative-test",
    "title": "7  The Sampling Distribution",
    "section": "\n7.2 Formative Test",
    "text": "7.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nIntroversion is normally distributed with a mean of 50 and a standard deviation of 10. What is the probability that the mean introversion level of a randomly selected group of 16 people is smaller than 52? Round the answer to 3 decimal places.\n\n0.7880.3450.6550.045\n\nQuestion 2\nVariable X is not normally distributed in the population. Variable X has a population mean of 30 and a population standard deviation of 6. A random sample of N = 36 scores is drawn from the population for variable X. The sample mean is equal to 32. Which of the following statements about the sampling distribution of the sample means for this sample (n = 36) is incorrect?\n\nThe standard deviation of the sampling distribution of sample means is equal to 1.The mean of the sampling distribution of the sample means is equal to 32.The standard error is smaller than the population standard deviation.The sampling distribution of sample means is approximately normally distributed.\n\nQuestion 3\nWhat does the sampling distribution represent?\n\nThe distribution of sample meansThe distribution of raw dataA theoretical distribution of sample statisticsThe distribution of population parameters\n\nQuestion 4\nWhich of the following statements about the sampling distribution is true?\n\nThe sampling distribution of a skewed variable is also skewedThe sampling distribution is centered around the population parameterThe sampling distribution is based on a single sampleThe sampling distribution is identical to the population distribution\n\nQuestion 5\nWhat is the standard deviation of the sampling distribution called?\n\nStandard errorVarianceBiasPopulation standard deviation\n\nQuestion 6\nHow does sample size affect the shape of the sampling distribution?\n\nLarger sample sizes increase the variability of sample statisticsLarger sample sizes make the sampling distribution more spread outLarger sample sizes result in smaller standard errorsLarger sample sizes have no impact on the sampling distribution\n\nQuestion 7\nWhat is the probability that a sample mean falls within +/- 1 standard deviation of the population mean, assuming a normal distribution of sample means?\n\n0.480.950.680.34\n\nQuestion 8\nIf the standard deviation of the population is 10 and the sample size is 25, what is the standard error of the sample mean?\n\n0.420.25\n\nQuestion 9\nWhat is the probability that the sample proportion falls within +/- 2 standard deviations of the population proportion, assuming a large sample size?\n\n0.340.480.950.68\n\nQuestion 10\nIf the standard deviation of the population is 5 and the sample size is 50, what is the standard error of the sample mean?\n\n0.2830.07070.7070.141\n\n\n\n\nShow explanations\n\nQuestion 1\nCalculate the standard error as 10/sqrt(16). Then, calculate the Z-score as (52-50)/SE. Find the right tailed probability of that Z-score, then calculate 1 minus that probability.\nQuestion 2\nThe sampling distribution will be approximately normal because n &gt;= 30. The SE is indeed 1, because sigma/sqrt(n) = 6 /sqrt(36) = 1. The SE is always smaller than sigma, because it is calculated as sigma divided by square root of n.\nQuestion 3\nThe sampling distribution represents the distribution of sample statistics, such as sample means or proportions, derived from multiple samples drawn from the same population. It provides insights into the variability and characteristics of these sample statistics.\nQuestion 4\nThe sampling distribution is centered around the population parameter.\nQuestion 5\nThe standard deviation of the sampling distribution is known as the standard error. It measures the average variability or spread of sample statistics around the population parameter, reflecting the precision of the estimation.\nQuestion 6\nLarger sample sizes result in smaller standard errors. As the sample size increases, the sampling distribution becomes more concentrated around the population parameter, leading to a decrease in the standard error. This implies that larger samples provide more precise estimates of the population parameter.\nQuestion 7\nThe probability that a sample mean falls within +/- 1 standard deviations of the population mean, assuming a normal distribution of sample means, is 68%. This is based on ‘the empirical rule’.\nQuestion 8\nThe standard error of the sample mean is 2. The standard error can be calculated by dividing the standard deviation of the population by the square root of the sample size. In this case, it would be 10 / √25 = 2.\nQuestion 9\nThe key lesson here is that everything you learned about the sampling distribution also applies to other statistics than the mean, so according to the empirical rule, 95% of sample proportions will fall within +/- 2 standard deviation of the population proportion.\nQuestion 10\nThe standard error of the sample mean is SD/sqrt(n), so 5/sqrt(50) = .707",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "samplingdistribution.html#tutorial",
    "href": "samplingdistribution.html#tutorial",
    "title": "7  The Sampling Distribution",
    "section": "\n7.3 Tutorial",
    "text": "7.3 Tutorial\n\n7.3.1 Sampling Distribution\nComplete the following sentences:\nIQ scores in the population of potential students are normally distributed with mean 120 and an SD of 10. If a cohort contains 75 students, 95% of cohorts will have an average IQ in between \n117.69\n118.27\n100\n118.85 and \n121.15\n130\n122.31\n121.73.\nAfter graduating, a cohort of 75 LAS students can expect to earn a starter salary of 2650 Euros, with an SD of 300 euros. What percentage of cohorts will have a mean starter salary greater than 2750 euros? \n99%\n2%\n4%\n0.2%.\nIn a sample of 5000 babies, the average birthweight is 3.213 kg, with an SD of 254 grams. What is the mean birthweight of the sampling distribution? \n3.213 kg\nCan’t say\nBetween 3202.22 and 3223.78 grams\nConsider a continuous variable X, which is normally distributed with \\(X \\sim(\\mu = 30, \\sigma = 4)\\). We draw a sample of 15 participants. What is the probability that the sample mean will be smaller than 32? \nThe proportion of male babies is .51. Assume babies born in each hospital in a given month constitute a random sample of size 100. The standard error of a proportion is given by \\(\\sqrt(p*(1-p) / n) = \\sqrt(.51*.49 / 100) = 0.05\\). What proportion of hospitals will have more than 60% male babies? \n\n7.3.2 In SPSS\n\n7.3.2.1 SE for Means\nOpen the file called student_questionnaire.sav.\nThese are data from a previous cohort of students. Note that we have data about biometric differences (e.g., age, height, shoesize), as well as school-related questions (which program they are enrolled in), variables about their love for statistics, and about moral preferences (based on the “Morality As Cooperation” questionnaire that I helped develop).\nGo to Analyze -&gt; Descriptives and ask for descriptive statistics on height and shoesize. Click Options, and notice that there’s an option to request the standard error of the mean. Select this option, then paste and run your syntax. Check if it corresponds to the syntax below.\n\n\nAnswer\n\nDESCRIPTIVES VARIABLES=height\n  /STATISTICS=MEAN STDDEV MIN MAX SEMEAN.\nNote the SEMEAN option was added by clicking that option!\n\nThe mean length in the population of Dutch people is 177.434. With this in mind, calculate the probability that a random sample of the same size as this sample would have the mean length you calculated for this sample or smaller. \n\n\nExplanation\n\nThe question asks for the lower-tail probability below a value of 174.62 in a distribution with mean 177.434 and SD .772 (the SE you obtained from SPSS).\n\\(\\frac{174.62-177.34}{.772} = -3.65\\)\nA Z-score of nearly -4, so this probability is going to be extremely small, &lt; .001.\n\n\n7.3.2.2 SE for Proportions\nGo to Analyze -&gt; Compare Means -&gt; One Sample Proportions.\nThis procedure allows you to estimate proportions and their standard errors. It’s not very common, in fact I learned about it by Googling “standard error for proportion spss”! Any time you need to know how to do something in SPSS, you can find advice on the internet.\nCalculate the proportion for the variable sex, and paste and run your syntax.\n\nPROPORTIONS\n  /ONESAMPLE sex TESTVAL=0.5 TESTTYPES=MIDP SCORE  CITYPES=AGRESTI_COULL JEFFREYS WILSON_SCORE \n  /SUCCESS VALUE=LAST\n  /CRITERIA CILEVEL=95\n  /MISSING SCOPE=ANALYSIS USERMISSING=EXCLUDE.\n\nNote the table labelled “One-Sample Proportions Confidence Intervals”. This table contains confidence intervals for the proportion, calculated according to three different procedures. In the lecture, you also learned a procedure to calculate confidence intervals.\nUsing the procedure from the lecture, calculate a 95% confidence interval for the proportion. You can round the Z-score for this confidence interval to 2.\nThe 95% CI for the proportion of male students is [, ].\nNote that the differences between this procedure and the three procedures in the table only differ in the third decimal.\nHow do you interpret a confidence interval?\n\nA range of values that contains the population parameter with 95% certainty.A range of values that, 95% of the time, contains the population parameter.The population value, with a 95% margin of error.The range of likely population values.\n\n\n7.3.2.3 SE for Correlation\nRecall from the first lecture that the correlation coefficient is a measure of linear association between two variables, or: a descriptive statistic that describes how strongly two continuous variables are associated.\nGo to Analyze -&gt; Correlate -&gt; Bivariate. Add the variables work_hours and study_hours, paste and run the syntax.\nThe value of the correlation coefficient is labelled “Pearson Correlation”. What value do you observe? \nThe correlation coefficient ranges from 0-1 (or minus 1). With this in mind, answer the following question:\nTrue or false: This correlation coefficient is near zero. \nTRUE\nFALSE\nThe calculation of a standard error is a bit more complicated, but there’s an “approximation” (a quick approach that gives reasonable results in some cases, but could be wrong in other cases). It is calculated as:\n\\[\nSE_r = \\sqrt{\\frac{1-r^2}{n-2}}\n\\]\nCalculate the SE this way. What is its value? \nAssume for a moment that the true population correlation is zero (r = 0). Using the SE you calculated, what would then be the probability of observing a correlation between 0 and the correlation you actually observed? \n\n\nExplanation\n\nThe question asks for the probability between the mean (0) and a value of .057 in a distribution with mean 0 and SD .075 (the SE you calculated).\nSo we first calculate the right-tailed probability for the value of .057.\n\\(Z = \\frac{.057-0}{.075} = 0.76\\)\nA Z-score of 0.76, so the right-tailed probability is 0.22.\nThen, take .5 (the probability to the right of 0), and subtraCT .22: .28",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "Week_4.html",
    "href": "Week_4.html",
    "title": "8  Philosophy of Science",
    "section": "",
    "text": "8.0.1 Deduction and Induction in Hypothesis Testing\nStudents of statistics often have the impression that this course is all about cold, hard facts. Nothing could be further from the truth: statistics is a direct extension of philosophy of science. The numbers we calculate here only have relevance to real-world research questions thanks to some complex philosophical arguments. While most of these are outside the scope of the present course, we want you to be familiar with the main concepts before moving on.\nHypothesis testing has roots in the philosophy of logic, or correct reasoning. In logic, arguments consist of a set of premises, which can be true or false, that together lead to a conclusion, which can also be true or false. The most famous example is:\nThis is a deductive argument, which has the property that if the premises are true, then the conclusion must also be true. Deductive arguments are also often thought of as arguments from the general to the specific. In this case, the general rule “all men are mortal” gives rise to the specific claim that one specific man, Socrates, is also mortal.\nThe counterpart to deductive reasoning is inductive reasoning, which proceeds from specific observations or claims to general rules. The most famous example is:\nUnlike the deductive argument above, however, inductive reasoning does not guarantee that true premises always produce true conclusions. Even if it is true that I have only seen white swans, the conclusion is false - black swans exist. David Hume introduced another famous example:\nHere, too, the conclusion is not supported by the premise. This might make us feel uncomfortable - which sane person would reject the conclusion that the sun will rise in the east tomorrow? This discomfort illustrates that people are naturally inclined to reason inductively. As scientists, however, we should be very cautious to remember that this way of reasoning is not guaranteed to produce true conclusions.\nInductive and deductive reasoning are both used in statistical hypothesis testing. Deduction is used when we derive a specific prediction (hypothesis) from a general theory. For example, we could say that:\nIf the premise is true, then we would expect to observe the corresponding pattern in our data.\nInduction comes into play when we draw general conclusions from observed data.\nThis conclusion does not logically follow from the premise. The problem is not resolved by removing the word “causes”:\nIt follows that we can never conclusively “prove” general conclusions from specific observations. The philosopher David Hume wrote extensively about this “problem of induction”: How can we justify the assumption that unobserved cases will follow the same patterns as observed ones? Hume argued that this assumption cannot be logically justified. Our sense that generalization is justified might be based on intuition, but not logic. The problem of induction challenges the very foundation of science. We cannot escape the use of induction when seeking to learn general insights from specific observations, but Hume showed that induction lacks a purely rational justification.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Philosophy of Science</span>"
    ]
  },
  {
    "objectID": "Week_4.html#lecture",
    "href": "Week_4.html#lecture",
    "title": "8  Philosophy of Science",
    "section": "\n8.1 Lecture",
    "text": "8.1 Lecture\nTO DO",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Philosophy of Science</span>"
    ]
  },
  {
    "objectID": "Week_4.html#formative-test",
    "href": "Week_4.html#formative-test",
    "title": "8  Philosophy of Science",
    "section": "\n8.2 Formative Test",
    "text": "8.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat is the main difference between deductive and inductive reasoning?\n\nDeductive reasoning is used in science; inductive is not.Deductive reasoning guarantees the conclusion if the premises are true, while inductive reasoning does not.There is no real difference; both produce valid conclusions.Inductive reasoning guarantees conclusions; deductive does not.\n\nQuestion 2\nWhich of the following is an example of inductive reasoning?\n\nThe sun has always risen in the east, so it will rise in the east tomorrow.Students who study more get better grades.Water boils at 100 degrees at sea level.All humans are mortal, so Socrates is mortal.\n\nQuestion 3\nWhat is the role of deduction in statistical hypothesis testing?\n\nIt is used to generalize observations to populations.It tests if data supports any hypothesis.It is used to derive specific predictions from general theories.It determines the sample size needed.\n\nQuestion 4\nWhat was Karl Popper’s proposed solution to the problem of induction?\n\nFalsification: trying to disprove theories rather than prove them.Using more data to confirm theories.Renouncing inductively constructed theories as pseudoscience.Only using observational data.\n\nQuestion 5\nIn Null-Hypothesis Significance Testing (NHST), what role does the null hypothesis play?\n\nIt provides definitive proof of a theory.It merely exists to be refuted.It shows causality when rejected.It reflects the researcher’s true belief.\n\nQuestion 6\nWhat does the phrase ‘correlation does not imply causation’ mean?\n\nDon't use a correlation coefficient if the effect is causal.A statistical association may not reflect a true causal relationship.Causal relationships cannot be studied using statistics.Causation can only be studied with regression.\n\nQuestion 7\nAccording to a common interpretation of Hume, which three conditions are necessary for causality?\n\nAssociation, temporal precedence, and non-spuriousness.Randomization, generalization, and verification.Observation, correlation, and prediction.Falsifiability, deduction, and experimentation.\n\nQuestion 8\nHow do randomized controlled experiments support causal inference?\n\nThey always prove causality through replication.They eliminate alternative explanations through random assignment.They confirm inductive conclusions.They eliminate the need for statistical testing.\n\n\n\n\nShow explanations\n\nQuestion 1\nDeductive reasoning leads to necessarily true conclusions if the premises are true, while inductive reasoning involves probable conclusions based on patterns.\nQuestion 2\nAssuming that the sun will rise tomorrow because it always has is an example of induction, and it is not logically justified.\nQuestion 3\nDeduction is used to derive derive specific, testable predictions from general theories, such as expecting a correlation between study time and grades if studying improves performance.\nQuestion 4\nPopper argued that science should focus on rejecting falsifiable hypotheses rather than confirming them, avoiding the pitfalls of induction.\nQuestion 5\nThe null hypothesis is a ‘straw man’ hypothesis, only intended to be rejected. This interpretation of Popper’s falsificationism actually never submits theories to the test, and aligns more closely with confirmationism.\nQuestion 6\nNo statistical finding can prove causation, so a correlation (or other type of association) between two variables doesn’t confirm a causal link.\nQuestion 7\nA common interpretation of Hume is that for a cause-effect relationship, the variables must be associated, the cause must precede the effect, and other explanations must be ruled out.\nQuestion 8\nRandom assignment maximizes the probability that the treatment and control groups do not differ on any confounding variables, thus ensuring nonspuriousness. Furthermore, a well-designed experiment ensures association (if there is an effect) and temporal precedence.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Philosophy of Science</span>"
    ]
  },
  {
    "objectID": "Week_4.html#tutorial",
    "href": "Week_4.html#tutorial",
    "title": "8  Philosophy of Science",
    "section": "\n8.3 Tutorial",
    "text": "8.3 Tutorial\n\n8.3.1 Assignment 1: Induction and Deduction\nBelow are two texts like you might find in a social science publication. Carefully read both, and highlight examples of inductive- and deductive reasoning. Then discuss with your group mates:\n\nDid you all highlight the same examples? Did you disagree about any of them?\nDid you find examples of induction and deduction in both texts? Is one mode of reasoning more prominent in one text than the other?\nAre all inductive and deductive inferences warranted? Do the fragments show sufficient awareness of the potential limitations of these modes of reasoning? Or conversely, are they too careful?\n\nFragment A:\n\nUsing a cross‑sectional survey of adolescents from six urban schools (N = 2,184), we examined associations between evening screen time (self‑reported minutes after 8 p.m.) and sleep quality (Pittsburgh Sleep Quality Index). Across schools and controlling for grade and gender, greater evening screen time was consistently associated with poorer sleep quality, r = .30, 95% CI [.25, .33]. Subgroup analyses by device type (phone vs. tablet) and by extracurricular workload yielded similar patterns. While these observational data cannot establish causation, finding a moderate effect across all schools suggests that higher evening screen exposure is related to adolescents’ diminished sleep quality. Reducing evening device use could help improve adolescents’ sleep outcomes.\n\nFragment B:\n\nSocial norms theory posits that behavior is shaped by perceptions of typical peer behavior. We thus hypothesized that providing households with descriptive norm feedback about neighborhood electricity use would reduce individual electricity consumption, relative to a neutral informational control. We preregistered H1: households receiving monthly comparative reports will exhibit lower kWh usage over three billing cycles than controls. The null hypothesis H0 was that there was no difference between the experimental and control conditions. In a randomized field experiment (N = 3,042 households), treatment households received reports comparing their usage to that of “similar homes”, alongside efficiency tips; control households received tips only. Mixed-effects models with random intercepts for household indicated a 2.4% reduction in usage for households in the experimental condition, \\(\\beta\\) = −0.024, SE = 0.007, p &lt; .001. We rejected the null hypothesis of no difference. These results are consistent with H1, indicating that social norms theory is a relevant framework for understanding household electricity consumption.\n\n\n8.3.2 Assignment X: Causality\nBelow are five examples of statements from social scientific papers, courtesy of Calvin Isch.\n\nFirst, read all the examples, and sort them into causal claims/non-causal claims. Discuss with your groupmates: did you classify each claim the same way? What makes the difference for you?\nWith your group, choose one of these claims and examine the associated paper. Discuss:\n\nDo you still think the claim is causal/noncausal?\nWould a causal claim be justified in this case?\nWhy/why not?\n\n\n\nExample 1:\n\nViolence exposure hampers compromise among Israelis, emphasizing the importance of abstaining from violence for conflict resolution.\n\nhttps://doi.org/10.1093/pnasnexus/pgae581\nExample 2:\n\nWe investigate both the role of gender and feminism in friends-with-benefits (FWB) relationships at a United States college, and ask whether identification with feminist ideology impacts students’ motivations and assessments of their relationships.\n\nhttps://link.springer.com/article/10.1007/s12119-014-9252-3\nExampe 3:\n\nThis distrust of atheists is driven by religious predictors, social location, and broader value orientations.\n\nhttps://doi.org/10.1177/000312240607100203\nExample 4:\n\nwe show that being in a dual-career household increases one’s willingness and lowers the perceived risk of leaving their job and joining a startup venture—especially if the household prioritizes their spouse’s career.\n\nhttps://doi.org/10.1002/smj.3481\nExample 5:\n\nWe find that those who live in regions with a greater share of migrants from Eastern Europe have more positive attitudes towards the EU but that this positive influence diminishes in highly segregated areas.\n\nhttps://doi.org/10.1080/13501763.2023.2271504\n\n\n\n\nHalpern, J. Y. (2015, May 1). A Modification of the Halpern-Pearl Definition of Causality. https://doi.org/10.48550/arXiv.1505.00162\n\n\nMorabia, A. (2013). Hume, Mill, Hill, and the Sui Generis Epidemiologic Approach to Causal Inference. American Journal of Epidemiology, 178(10), 1526–1532. https://doi.org/10.1093/aje/kwt223\n\n\nPearl, J. (2009). Causal inference in statistics: An overview. Statistics Surveys, 3, 96–146. https://doi.org/10.1214/09-SS057",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Philosophy of Science</span>"
    ]
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "9  Hypothesis Testing",
    "section": "",
    "text": "9.0.1 Falsificationism\nIn science, it is rarely possible to prove a general theory true. Instead, theories earn credibility by surviving serious attempts to refute them. This is the core of falsificationism, most closely associated with Karl Popper (Popper, 1959/2002). A scientific claim must be testable and falsifiable: it should make predictions that could, in principle, be shown false by observation. The proverbially simple example is “All swans are white.” No number of white swans can verify the claim, but a single black swan would falsify it.\nThis logic connects directly to statistical practice. In hypothesis testing we do not “prove the theory”; rather, we pose a precise claim—the null hypothesis \\(H_0\\)—and ask whether the observed data are sufficiently incompatible with \\(H_0\\) to warrant rejecting it. A small \\(p\\)-value is evidence against \\(H_0\\), not proof for any particular alternative. Likewise, failing to reject \\(H_0\\) does not verify \\(H_0\\); it merely indicates that the data are not unusually discordant with it given the test’s design and assumptions. Good scientific practice increases the riskiness of tests—deriving clear, prior predictions; minimizing researcher degrees of freedom; and using designs that would make discordant data likely if the theory were false (e.g., preregistration and prospective power analysis). In short, falsificationism reminds us that scientific conclusions are provisional and should be sharpened by attempts to refute, not by post hoc confirmation.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "testing.html#lecture",
    "href": "testing.html#lecture",
    "title": "9  Hypothesis Testing",
    "section": "\n9.1 Lecture",
    "text": "9.1 Lecture",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "testing.html#statistical-power",
    "href": "testing.html#statistical-power",
    "title": "9  Hypothesis Testing",
    "section": "\n9.2 Statistical Power",
    "text": "9.2 Statistical Power\n\n9.2.1 Hypothesis Testing: Type I and Type II Errors\nWhen we conduct a null-hypothesis significance test, we select the significance level \\(\\alpha\\). Alpha is the probability of committing a Type I error (drawing a false-positive conclusion). Since we select the alpha level, it is known. If we use \\(\\alpha = .05\\), that means that - by definition - we accept a 5% risk of committing a Type I error.\nThere is also the probability of committing a Type II error. This is called \\(\\beta\\). We don’t know the value of \\(\\beta\\) beforehand, but we can calculate it if we make some assumptions. The probability of committing a Type II error (drawing a false-negative conclusion) depends on a few factors:\n\n9.2.1.1 How big the effect is\nBig effects are harder to miss; imagine trying to detect a difference between two groups. If the mean of both groups is really close together, it will be harder to detect a difference (see below):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.1: Effect of Effect Size\n\n\n\n9.2.1.2 How big the sample is\nLarge samples make it easier to detect smaller effects; imagine that the two distributions below are sampling distributions for two groups with very small sample sizes (left) and very large sample sizes (right):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: Effect of Sample Size\n\n\n\n9.2.1.3 How ‘noisy’ the data are\nThe standard deviation is a measure of how “noisy” the data are. If observations are very spread out (high standard deviation), it will be harder to detect small differences. Consider that a small difference between two groups would be hard to detect if the two groups overlapped very much (= high standard deviation). Look at the same picture from the previous point (sample size); it illustrates this principle. The reason that both sample size and “noise in the data” have an impact on the probability of committing a Type II error is because they are used to calculate the standard error:\n\\[\nSE_M = \\frac{SD}{\\sqrt{n}}\n\\]\n\n9.2.2 Power of a Test\nThe “power” of a test is the probability that it will correctly detect a true effect of a specific size. Since \\(\\beta\\) is the probability of missing a true effect, it follows that \\(1-\\beta\\) must be the probability of detecting a true effect, or the power.\nAs explained in the previous paragraph, we must know a few pieces of information to be able to calculate \\(\\beta\\):\n\nEffect size\nSample size\nStandard deviation\n\nWhen we conduct a study, we often know the sample size and standard deviation. The effect size is unknown, but we can assume a specific effect size. Think of this as an “informative” alternative hypothesis. The standard alternative hypothesis in null-hypothesis significance testing is just “anything that’s not the null hypothesis”. So if \\(H_0: \\mu = 0\\), then \\(H_a: \\mu \\neq 0\\). Now, we must specify an exact value. For example, we could choose the smallest effect size of interest as the alternative hypothesis: Let’s say we’d be interested in a mean value of \\(\\mu = 0.2\\). Then we could set our informative alternative hypothesis as \\(H_i: \\mu = 0.2\\).\nNow we have all the information needed to calculate the power of the test. To do so, we draw two sampling distributions (see illustration below): One (in red) centered around the null hypothesis, \\(H_0: \\mu = 0\\), and one centered around the informative alternative hypothesis, \\(H_i: \\mu = 0.2\\). We find the critical value in the red distribution around the null hypothesis; remember that \\(\\alpha\\) is the 5% of probability in the right tail of the red distribution. But we can now also calculate \\(\\beta\\), the unknown probability in the tail of the blue distribution to the left of the critical value. If the informative alternative hypothesis is true, then this is the probability of failing to detect that true effect. Although this example has no numeric values, we see that the blue shaded area representing \\(\\beta\\) is slightly smaller than the red shaded area representing \\(\\alpha\\), so the probability of committing a Type II error must be less than .05, and therefore the power \\(1-\\beta\\) must be greater than 95%! If our assumptions are correct, we’d be really well able to detect a true effect of the size specified under \\(H_i\\).\n\n\n\n\n\n\n\n\n\n9.2.3 Try it Yourself\nNow, let’s calculate this by hand. Imagine that last year’s average grade was \\(M = 5\\), with a standard deviation of \\(SD = 1.5\\). This year, we have 73 students. We’ve made some changes to the teaching material, and we hope to reach an average grade of \\(M = 6\\).\nAssume that the standard deviation this year will be the same as last year, and calculate the power of being able to detect a mean grade of \\(H_i: \\mu = 6\\) when the null hypothesis is that the mean grade is the same as last year, \\(H_0: \\mu = 5\\).\nStep 1: Calculate the SE\nWe calculate the SE as \\(SE = \\frac{SD}{\\sqrt{n}} = \\frac{1.5}{\\sqrt{73}} = 0.18\\)\nStep 2: Calculate Critical Value\nThe critical value is the boundary that corresponds to \\(\\alpha = .05\\) in the distribution centered around \\(H_0\\). Looking at the t- or Z-table (because sample size is &gt;&gt;30), we see that this corresponds to a Z-value of about 1.64.\n\nConverting this back to a score on the grades scale, we get:\n\\[\n\\text{Grade}_{\\text{critical}} = (Z_{\\text{critical}} * SE) + \\mu_{H_0} = (1.64 * 0.18) + 5 = 5.3\n\\]\nStep 3: Get Left-Tail Probability for That Value\nNow, we just need to get the left-tail probability for that critical value, in the blue distribution. Convert that critical value back to a Z-value, but now in the blue distribution which is centered around \\(\\mu_{H_i} = 6\\):\n\\[\nZ = \\frac{\\text{Grade}_{\\text{critical}}-\\mu_{H_i}}{SE} = \\frac{5.3 - 6}{0.18}  = -3.89\n\\]\nThis is an extremely large (negative) Z-value; it’s not even in our table. Thus, the left-tail probability \\(\\beta\\) will be tiny - \\(\\beta &lt; .01\\).\nThat means that our power to detect a true effect of 6 would be very high - \\(1-\\beta = 1-.01 = .99\\), 99%!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "testing.html#formative-test",
    "href": "testing.html#formative-test",
    "title": "9  Hypothesis Testing",
    "section": "\n9.3 Formative Test",
    "text": "9.3 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nFor a two-tailed Z-test of the sample mean, the p-value is the probability of finding a more extreme sample mean than the observed sample mean, if the alternative hypothesis were true.\n\nTRUEFALSE\n\nQuestion 2\nA researcher performs a Z-test to test the hypotheses H0: mu = 0 versus H1: mu &gt; 0. She finds a test statistic of Z = 2.03 and a one-tailed p-value of 0.02. Statement: If the researcher had performed a two-tailed Z-test, the value of the two-tailed p-value would have been halved: 0.01.\n\nTRUEFALSE\n\nQuestion 3\nChris expects that people who have bungee jumped will score high on average on the Big 5 personality trait ‘Openness to Experience’. Openness to Experience has been measured on a 10 point scale (1= not at all open, 10 = extremely open). He takes a random sample of 45 persons who have bungee jumped and observes a mean of 6 and SD of 1.954. He tests the following hypotheses: H0: mu =&lt; 5.5 versus H1: mu &gt; 5.5 with a one-sample t-test. He assumes that Openness to Experience is normally distributed in the population. What is the smallest significance level for which Chris can reject the null hypothesis?\n\n0.1Cannot reject H00.010.05\n\nQuestion 4\nWhat is the purpose of inferential statistics?\n\nCalculating sample statisticsUsing sample data to infer properties of the populationEstimation of population parametersTesting the null hypothesis\n\nQuestion 5\nWhat is the standard error?\n\nAn estimate of the average sampling error when estimating a population parameter using a sample statisticAn estimate of the uncertainty about the sample statisticThe spread of the sample dataAn estimate of the uncertainty about the average of the sample values\n\nQuestion 6\nWhat is a hypothesis in the context of statistical testing?\n\nSomething you want to know about the populationA proposition about the population that can be tested in a sampleA statement that some parameter is equal to zeroAn explanation for observed phenomena\n\nQuestion 7\nWhat is meant by ‘power’ in statistical testing?\n\nThe probability of rejecting the null hypothesisThe probability of committing a Type I errorThe probability of committing a Type II errorThe probability of correctly finding a true effect\n\nQuestion 8\nYou want to test if the mean height of a sample of 50 students is significantly different from the population mean of 65 inches. The sample mean is 68 inches, and the standard deviation is 2 inches. What is the calculated t-value for this hypothesis test?\n\n240.4275.001.5010.61\n\nQuestion 9\nYou want to test if the average time spent on a particular task is different from 30 minutes. You collect a sample of 25 participants, and the sample mean time spent on the task is 28 minutes with a standard deviation of 3 minutes. Conducting a two-tailed t-test, what is the calculated t-value?\n\n-16.673.33-0.67-3.33\n\n\n\n\nShow explanations\n\nQuestion 1\nWhen calculating a test statistic, we assume the null hypothesis to be true - not the alternative hypothesis.\nQuestion 2\nThe p-value for a two-tailed test is twice as large as for a one-tailed test (because you have the same one-tailed probability in both tails). For two-tailed tests, if the observed effect is in the direction of the alternative hypothesis, you can half the two-tailed p-value.\nQuestion 3\nDivide the standard deviation by the square root of 45 to get the standard error. Then, divide the difference between the observed mean of 6 and the hypothesized mean of 5.5 by that standard error to get the test statistic. Then, find the critical t-values for a one-sided test with the three alpha levels mentioned in the answers in the t-distribution for 44 degrees of freedom (n - 1). Note that the answer is .05!\nQuestion 4\nInferential statistics involves using sample data to make inferences or draw conclusions about the larger population from which the sample was drawn. It allows researchers to make educated guesses about population parameters based on the information collected from the sample. Calculating sample statistics is a step in the inferential process, but it is not the primary purpose of inferential statistics. Testing the null hypothesis is another inferential procedure, but it is a specific type of hypothesis testing, and not the overall purpose of inferential statistics.\nQuestion 5\nThe standard error is a measure of the uncertainty associated with the sample statistic as estimator of the population parameter. It represents how much the sample statistic is expected to vary from one sample to another if multiple samples were drawn from the same population.\nQuestion 6\nIn statistical testing, a hypothesis is a testable proposition about the population that can be examined using sample data. It is a statement or assumption that researchers put to the test to determine if there is evidence to support it or not. The hypothesis is formulated based on the theory or observations made about the population.\nQuestion 7\nPower in statistical testing refers to the probability of correctly detecting a true effect or relationship between variables. It is the likelihood of finding a significant result in a study when the effect being investigated truly exists in the population. It is important to have sufficient power in a study to avoid false-negative findings, where we fail to reject the null hypothesis when there is a real effect. Power is one minus the probability of committing a Type II error, or 1-beta.\nQuestion 8\nTo calculate the t-value for this hypothesis test, you can use the formula: t = (sample mean - population mean) / (standard deviation / square root of sample size). Plugging in the values, t = (68 - 65) / (2 / sqrt(50)) = 10.61.\nQuestion 9\nThe t-value for a two-tailed t-test can be calculated using the formula: t = (sample mean - population mean) / (standard deviation / square root of sample size). In this case, the population mean is 30 minutes. Plugging in the values, t = (28 - 30) / (3 / sqrt(25)) = -3.33.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "testing.html#tutorial",
    "href": "testing.html#tutorial",
    "title": "9  Hypothesis Testing",
    "section": "\n9.4 Tutorial",
    "text": "9.4 Tutorial\n\n9.4.1 Assignment 1: Hypothesis Testing - Formulating Hypotheses\nDiscuss with your portfolio group the logic behind hypothesis testing, and how it relates to your personal (and group’s) research interests.\nConsider the following three research descriptions. Formulate H0 and H1 in words. Discuss your answers with your group members.\nResearchers want to know whether it matters for test performance if an exam is completed on a computer or using paper and pencil. Hence, the research question reads: Is there an effect of the type of administration (computer or paper and pencil) on the test performance?\nWhat would be the H0 and HA for this study?\n\n\nShow answer\n\nThis appears to be an undirected hypothesis about a mean difference for two independent samples, without a clearly specified alternative hypothesis. Thus, we could state:\n\\(H_0: \\mu_{computer} = \\mu_{paper}\\) \\(H_A: \\mu_{computer} \\neq \\mu_{paper}\\)\n\nResearchers want to know whether the alcohol consumption among Dutch students differs from the alcohol consumption in the general Dutch population. Using CBS statistics, they know that in the general population the average alcohol consumption is 5.6 glasses a week. The question is whether the average alcohol consumption among students is different from this national average.\nWhat would be the H0 and H1 for this study?\n\n\nShow answer\n\nThis appears to be an undirected hypothesis about the difference between a mean and a hypothesized value, without a clearly specified alternative hypothesis. Thus, we could state:\n\\(H_0: \\mu = 5.6\\) \\(H_A: \\mu \\neq 5.6\\)\n\nResearchers want to study whether social isolation is associated with income.\nWhat would be the H0 and H1 for this study?\n\n\nShow answer\n\nThis appears to be an undirected hypothesis about an association between two variables, without a clearly specified alternative hypothesis. We could thus state:\n\\(H_0: \\rho = 0\\) \\(H_A: \\rho \\neq 0\\)\n\nFormulating the hypothesis is an important very first step in hypotheses testing. Continue with the next assignment, in which we will go through the steps of a hypothesis test.\n\n9.4.2 Assignment 2: Test Statistics, Alpha and Significance\nIn this assignment we will go through the steps of a hypothesis test.\nWhile going through the steps we will come across the most important concepts related to hypothesis testing.\nFor the next steps, we consider the following situation:\nSuppose we are interested in the personality profile of musicians; that is, we want to know whether, on average, personality characteristics of musicians differ from those of the general population. For now, we’ll only focus on Openness. We pretend that we have collected data among 25 musicians using a validated scale for which previous research has shown that in the general population the scores are normally distributed with mean 50 and SD 15. It is our task to test whether the mean of Openness for musicians differs from the mean in the general population. To keep things simple, we assume that in the population of musicians the SD is the same as in the general population; that is, we assume that LaTeX: \\(\\sigma = 15\\).\nLet openness be the variable of interest. Let \\(\\mu_{musicians}\\) represent the mean openness in the population of musicians. The hypothesis test amounts to testing:\n\\(H0: \\mu_{musicians} = 50\\)\n\\(H1: \\mu_{musicians} \\neq 50\\)\nNow, when we do the hypothesis test, we seek for evidence against the null hypothesis. More specifically, our testing procedure starts with the assumption that H0 represents the truth and as long as we don’t have convincing evidence that our assumption is false we stick to that assumption.\nThe question is, however, when do we have convincing evidence against H0?\nFinding evidence against H0 works as follows:\nIf H0 is true, we expect mean values close to H0. And, if we observe a mean value that is much different from the value under H0, we have convincing evidence against H0. If this happens, we reject H0 as representing the truth and accept the alternative hypothesis, H1.\nHypothesis testing fits Popper’s philosophy of falsification. He introduced this well-known analogy to explain the logic of falsificationism:\n\nSuppose we assume that all Swans are white, \\(H_0: Swans = white\\)\n\nWe would then not expect to observe black ones.\nIf we do observe black swans, our initial hypothesis is called into question.\nThe number of white swans we see (= observations consistent with the hypothesis) does not provide evidence for \\(H_0\\), because there could always be a black swan out there we haven’t observed yet.\n\nSo, the next questions are:\nWhat are the sample values we can expect under H0? When is evidence “convincing” enough? To answer the first question we have to go back to sampling distributions!\nFor the second question, we need a criterion. We have to realize that even if H0 is true, sample values can be far off just by sampling fluctuations (i.e., by chance). The common criterion is: if the observed value is among the 5% most unlikely samples under H0 (i.e. if H0 is true), we reject the null hypothesis.\nLet’s go back to our example about musicians.\nLet X be openness. Under H0 we assume that X is normally distributed with mean 50 and SD equal to 15.\nWhat are the mean and standard deviation of the sampling distribution of the mean under H0 given that the sample size is 25? And what do we call the standard deviation of the sampling distribution?\n(Use what you have learned in the previous lectures. Hint: first make a drawing of the situation, then do the computations).\n\n\nExplanation\n\nSampling distribution:\n\nMean: \\(\\mu = 50\\)\n\nStandard error ( =SD of sampling distribution!): \\(\\sigma_{\\bar{X}} = \\frac{15}{\\sqrt{25}}= 3\\)\n\n\n\nSuppose we want to indicate sample means that are unlikely if H0 would be true. In particular, we want to know how far the sample mean must be from the hypothesized mean to be among the 5% of all possible samples under H0 that are furthest away from the hypothesized means.\nWhat should the value of the sample mean be to fall within the 5% most deviant samples if the sample size is 25?\n\n\nExplanation\n\nWe are talking about the distribution of the mean; so we need to work with the sampling distribution. We want to know the cut offs that mark the 2.5% highest and 2.5% lowest means. We first have to find the Z-values: they are 1.96 for the highest 2.5%, and (by symmetry) -1.96 marks the 2.5% lowest.\nHence, to be among the 5% of all possible sample means that are most unlikely under H0, the sample mean should be:\nlarger than 50+1.96 x 3 = 55.88 or smaller than 50-1.96 x 3 = 44.12\n\nLet’s do some more exercises on the Z-test.\nSuppose the mean for Openness we found in our sample was 59.\nIf we use a significance level of 5%, would we reject the null hypothesis? \nYes\nNo\nIn the previous step we used cut offs for the sample means to decide about significance. The cut off scores were obtained via the Z-distribution. However, doing all these computations is not necessary (there’s a shortcut!!). In fact, if we know the Z-value for the sample, we can easily find out if the sample is among the 5% of the most unlikely sample means. We only have to compare the value with 1.96 and -1.96 to see whether that is the case.\nIn this course, we will use Z-values for different purposes. In these specific calculations, Z is used as a Test Statistic. A test statistic quantifies evidence against the null hypothesis. In this case, the Z test statistic expresses how far away from the mean under the null hypothesis the observed mean is, in terms of the number of standard errors.\nThe Z test-statistic follows the standard normal distribution. The values 1.96 and -1.96 are called the critical values and they mark the 5% most unlikely sample means under H0. In other words, the critical values mark the reject region for H0.\nSo, if we compute the Z-value for the sample mean, and if that sample value of Z falls in the rejection region, we reject H0 (we found something that is unlikely enough to no longer believe H0 is true). If H0 is rejected we speak of a significant result. See the graph below:\n\nFollowing these steps to test a mean is one example of performing a “Z-test”!\nWe can use the Z-test to test hypotheses about the population mean if we know the population \\(\\sigma\\).\nThe test statistic for the Z-test is:\n\\(z  = \\frac{\\bar{X}-\\mu_{H_0}}{\\sigma_{\\bar{X}}}\\)\nThis statistic is computed using the mean from the sample, the hypothesized mean under H0 and \\(\\sigma\\).\nH0 is rejected at the 5% significance level if z is either larger than 1.96 or smaller than -1.96.\nSo far, we rejected the null hypothesis if the sample is among the 5% most unlikely sample means under H0. This 5% was called the significance level, and is denoted as \\(\\alpha = .05\\). However, we could just as well choose 1% or .5%.\nWhat would be the critical values for the Z-test if one tests at \\(\\alpha = .01\\)? \nWhat would be the critical values for the Z-test if one tests at \\(\\alpha = .005\\%\\)? \nFor historical reasons, social scientists tend to use \\(\\alpha = 0.05\\) as a default. So in this course, if alpha is not explicitly stated, assume \\(\\alpha = 0.05\\).\nWhen we test hypotheses we reject H0 if the sample we find is unlikely if H0 is true. However, the flip side is that, even though H0 is true, we may find a sample that is much different by chance, and erroneously reject H0. Or, in other words, we could make an error. Rejecting H0 while it is true in reality is called a Type I error!\nConsider the following:\n\nIf H0 is true, and you test at \\(\\alpha = 0.05\\), what is the probability of committing a Type I error?\nWhat is the link between the \\(\\alpha\\)-level and type I error rate?\n\n\n\nExplanation\n\n\nIf H0 is really true (i.e., H0 should not be rejected), then the probability that the sample mean is among the 5% most unlikely is equal to 5%.\nThe alpha level specifies the risk of a Type I error. So if one tests at an alpha level of .05, it means that one accepts a risk of 5% to commit a Type I error.\n\n\nProperties of the Z-test:\nUsed to test hypotheses about the mean in a population, assuming \\(\\sigma\\) known.\nThe test-statistic equals \\(z = \\frac{\\bar{X}-\\mu}{\\sigma_{\\bar{X}}}\\)\nThe test statistic is normally distributed.\n\n9.4.3 Assignment 3: Z-test\nIn this assignment we will apply the Z-test.\nThis assignment first presents an example, followed by two practice questions.\nA researcher wants to test \\(H_0: \\mu = 50\\) against \\(H_1: \\mu \\neq 50\\)\nData are available from a random sample of 26 respondents. The mean was 53.7. The researcher assumes the SD in the population is 8.5. Perform all steps of the Z-test.\n\n\nExplanation\n\nStep 1: Formulate hypotheses\n\\(H_0: \\mu = 50\\) \\(H_1: \\mu \\neq 50\\)\nStep 2: Compute test statistic\nStandard error: \\(\\frac{8.5}{\\sqrt{26}}=1.667\\)\nTest statistic: \\(z = \\frac{53.7-50}{1.667}=2.212\\)\nStep 3: Decide about significance\n\\(\\alpha = .05\\), so critical values +/- 1.96.\nOur test statistic exceeds this critical value.\nThe sample mean thus falls in the rejection region, and we should conclude that the test is significant so \\(H_0\\) s rejected.\nStep 4: Draw conclusion\nWe have convincing evidence that the population mean differs from 50.\n\nA researcher wants to test whether the population mean is equal to 80. Data are available from a random sample of 60 respondents. The mean was 74. The researchers assume the SD in the population is 40. Perform and report all steps of the Z-test. What is the resulting p-value? \nA researcher wants to test whether the population mean is equal to 500. Data are available from a random sample of 75 respondents. The mean was 546. The researchers assume that the SD in the population is 200. Perform all steps of the Z-test. Use \\(\\alpha = .01\\). Perform and report all steps.\n\n\nShow answer\n\nStep 1: Hypotheses: \\(H_0: \\mu=500\\), \\(H_1: \\mu \\neq 500\\)\nstep 2: Compute Statistic:\n\nstandard error: \\(\\frac{200}{\\sqrt{75}} = 23.094\\)\n\ntest statistic: \\(z  = \\frac{546-500}{23.094}=1.992\\)\n\n\nStep 3: Decide about significance.\nZ does not exceed +/- 2.576. This means that Z does not fall in the reject region when tested at the 1% significance level. The test is not significant.\nStep 4: Draw conclusion\n\\(H_0\\) is not rejected.\n\n\n9.4.4 Quiz\n\n“The null and alternative hypothesis are deduced from the data.” \nTRUE\nFALSE\n“When performing a hypothesis test, we start by assuming \\(H_0\\) is true.” \nTRUE\nFALSE\n“If we reject \\(H_0\\) with \\(\\alpha=0.05\\), then we will also reject it at \\(\\alpha=0.10\\), assuming all other quantities are held constant.” \nTRUE\nFALSE\n\n\nExplanation\n\nThe critical values of \\(\\alpha =0.05\\) are +/- 1.96. Hence, if \\(H_0\\) is rejected it means that z in the sample is larger than 1.96 or smaller than -1.96.”\nThe critical values of \\(\\alpha =0.1\\) are +/- 1.645. This means that for rejecting \\(H_0\\) at this alpha level, that z should be larger than 1.645 or smaller than -1.645. That is implied by the fact that it exceeds +/- 1.96.\n\n“If we reject \\(H_0\\), then \\(H_0\\) is surely wrong.” \nTRUE\nFALSE\n\n\nExplanation\n\nWe should always be aware of the possibility of making a Type I error. The probability of making a Type I error is equal to \\(\\alpha\\).\n\n“Increasing the sample size n (and holding all the rest constant) decreases the probability of a Type I error.” \nTRUE\nFALSE\n\n\nExplanation\n\nIncreasing the sample size n (and holding all the rest constant) does not decrease the probability of a Type I error.\nThe Type I error is determined by the alpha level.\nIf our sample is among the 5% most unlikely sample means of all possible sample means with the same size under \\(H_0\\), whatever that sample size N may be.\nIncreasing the sample size n (and holding all the rest constant) does not decrease the probability of a Type I error.\n\n\n\n9.4.5 Assignment 4: Z-test and Alpha-levels\nIn this assignment we will practice some more with the Z-test, meanwhile we will review important concepts of hypothesis testing. In particular, we will look at significance levels.\nTo test hypotheses, we need to specify the “significance level”, usually denoted by \\(\\alpha\\). The significance level is our decision criterion to reject H0.\nThe most common choice is .05. But what does this criterion exactly entail?\nDiscuss with your group what an \\(\\alpha\\) level entails.\n\n\nExplanation\n\nIf we test at an \\(\\alpha\\) of .05 it means that we are willing to reject H0 in favor of H1 if our sample mean belongs to the 5% most extreme scores (2.5% in each tail) under the null hypothesis.\nIf indeed the sample mean is among this 5%, it means that we have observed a sample in a range that is quite unlikely if the null hypothesis would be true and, therefore, justifies rejection of the null hypothesis.\n\nIn the previous assignments you already used the critical values for the Z-test for specific alpha levels.\nFor two-tailed tests, it holds that if the absolute value of Z exceeds the critical value, we may reject \\(H_0\\).\nLet \\(Z_\\text{crit}\\) be the critical value. For the Z-test it holds that:\n\n\n\\(Z_\\text{crit} = 1.65\\), if \\(\\alpha = 0.10\\) (two-tailed)\n\n\\(Z_\\text{crit} = 1.96\\), if \\(\\alpha = 0.05\\) (two-tailed)\n\n\\(Z_\\text{crit} = 2.58\\), if \\(\\alpha = 0.01\\) (two-tailed)\n\n9.4.6 Quiz\n\nResearchers want to test whether \\(\\mu=70\\). They assume that \\(\\sigma = 10\\). Researchers found a mean of 72 in a random sample of 40 persons.\nTrue or false:\n\\(H_0\\) can be rejected at one of the three levels discussed above (\\(\\alpha = .10, .05, .01\\). \nTRUE\nFALSE\n“If the two-tailed test is significant at the 5% level, it will also be significant at the 1% level (keeping everything else fixed).” \nTRUE\nFALSE\n“If the two-tailed test is not significant at the 10% level, it won’t be significant at the 5% level either (keeping everything else fixed).” \nTRUE\nFALSE\n“If the two-tailed test is not significant at the 5% level, it could still be significant at the 10% level (keeping everything else fixed).” \nTRUE\nFALSE\n“If the two-tailed test is significant at the 1% level, it might not be significant at the 5% level (keeping everything else fixed).” \nTRUE\nFALSE\n\n\n9.4.7 Assignment 5: P-values\nWe will now focus on the interpretation of the p-values and how to use the p-values to decide about significance.\nConsider the following situation:\nScores on a test measuring confidence in police are normally distributed in the general population, with \\(\\mu = 500\\) and an \\(\\sigma = 50\\). Researchers want to know if the average confidence level is different for those who have been a victim of crime. They collect data for 60 victims. They find a sample mean of 511. They test \\(H_0: \\mu = 500\\) against \\(H_1: \\mu \\neq 500\\), while assuming that the population variance is \\(\\sigma = 50\\).\nCompute the p-value. Draw a graph for the two-tailed p-value. Write down in your own words and as precise as possible the interpretation of the p-value in the answer box below. Then, discuss your response with your group.\n\n\nExplanation\n\n\nThe p-value represents the proportion of all possible sample means that are further away from our hypothesized mean than the observed sample mean is.\nWe have the sampling distribution with \\(\\mu = 500\\) and \\(\\sigma_{\\bar{X}} = \\frac{50}{\\sqrt{60}} = 6.455\\).\nFirst, we compute the right-tail area: \\(P(\\bar{X} &gt; 511) = P(Z &gt; 1.70) = 0.0446\\).\nHence, 4.66% of all possible samples is further away from \\(H_0\\) on the right side.\nSecond, we compute the left-tail area. These are the sample means that are more than 11 points from the hypothesized mean to the left \\(P(\\bar{X} &lt; 489) = P(Z &lt; -1.70) = 0.0446\\).\nHence, the two-tailed p-value is 0.0892.\n\n\nIs the test significant at the 5% level? \nTRUE\nFALSE\nIs it significant at the 1% level? \nTRUE\nFALSE\nResearchers test whether \\(\\mu = 90\\). They assume that \\(\\sigma=21\\). The sample mean was 85. Sample size was 50.\nWhat is the two-tailed p-value? \nWhat is the highest level at which the test is significant? \n0.1\n0.05\n0.01\n0.005\nResearchers test whether \\(\\mu = 35\\). They assume \\(\\sigma =16\\). The sample mean was 38. Sample size was 64.\nCompute the two-tailed p-value and indicate which of the following statements is true.\n\nThe test is not significant at 10%, not significant at 5% and not significant at 1%.The test is significant at the 10% and 5% level, but not at the 1% level.The test is significant at the 10%, 5% level, and 1% level.The test is significant at the 10% level, but not at 5% or 1% level.\n\nConsider these true- or false statements:\nIf a two-tailed p-value is .0567 then the test is significant at the 10% level but not at the 5% level. \nTRUE\nFALSE\nIf a two-tailed test is significant at the 5% level but not at the 1% level, then the two-tailed p-value will be less than 0.01. \nTRUE\nFALSE\nA two-tailed p-value of 0.060 indicates that we have 6% chance that the null hypothesis is true. \nTRUE\nFALSE",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "glm1_regression.html",
    "href": "glm1_regression.html",
    "title": "10  GLM-I: Linear Regression",
    "section": "",
    "text": "10.1 Lecture\nThe General Linear Model (GLM) is a family of models used to analyze the relationship between an outcome variable and one or more predictors. In this lecture, we will focus on bivariate linear regression, which describes a linear relationship between a continuous outcome variable and a continuous predictor. However, it’s important to note that the GLM encompasses other members that can handle predictors of any measurement level (continuous or categorical), multiple predictors, transformations of the outcome and predictors, and different error distributions.\nLinear regression is based on the concept of using information about other variables associated with the outcome to improve predictions. It begins with the understanding that the mean is the best predictor (expected value) when no further relevant information is available. However, if we have information about other variables, such as the number of hours studied being strongly associated with exam grades, we can use that information to enhance our predictions. This process is known as regression.\nTo visually explore associations between two variables, we often use scatterplots. Scatterplots require both variables to be at least of ordinal measurement level. By plotting the data points, we can observe whether there is a linear pattern or trend. In linear regression, we aim to find a line that represents the best possible predictions. This line, called the regression line, goes through the middle of the cloud of data points.\nThe regression line is described by the formula Y = a + bX, where “a” is the intercept (the predicted value when X equals 0) and “b” is the slope (how steeply the line increases or decreases). The predictions made using the regression line are not identical to the observed values, as there is always some prediction error. The Ordinary Least Squares method is used to obtain the line that minimizes the sum of squared prediction errors.\nIn a bivariate regression, the regression formula expands to include the individual prediction error, assuming that the errors are normally distributed around the regression line with a mean of zero. The regression model is represented as Yi = a + b * Xi + ei, where Yi is the individual’s score on the dependent variable, a is the intercept, b is the slope, Xi is the individual’s score on the independent variable, and ei is the individual prediction error.\nHypothesis tests can be conducted on the regression coefficients to determine their significance. The default null hypothesis for the intercept is that it is equal to zero, while the null hypothesis for the slope is also zero. The t-test is commonly used, with the degrees of freedom being n - p, where n is the sample size and p is the number of parameters. By testing the coefficients, we can determine the statistical significance of the relationship between the predictor and the outcome.\nWhile linear regression offers valuable insights, it is essential to consider the assumptions underlying the model. These assumptions include linearity of the relationship between the predictor and the outcome, normality of residuals (prediction errors), homoscedasticity (equal variance of residuals), and independence of observations. Violations of these assumptions can affect the validity of the model and lead to misleading results. Checking and addressing these assumptions is crucial for accurate and reliable regression analysis.\nLinear regression is a powerful tool for analyzing the relationship between variables, and a building block for many more advanced analysis techniques. It allows us to make predictions based on available information and understand the strength and significance of the relationship between a continuous predictor and continuous outcome. By considering the assumptions and conducting hypothesis tests, we can ensure the validity of our regression models and draw meaningful conclusions from the analysis.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GLM-I: Linear Regression</span>"
    ]
  },
  {
    "objectID": "glm1_regression.html#formative-test",
    "href": "glm1_regression.html#formative-test",
    "title": "10  GLM-I: Linear Regression",
    "section": "\n10.2 Formative Test",
    "text": "10.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat is the General Linear Model (GLM)?\n\nA family of models to analyze the relationship between multiple outcomes and multiple predictorsA family of models to analyze the relationship between categorical outcomes and continuous predictorsA family of models to analyze the relationship between continuous outcomes and categorical predictorsA family of models to analyze the relationship between one outcome and one or more predictors\n\nQuestion 2\nWhat type of relationship does bivariate linear regression describe?\n\nA linear relationship between a continuous outcome variable and a predictor of any measurement level with normally distributed prediction errorsA linear relationship between a categorical outcome variable and a continuous predictorA relationship of any shape between a continuous outcome variable and a predictor of any measurement level, with normally distributed prediction errorsA nonlinear relationship between a continuous outcome variable and a continuous predictor\n\nQuestion 3\nWhat does it mean when we say ‘The mean is the best predictor when there’s no further relevant information’ in the context of regression?\n\nThe mean is the expected value when we have no additional information about predictorsThe mean is only a good predictor when there's no variability in the outcomeThe mean is the best predictor regardless of whether we have additional information about predictorsThe mean is the least accurate predictor when we have no additional information about predictors\n\nQuestion 4\nWhat is the purpose of a scatterplot in the context of regression analysis?\n\nTo explore causal relationships between two variablesTo calculate the mean and standard deviation of two variablesTo determine the distribution of two variablesTo visualize associations between two variables\n\nQuestion 5\nWhat is the primary goal of ordinary least squares regression in linear modeling?\n\nTo find the line that predicts the maximum number of data points correctlyTo find the line that fits the data exactly by minimizing the sum of absolute prediction errorsTo find the line that passes through the mean of the data pointsTo find the line that gives the best possible predictions by minimizing the sum of squared prediction errors\n\nQuestion 6\nIn the formula ‘Yi = a + bXi + ei’, what are the parameters?,\n\nX and YXi and eiYi, Xi, and eia and b\n\nQuestion 7\nHow are the coefficients ‘a’ and ‘b’ interpreted in the context of linear regression?\n\n'a' is the intercept where the line crosses the Y-axis, and 'b' is the predicted value when X equals 0'a' is the intercept where the line crosses the Y-axis, and 'b' is the slope indicating how steeply the line increases or decreases'a' is the slope indicating how steeply the line increases or decreases, and 'b' is the intercept where the line crosses the Y-axis'a' is the predicted value when b equals 0, and 'b' is the slope indicating how steeply the line increases or decreases\n\nQuestion 8\nWhat is the purpose of checking assumptions in linear regression?\n\nTo ensure that the model accurately represents the data and that inferences are validTo determine the significance of the predictorsTo improve the visualization of the scatterplotTo find ways to manipulate the data to fit the model better\n\nQuestion 9\nWhat is the assumption of homoscedasticity?\n\nThat prediction errors are equally distributed for all values of the predictorThat the dependent variable is normally distributedThat the prediction errors are normally distributedThat the effect of the predictor on the outcome is linear and monotonous\n\nQuestion 10\nGiven regression formula Yi = 65.13 + 95.27*Xi+ei, what is the predicted score for a person who scores 15 on X?\n\n1072.221494.1880.13Can't say\n\nQuestion 11\nFrank scores 22 on Yi and has a prediction error of 7.33. What was his predicted value, given regression formula Yi = 65.13 + 95.27*Xi+ei?\n\n27.332214.67-0.53\n\n\n\n\nShow explanations\n\nQuestion 1\nThe GLM is used to analyze the relationship between a single outcome and one or more predictors.\nQuestion 2\nBivariate linear regression specifically describes a linear relationship between two continuous variables.\nQuestion 3\nWhen there’s no further information available, the mean is the most reasonable estimate for the outcome.\nQuestion 4\nScatterplots visually depict the relationships and associations between two variables.\nQuestion 5\nOrdinary least squares regression aims to minimize the sum of squared prediction errors to find the best-fitting line.\nQuestion 6\nThe parameters of a model are the quantities estimated from data. Yi and Xi are the data; ei is calculated based on the model-implied predictions.\nQuestion 7\nThe coefficient ‘a’ represents the intercept, and the coefficient ‘b’ represents the slope of the regression line.\nQuestion 8\nAssumption checks ensure that the model accurately represents the data and that any inferences drawn from the model are valid.\nQuestion 9\nHomoscedasticity literally means: equal variances; this assumption means that the variance of prediction errors is equal at all values of the predictor.\nQuestion 10\n65.13 + 95.27*15 = 1494.18\nQuestion 11\nThe observed score Yi is equal to the predicted score plus prediction error. If prediction error was 7.33, the predicted score must have been 22-7.33 = 14.67",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GLM-I: Linear Regression</span>"
    ]
  },
  {
    "objectID": "glm1_regression.html#in-spss",
    "href": "glm1_regression.html#in-spss",
    "title": "10  GLM-I: Linear Regression",
    "section": "\n10.3 In SPSS",
    "text": "10.3 In SPSS\n\n10.3.1 Linear Regression",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GLM-I: Linear Regression</span>"
    ]
  },
  {
    "objectID": "glm1_regression.html#tutorial",
    "href": "glm1_regression.html#tutorial",
    "title": "10  GLM-I: Linear Regression",
    "section": "\n10.4 Tutorial",
    "text": "10.4 Tutorial\n\n10.4.1 Regression Analysis\nIn this assignment we will make a start with regression analysis.\nWe will go through the different steps of running and interpreting a regression analysis.\nOpen the file Work.sav to get started.\nConsider the following research question: “Does variety at work predict pleasure at work?”\nWhat is the dependent variable in this case? \nPleasure\nVariety\nTo answer the research question, we will run a linear regression analysis.\nSelect the following menu item: Analyze &gt; Regression &gt; Linear\nChoose the dependent variable (scpleasure) and independent variable (scvariety). Paste and run the syntax.\nIf you look in the output, you will see that SPSS shows four tables in the output file.\nIn the table labeled “Model Summary” we can find the R2 value. R2 indicates the total proportion of explained variance in the dependent variable in the model; this is the focus of next week’s class.\nWhat proportion of the variance Emotional pressure (scpleasure) is explained by our single predictor Variety at work (scvariety)? \nConsider the unstandardized Coefficients in the table labeled “Coefficients”.\nWhat is the value of the intercept (b0) for the regression line? \nHow should we interpret the intercept (or “constant”) within the context of this analysis?\n\nThe sample average of Pleasure is this many pointsEveryone who reports zero Variety at work (meaning a score of 0 on scvariety) has a value of this many points on Pleasure.Someone who reports zero Variety at work (meaning a score of 0 on scvariety) has an expected value of this many points on Pleasure.For every point in Variety at work, we expect an increase of this many points in Pleasure.\n\nConsider the unstandardized regression coefficients again.\nWhat is the value of the regression coefficient of scpleasure on scemoti (b1)? \nHow should we interpret the regression coefficient of scvariety within the context of this analysis?\n\nIf someone’s score on Variety at work increases with 1 SD, their score on Pleasure increases by this many SDs.This is the sample average score of Variety at work.This is the sample average of PleasureIf someone’s score on Variety at work increases with 1 point, their score on Pleasure increases by this many points.\n\nThe “Coefficients” table also shows whether or not the effect of scvariety on scpleasure is significant.\nWhat is the p-value for the regression coefficient for scvariety? \nCan we conclude that the effect of scvariety on scpleasure is significant? (use \\(\\alpha\\) = .05). \nYes\nNo\n\n10.4.2 Assumptions\nRecall that regression assumes linearity, normality of residuals, homoscedasticity (equal variance of residuals), and independence of observations. We will check each of these assumptions in turn, except for independence of observations because this is a property of our sampling method and cannot be checked statistically.\n\n10.4.2.1 Scatterplot\nA scatter plot can provide some insight into linearity.\nTo make a scatter plot: Graphs &gt; Legacy Dialogs &gt; Scatter/Dot &gt; Simple Scatter\nPlace variety at work on the X axis and emotional pressure on the Y axis.\nIs the assumption of linearity met in this case? \nYes\nNo\n\n10.4.2.2 Regression Diagnostics\nAside from the scatterplot, we can check the assumptions of regression by requesting additional options in the analysis.\nGo back to the analysis dialog via Analyze –&gt; Regression –&gt; Linear. Verify that you still have the correct predictor and outcome.\nThen, click the Plots button. You want a plot of the predicted values against the residual values, so put ZPRED in the X box and ZRESID in the Y box.\nAlso check the boxes for a Histogram and normal probability plot, then hit continue.\nNow paste and run the syntax. You should see the following added to your previous regression syntax:\n  /SCATTERPLOT=(*ZRESID ,*ZPRED)\n  /RESIDUALS HISTOGRAM(ZRESID) NORMPROB(ZRESID)\n\n10.4.2.3 Linearity\nHow can we test linearity using this additional output?\nFirst, we can use the “Normal P-P plot”. If the relationship is perfectly linear, all dots should be on the diagonal line. If the points are deviating from the line, the relationship is not perfectly linear. Small deviations are OK; for example, the plot below shows a linear association:\n\nDoes the P-P plot for your regression give cause for concern for violation of the assumption of linearity? \nYes\nNo\nUnclear\n\n10.4.2.4 Normality\nOne way to check normality is by examining the histogram of residuals. This histogram displays a normal curve by default. If the observed residuals deviate strongly from this histogram, there may be a problem.\nThe plot below shows a residual histogram with some minor deviations from normality (too few scores near the mean). This is probably still fine:\n\nDoes the residual histogram for your regression give cause for concern for violation of the assumption of normal residuals? \nYes\nNo\nUnclear\n\n10.4.2.5 Homoscedasticity\nWe examine homoscedasticity using a plot of standardized predicted values against standardized residuals. We want residuals to be identically distributed on the Y-axis for all values on the X-axs. In other words, this scatterplot should look like a dot cloud (no pattern) around the zero line (left picture below), and not like a pattern (right picture below).\n\nDoes the scatterplot for standardized predicted values against residual values for your regression give cause for concern for violation of the assumption of homoscedasticity? \nYes\nNo\nUnclear\nWrite up a discussion of potential violations of the assumptions for your regression, then check your answer.\n\n\nExplanation\n\nWe observed that the observed scores deviated from the P-P plot in an S-shaped pattern. We further observed that, in a histogram of standardized residuals, the observed residuals were right-skewed. Finally, we observed less variance around the regression line for low scores and more variance around the regression line for high scores.\nThese findings give cause for concern of violations of the assumptions of regression. One potential explanation is that the effect might be quadratic instead of linear. (Optional)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GLM-I: Linear Regression</span>"
    ]
  },
  {
    "objectID": "glm2_sum_squares.html",
    "href": "glm2_sum_squares.html",
    "title": "11  GLM-II: Sums of Squares",
    "section": "",
    "text": "11.1 Lecture\nLast week we discussed how linear regression represents the relationship between a predictor variable (X) and an outcome variable (Y) as a diagonal line. This line will have some prediction error for each individual data point. The regression line, by definition, is the line with the smallest possible overall prediction error (across all participants). Today, we explore this concept of “smallest possible overall prediction error” in more detail.\nThe sum of prediction errors across all participants is always zero because the regression line passes through the “middle” of the data. So there’s always an equal amount of negative prediction errors and positive ones, which cancel each other out. To calculate the “total prediction error”, we must square the prediction errors, which eliminates the negative values and ensures that we can add them up to a positive number. We call the sum of squared prediction errors the the “sum of squared errors” (SSE). When we estimate a regression model in statistical software, we ask it to find the regression line that minimizes the SSE and give us the line with the smallest prediction errors. For bivariate linear regression, we can calculate this line using matrix algebra (outside the scope of this course); we call this the “ordinary least squares” method.\nNow that we know the total amount of prediction error (SSE), we also have a basic measure of goodness of fit for the regression line. However, SSE is not readily interpretable because it lacks a meaningful scale. To assess the goodness of fit relative to a baseline, we compare the SSE of the regression line to the sum of squares we would obtain if we did not use the predictor variable - that is, if we just predicted the mean value for each individual. A model with only the mean and no predictor variables is called the null model. The sum of squared distances between the mean and individual observations is referred to as the Total Sum of Squares (TSS), which represents the average squared distance of individual observations from the mean of Y.\nTo understand how much of the TSS is explained by the regression line, we calculate the Regression Sum of Squares (RSS). This is the difference between the TSS and the SSE: the reduction in TSS achieved by using the regression line to predict observations instead of just the mean. It indicates how well the regression line explains the variance in the dependent variable.\nWe can standardize this RSS by dividing it by the SSE, which gives us the “explained variance” \\(R^2\\), which ranges from 0 to 1. A higher R^2 value indicates that a larger portion of the total variance in the dependent variable is accounted for by the predictor variable. Explained variance is the proportion of the total sum of squares (TSS) that is explained by the regression line (RSS).\nUnderstanding these sums of squares gives us a good foundation for understanding another statistic: the correlation coefficient \\(r\\). The correlation coefficient describes the strength and direction of the linear relationship between two variables. It differs from regression because regression describes one of these variables as an outcome of the other: an asymmetrical relationship. Correlation instead just describes how strongly these two variables are associated without labeling one as the predictor and the other as the outcome: a symmetrical relationship. The correlation coefficient is a standardized measure of the strength of this association that ranges from -1 (perfectly negatively associated) to 1 (perfectly positively associated). A correlation coefficient of 0 means that there is no association between X and Y. Correlation and regression are very closely related, as the squared correlation coefficient (\\(r\\), squared) is the same as the measure of explained variance from simple linear regression, \\(R^2\\), and is also the same as the standardized regression coefficient.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GLM-II: Sums of Squares</span>"
    ]
  },
  {
    "objectID": "glm2_sum_squares.html#formative-test",
    "href": "glm2_sum_squares.html#formative-test",
    "title": "11  GLM-II: Sums of Squares",
    "section": "\n11.2 Formative Test",
    "text": "11.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat does SSE stand for in linear regression?\n\nSum of Squared ErrorsSum of Squares for the ExpectationSum of Squared Explained VarianceSum of Standard Errors\n\nQuestion 2\nWhich of the following describes the concept of Ordinary Least Squares?\n\nMinimizing the sum of squared errorsMinimizing the total sum of squaresMinimizing the correlationMinimizing the regression sum of squares\n\nQuestion 3\nWhat does the Regression Sum of Squares (RSS) represent?\n\nThe total sum of squares minus the predicted valuesThe sum of squared prediction errorsThe explained variance by the regression lineThe reduction in sum of squares by using the regression line as a predictor, rather than the mean\n\nQuestion 4\nWhat is the purpose of the Total Sum of Squares (TSS) in linear regression?\n\nIt measures the standard deviation of the errors.It measures the residual error in the model.It measures the total variability of the dependent variable.It measures the explained variance of the predictor.\n\nQuestion 5\nWhat does the term ‘explained variance’ refer to in linear regression?\n\nThe portion of the regression sum of squares explained by the predictor.The portion of the standard deviation explained by the predictor.The portion of the total variance explained by the regression line.The portion of the total variance explained by the null model.\n\nQuestion 6\nHow is correlation different from regression?\n\nCorrelation measures the total variance, while regression measures the explained variance.Correlation focuses on categorical variables, while regression focuses on continuous variables.Correlation predicts one variable from another, while regression measures the strength of linear association.Correlation measures the strength of linear association, while regression predicts one variable from another.\n\nQuestion 7\nWhat is the range of the correlation coefficient (r) between two variables?\n\n[-1, 1][-∞, ∞][0, 1][-1, 0]\n\nQuestion 8\nWhich statistic is equivalent to the standardized regression coefficient in bivariate regression?\n\nSum of Squared Errors (SSE)Standard deviation of the predictorCoefficient of determination (R²)Correlation coefficient (r)\n\nQuestion 9\nWhat is the relationship between the Total Sum of Squares (TSS) and the Regression Sum of Squares (RSS)?\n\nTSS + RSS = SSESSR- SSE = SSTTSS + SSE = SSRTSS - SSE = RSS\n\nQuestion 10\nHow is the proportion of explained variance (R²) related to the correlation coefficient (r)?\n\nR² = 1 / rR² = 1 - r²R² = r²R² = 2 * r\n\n\n\n\nShow explanations\n\nQuestion 1\nSSE stands for Sum of Squared Errors, which represents the sum of the squared differences between actual and predicted values.\nQuestion 2\nOrdinary Least Squares aims to minimize the total prediction error, which is achieved by finding the regression line.\nQuestion 3\nRSS is the reduction in sum of squares that occurs when using the regression line to predict observations instead of just the mean.\nQuestion 4\nTSS measures the total variability of the dependent variable around its mean.\nQuestion 5\nExplained variance refers to the proportion of the total variance in the dependent variable that is explained by the predictor.\nQuestion 6\nCorrelation quantifies the strength and direction of the linear relationship between two variables, while regression aims to predict one variable from another using the regression line.\nQuestion 7\nThe correlation coefficient (r) ranges from -1 to 1, where -1 represents a perfect negative association, 0 represents no association, and 1 represents a perfect positive association.\nQuestion 8\nIn bivariate linear regression, the standardized regression coefficient is equivalent to the correlation coefficient (r) between the predictor and the outcome.\nQuestion 9\nThe relationship between TSS and RSS is given by TSS - RSS = SSE, indicating that the difference between TSS and RSS accounts for the error sum of squares.\nQuestion 10\nThe proportion of explained variance (R²) is equal to the square of the correlation coefficient (r), meaning R² = r².",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GLM-II: Sums of Squares</span>"
    ]
  },
  {
    "objectID": "glm2_sum_squares.html#in-spss",
    "href": "glm2_sum_squares.html#in-spss",
    "title": "11  GLM-II: Sums of Squares",
    "section": "\n11.3 In SPSS",
    "text": "11.3 In SPSS\n\n11.3.1 Correlation Analysis",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GLM-II: Sums of Squares</span>"
    ]
  },
  {
    "objectID": "glm2_sum_squares.html#tutorial",
    "href": "glm2_sum_squares.html#tutorial",
    "title": "11  GLM-II: Sums of Squares",
    "section": "\n11.4 Tutorial",
    "text": "11.4 Tutorial\n\n11.4.1 Bivariate Regression\nSocial science students were asked about their opinion towards Tilburg’s nightlife, number of Facebook friends, and some other characteristics. The data are in the SocScSurvey.sav file.\nDownload the file to your computer and open it in SPSS.\nSuppose researchers are interested in the relationship between personality and social media use. In particular, they want to know if extraversion explains the number of Facebook friends.\nWhat is the independent variable here? \nExtraversion\nFacebook friends\nRemember that the independent variable is the variable that predicts the other variable (which we call the dependent variable). The dependent variable is influenced by the independent variable (its value depends on the independent variable).\nRun a regression analysis in which you regress Facebook friends on extraversion (via analyze &gt; regression &gt; linear).\nKeep in mind that we “regress the dependent variable Y on the independent variables (X)”.\nConsult the output.\nWrite down the estimated unstandardized regression equation.\n\n\nAnswer\n\n\\(\\text{Friends}_i = -62.377 + 26.788*\\text{Extraversion}_i + e_i\\)\n\nWhich of the following statements is true?\n\nIf Extraversion increases with one unit, the number of facebook friends increases with 26.788 unitsIf Facebook friends increases with one unit, extraversion increases with 26.788 unitsIf Extraversion increases with 26.788 units, the number of facebook friends increases with 1 unit\n\nRemember that the general form of interpretation of the unstandardized effect is: “If X increases with 1 unit, Y increases/decreases with ‘unstandardized regression coefficient’ units”.\nWhat is the value of the standardized regression coefficient? \nYou can find the standardized regression coefficients in the column called ‘Standardized Coefficients Beta’.\nWhich of the following statements about the standardized regression coefficients is correct?\n\nIf extraversion increases with one unit, the number of facebook friends increases with 0.438 SDsIf extraversion increases with one SD, the number of facebook friends increases with 0.438 unitsIf extraversion increases with one SD, the number of facebook friends increases with 0.438 SDs\n\nRemember that standardized regression coefficients are interpreted in a similar way as unstandardized regression coefficients are, with the one difference being they are interpreted in terms of standard deviations.\nConsider the first person in the data file. The person had an extraversion score of 9.\nWhat is the predicted number of Facebook friends for this person? \nConsider the first person again.\nGiven the predicted number of Facebook friends for this person, what is the prediction error (rounded to the nearest integer)? \nPrediction error = yobserved - ypredicted\nConsider two people, one with an extraversion score of 10 and the other with an extraversion score of 15.\nWhat is the difference in the predicted number of Facebook friends between the two persons? (report the absolute value) \nConsult the output of the regression analysis.\nWhat percentage of the total variance in number of Facebook friends can be explained by extraversion? \nConsult the ANOVA table.\nThe table shows the results of an F-test.\nWhat is the default null hypothesis and alternative hypothesis for the reported test?\n\n\nAnswer\n\n\\(H_0: \\rho^2 = 0\\), \\(H_A: \\rho^2 &gt; 0\\)\n\nSuppose three researchers test the significance of the R-square.\nResearcher I tests at the 10% level, researcher II tests at the 5% level, and researcher III at the 1% level.\nWhich researcher will reject the null hypothesis? \nOnly researcher II\nAll three researchers\nOnly researcher III\nOnly researcher I\nWhen reporting the F-test for the model, you would report \\(R^2\\), the F-test statistic, its degrees of freedom, and the p-value.\nThe F-test has two distinct degrees of freedom. The first refers to the degrees of freedom for the regression equation, and the second to the degrees of freedom for the residuals. The degrees of freedom are given in brackets. For example, if regression has 2 degrees of freedom and the residuals 100, we write the F-value as F(2,100) = ….\nWhich of the following F value and corresponding degrees of freedom should be reported? \nF(1,132) = 0.000\nF(1,132) = 31.283\nF(1,133) = 31.283\n\n\n\n11.4.2 Correlation\nCorrelations and regression analyses can both be used to study the relationship between variables, but there is an important difference.\nDiscuss with your group mates what the similarities and differences between the two methods are.\n\n\nAnswer\n\nA correlation is a symmetric measure of association, meaning we are agnostic about which is the predictor and which is the outcome (or neither are predictor/outcome). The correlation between X and Y is the same as the one between Y and X.\nIn regression analysis, we do define an independent and dependent variable. The goal is to predict the outcome using the predictor. Most of the time, this implies an assumption of causality - but not necessarily.\nFor example, we can use regression to predict sales based on customer characteristics without assuming that those characteristics CAUSE sales. But if we want to cause an increase in sales, and we look at the regression coefficients to decide where to intervene - then it suddenly matters a lot whether the predictors are causes of sales or not.\nYou see this a lot with online marketing when you are receiving a lot of adds for a product that you recently bought. Their regression model knows that looking at the product page is a great predictor of intention to buy it - but they don’t know that the reason you were looking at that page is because you were already buying it.\n\nNow, let’s have a look at the correlation between these two variables.\nAnalyze &gt; correlate &gt; bivariate.\nChoose as variables: Facebook Friends and Extraversion, and click OK.\nWhat is the correlation between Extraversion and number of Facebook friends? \nSuppose three researchers test the significance of the correlation between Extraversion and Facebook friends. Researcher I tests at the 10% level, researcher II tests at the 5% level, and researcher III at the 1% level.\nWhich researcher will reject the null hypothesis? \nAll three researchers\nOnly researcher II\nOnly researcher III\nOnly researcher I\nWhich of the following interpretations is true?\n\nIt would be very unlikely to observe a sample correlation of .44 by chance if the population correlation would be zero.We don't have convincing evidence that Facebook friends and extraversion are associated in the population.We have convincing evidence that Facebook friends and extraversion are associated in the population.\n\nCompare the correlation coefficient to the standardized regression coefficient from the bivariate regression you conducted previously.\nThen, compare it to the value labeled “R” in the “Model Summary” table from the regression.\nSquare the correlation, and compare it to the value labeled “R Square”.\nWhat do you observe?\n\n\nAnswer\n\nIf you did everything correctly, you should observe that the bivariate correlation is identical to the standardized regression coefficient. This is only the case with bivariate regression.\nFurthermore, the bivariate correlation should be identical to the R reported in the Model Summary table, because they are both just the correlation coefficient. R squared is the squared correlation coefficient, and we interpret it as the “proportion of variance in the outcome explained by the predictor”. Only in bivariate regression is this identical to the squared correlation coefficient.\n\n\n11.4.3 R squared\nThe R squared expresses how well the predictors explain variance in the outcome of a regression. In the next few steps we will look in more detail at this concept.\nConsider the results of the regression model again.\nWrite down the (unstandardized) regression equation based on your previous results, and use the raw data in the Data View to answer the following question.\n\n\nAnswer\n\n\\(\\text{pressure}_i = 37.863 -.320 * \\text{variety}_i + e_i\\)\n\nWhat is the predicted value (Y’) for emotional pressure at work for the first person in the data file (i.e., the person with respondent number 1)? \nWhat is the prediction error (a.k.a. the residual) for the first person? \nRemember Residual = Yobserved - Ypredicted\nWe’ve just computed the predicted value and error by hand. It would be very tedious if we would have to do that for all respondents. Fortunately, SPSS offers the option to compute predicted values and errors for all cases for us!\nNavigate to Analyze &gt; Regression &gt; Linear\nClick on the ‘Save’ button. SPSS opens a new window.\nAsk for the Unstandardized predicted values and the unstandardized residuals. Paste and run the syntax.\nLet’s inspect the Data View in SPSS again and verify that SPSS added two columns in the data file. One column is labeled PRE_1 and the other RES_1. These columns show the predicted values and residuals for each person, respectively.\nYou may verify this for the first person (i.e., the values should be the same as you computed in the previous steps).\nNow we will look at the variance of the observed values of Pleasure at work, the variance of the predicted values of pleasure at work, and variance of the residuals.\nCompute the variances of Emotional pressure, as well as for the predicted values of Emotional pressure, and for the residuals.\nNavigate to Analyze &gt; Descriptive statistics &gt; Descriptives Select scemoti, PRE_1, and RES_1. Click on ‘options’ and ask for the Variance. Paste and run the syntax.\nHow large is the variance of the observed scores for Emotional pressure? \nHow large is the variance of predicted values of Work pleasure? \nHow large is the variance of the residuals? \nIn the previous questions, we looked at three variance components.\nDiscuss with your group what the three variances represent.\n\n\nAnswer\n\nThe variance in observed values of Emotional pressure is the total variance in Y (i.e., the dependent variable). The variance in the predicted values of Emotional pressure reflects “differences in emotional pressure that can be explained because some persons have a job with a lot of variety and some have a monotonous job”. This variance component is also known as the explained variance. The variance of the residuals, also known as the residual variance, represents differences in emotional pressure that cannot be attributed to differences in variety at work. Hence, the residual describes differences that are unrelated to variety at work.\n\nIn the previous step, we looked at the variances itself, but the numbers are not very informative. A more convenient way to look at the explained variance is proportion wise.\nSo, let’s use the variances we just generated to calculate the proportion of variance in Emotional pressure that can be explained by Variety at work.\nWhat percentage of the total variance in emotional pressure can be explained by variety at work? \nConsult the output of the regression analysis again, particularly the table Model Summary.\nVerify that the R-square that is reported in the table is the same as the proportion of explained variance that you have calculated yourself.\nFinally, independently go through all the steps of a simple regression analysis using the data file Work.sav.\nYour theory suggests that independence at work predicts emotional pressure.\n\nConstruct an appropriate research question and hypotheses.\nConduct the analysis\nDescribe the relationship (i.e., regression coefficient)?\nDiscuss the effect size in terms of R2.\nPerform a significance test and report your results\n\nFinally, compare the standardized regression coefficient to the R coefficient in the Model Summary table, and optionally to a correlation computed via the Correlation interface. Verify that these are all identical.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GLM-II: Sums of Squares</span>"
    ]
  },
  {
    "objectID": "glm_assumptions.html",
    "href": "glm_assumptions.html",
    "title": "\n12  Assumptions\n",
    "section": "",
    "text": "12.0.1 Assumptions for Linear Regression\nEvery time we use a statistical model to describe data, we make certain simplifying assumptions. If these assumptions are met, the model is a good representation of the data (descriptive statistics), and we can make valid inferences about the population based on the model’s parameters (inferential statistics). However, when these assumptions are violated, the model is a bad descriptor of the data, and inferences based on the model can be misleading or difficult to interpret.\nTo de-mystify assumptions, let’s examine one of the simplest statistical models possible: the normal distribution. The normal distribution is a statistical model to describe the distribution of scores on a variable (or: in the population), and its two parameters are the mean and standard deviation. If I draw a random sample of 1000 participants from the population of the Netherlands, their observed heights might be distributed as in the histogram below. I could use the normal distribution as a model for these data, and it would do a pretty good job (see the red normal distribution). In this case, my assumption is that height is normally distributed around a mean \\(\\mu\\) and with standard deviation \\(\\sigma\\), or:\n\\[\n\\text{Height} \\sim N(\\mu, \\sigma)\n\\]\nIf this assumption holds, the mean and standard deviations will be pretty good descriptive statistics of the distribution of data in the sample. If I assume that height is also normally distributed in the population, and that my sample is representative - then my sample statistics are also pretty good estimators for the population parameters.\nNow imagine that I draw a convenience sample of 200 members of my local basketball association (figure below). Do you think I can assume that their heights will be normally distributed? Why (not)? Do you think these individuals will be representative of the Dutch population? Will they be representative of the population of Dutch basketball players? If the assumption that these scores are normally distributed is violated, then the mean and standard deviation of the normal distribution will be poor descriptive statistics. Moreover, these sample statistics will be poor estimators of the population parameters.\nThe same principles apply to more complex models than the normal distribution - for example, linear regression. In fact, linear regression can be written as a normal distribution whose mean depends on the value of a predictor variable. If this equation says that height is normally distributed:\n\\[\n\\text{Height} \\sim N(\\mu, \\sigma)\n\\]\nThen this equation says that height is normally distributed with a mean value that depends on age:\n\\[\n\\text{Height}_i \\sim N(\\alpha + \\beta*\\text{Age}_i, \\sigma)\n\\] Notice that the overall (population) mean of height \\(\\mu\\) is now replaced with a linear formula with population intercept \\(\\alpha\\) and effect of Age \\(\\beta\\). Another way to rewrite this formula without changing the meaning is:\n\\[\\begin{align}\n\\text{Height}_i &= \\alpha + \\beta * \\text{Age}_i + \\epsilon_i \\\\\n\\epsilon_i &\\sim N(0, \\sigma)\n\\end{align}\\]\nThis is the familiar notation of a regression equation. There are two points here: one, regression can be written as an extension of the normal distribution by plugging a linear formula in the place of the distribution mean. This means that regression inherits the assumptions of the normal distribution (e.g., no outliers), and gains a few more because of the added linear formula. Two, all of the assumptions are right there, in the formula itself: the fact that we specify height as a linear function of age means that we assume linearity. The fact that we use a little subscript \\(_i\\) for height and age means that we assume independent observations from different individuals for these variables. The fact that we have one normal distribution for the error term \\(\\epsilon_i\\) means that we do not expect the error distribution to vary at different values of the predictor, in other words, we expect homoscedasticity.\nBelow, we get deeper into the assumptions of linear regression, explains why each one matters, and shows how to check whether they are likely to hold in your data.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assumptions</span>"
    ]
  },
  {
    "objectID": "glm_assumptions.html#formative-test",
    "href": "glm_assumptions.html#formative-test",
    "title": "\n12  Assumptions\n",
    "section": "\n12.1 Formative Test",
    "text": "12.1 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhich statement best captures the linearity assumption in OLS regression?\n\nThe residuals must have constant varianceThe predictors must be normally distributedThe expected change in the outcome is proportional to changes in each predictor across its rangeThe outcome must be measured without error\n\nQuestion 2\nHomoscedasticity means:\n\nResiduals have constant variance across all levels of the predicted valuesPredictors are uncorrelated with each otherResiduals are centered at zeroPredictors are measured on a continuous scale\n\nQuestion 3\nA residuals versus predicted plot shows a clear funnel shape. Which assumption is most likely violated?\n\nIndependence of observationsNormality of predictorsHomoscedasticityLinearity\n\nQuestion 4\nWhy does normality of residuals matter most for small samples in OLS?\n\nIt underpins the accuracy of t tests and confidence intervals for coefficientsIt guarantees unbiased slope estimatesIt eliminates the need to check other assumptionsIt ensures predictors are measured reliably\n\nQuestion 5\nYour data consist of students nested within classrooms but you fit a single level OLS model that treats all rows as independent. Which assumption is threatened?\n\nIndependence of observationsNo outliersNormality of predictorsCorrect scale of measurement for the outcome\n\nQuestion 6\nTwo predictors are highly correlated. What is the most common consequence for the regression coefficients?\n\nThey become unbiased and more preciseThey remain unchanged but model fit worsensThey become systematically too largeThey become unstable with inflated standard errors and may change sign with small data changes\n\nQuestion 7\nWhich outcome variable violates the correct scale of measurement assumption for standard OLS regression?\n\nHeight in centimetersA binary pass or fail indicator coded 0 and 1Test score from 0 to 100Annual income in dollars\n\nQuestion 8\nWhat is the typical effect of measurement error in a predictor on its estimated slope in OLS?\n\nBias toward zeroNo bias but larger p valuesNo effect if the outcome is normalBias away from zero\n\nQuestion 9\nYou see a systematic curve in the residuals versus predicted plot. Which assumption is most suspect?\n\nLinearityIndependenceHomoscedasticityNo outliers\n\nQuestion 10\nWhich statement about outliers and influence in OLS is correct?\n\nOnly points with extreme outcome values can be influentialAny point far from the regression line is necessarily highly influentialA point can be influential if it has unusual predictor values and substantially changes the fitted lineInfluential points affect p values but never change coefficient signs\n\n\n\n\nShow explanations\n\nQuestion 1\nLinearity is about the average of Y at each X forming a straight line. The other options describe different assumptions.\nQuestion 2\nHomoscedasticity is constant spread of residuals across the range of fitted values.\nQuestion 3\nA funnel pattern indicates changing residual variance across fitted values.\nQuestion 4\nWith small samples inference for slopes uses normality. In large samples asymptotics help.\nQuestion 5\nClustering creates correlated errors. Rows are not independent.\nQuestion 6\nHigh correlation among predictors creates multicollinearity and unstable estimates.\nQuestion 7\nOLS assumes a continuous outcome. A binary outcome calls for a different model.\nQuestion 8\nClassical measurement error in X attenuates the slope.\nQuestion 9\nA patterned residual plot suggests a wrong functional form. The relation is not linear.\nQuestion 10\nInfluence depends on leverage and residual. Such points can change slopes and even signs.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assumptions</span>"
    ]
  },
  {
    "objectID": "glm_assumptions.html#tutorial",
    "href": "glm_assumptions.html#tutorial",
    "title": "\n12  Assumptions\n",
    "section": "\n12.2 Tutorial",
    "text": "12.2 Tutorial\n\n12.2.1 Before you start\n\nIn each exercise, fit an OLS model with y as the outcome and the listed predictors.\n\n12.2.2 Assignment 1\nFile: reg_assump_check.sav\nVariables: y, x1, x2\nSteps (SPSS GUI):\n\n\nCheck linearity (for each predictor)\n\nGraphs &gt; Legacy Dialogs &gt; Scatter/Dot &gt; Simple Scatter\n\nY axis: y; X axis: x1. Create plot.\n\nRepeat for x2.\n\n\n\nIn the Chart Editor: Element &gt; Fit Line at Total.\n\n\n\nAnalyze &gt; Regression &gt; Linear\n\nDependent: y\n\nIndependents: x1 x2\n\nStatistics: Estimates, Confidence intervals, Collinearity diagnostics\n\nPlots: ZPRED on X, ZRESID on Y\n\nSave: Standardized residuals (ZRESID), Predicted values (ZPRED)\n\n\nGraphs &gt; Legacy Dialogs &gt; Histogram\n\nVariable: ZRESID; check “Display normal curve”\n\n\nGraphs &gt; Legacy Dialogs &gt; Q-Q\n\nVariable: ZRESID\n\n\n\nQuestions:\n\nIs there a linear association among these variables?\nIs the measurement scale of these variables appropriate for regression analysis?\nAre residuals normally distributed?\n\nIs homoscedasticity supported?\n\nAre there any multicollinearity concerns?\n\n12.2.3 Assignment 2 — Linearity check\nFile: reg_linearity_check.sav\nVariables: y, x1\nSteps:\n\nGraphs &gt; Legacy Dialogs &gt; Scatter/Dot &gt; Simple Scatter\n\nY axis: y; X axis: x1\n\nIn Chart Editor, add a straight fit line (Fit Line at Total).\n\n\nAnalyze &gt; Regression &gt; Linear\n\ny on x1; request the same plots and saves as in Assignment 1.\n\n\n\nQuestions:\n\nDoes the y vs x1 scatterplot suggest a straight-line relation?\n\nDo residual plots show a pattern (e.g., systematic bends) inconsistent with linearity?\n\nBased on the diagnostics, is a linear specification for x1 adequate in this dataset?\n\n12.2.4 Assignment 3 — Homoscedasticity\nFile: reg_homoscedasticity_check.sav\nVariables: y, x1\nSteps: 1) Fit OLS as in Assignment 1 and save ZRESID and ZPRED.\n2) Graphs &gt; Legacy Dialogs &gt; Scatter/Dot &gt; Simple Scatter\n- X axis: ZPRED; Y axis: ZRESID.\nQuestions:\n\nDo residuals show roughly constant spread across predicted values, or a cone/funnel?\n\n12.2.5 Assignment 4 — Normality of residuals\nFile: reg_normality_check.sav\nVariables: y, x1\nSteps:\n\nFit the regression and save residuals\n\nAnalyze &gt; Regression &gt; Linear\n\nDependent: y\nIndependent(s): your predictor(s)\nSave &gt; Standardized residuals\nPlots &gt; Tick Histogram\n\n\n\n\nTo produce Q-Q plot\n\nAnalyze &gt; Descriptive Statistics &gt; Explore\nAdd the saved standardized residuals to the dependent list\nPlots &gt; Check Normality plots with tests\n\n\n\nQuestions:\n\nIs the residual distribution approximately symmetric and bell-shaped?\n\nDo Q-Q points track the diagonal?\n\n12.2.6 Assignment 5 — Outliers\nFile: reg_outliers.sav\nVariables: y, x1\nSteps:\n\nAnalyze &gt; Regression &gt; Linear\n\nDependent: y\n\nIndependent: x1\n\nStatistics: Estimates, Casewise diagnostics (e.g., standardized residuals &gt; 3), Collinearity diagnostics (optional here)\n\nSave: Cook’s distance, and Leverage (Hat)\n\n\nGraphs &gt; Legacy Dialogs &gt; Boxplot\n\nSummaries for separate variables: y, x1\n\n\n\nGraphs &gt; Legacy Dialogs &gt; Scatter/Dot &gt; Simple Scatter\n\nY axis: y; X axis: x1\n\n\n\n\nQuestions:\n\nAre any cases flagged in Casewise diagnostics (e.g., |Std. Residual| &gt; 3)?\n\nDo any observations show high leverage* or large Cook’s distance relative to others?\n\nBased on these diagnostics, could a single case plausibly dominate the fitted line? Identify the case ID if so.\n\n12.2.7 Exercise 6 — Multicollinearity\nFile: reg_multicollinearity.sav\nVariables: y, x1, x2, x3\nSteps: 1) Analyze &gt; Correlate &gt; Bivariate\n- Inspect correlations among x1, x2, x3.\n2) Analyze &gt; Regression &gt; Linear\n- y on x1 x2 x3\n- Statistics: Collinearity diagnostics, Estimates.\nQuestions:\n\nAre any predictor pairs highly correlated in the correlation matrix?\n\nWhat are the VIF and Tolerance values for each predictor?\n\nDo the signs and standardized Betas align with the simple correlations, or do you see suppression patterns?\n\nWhat to look for:\n\nVIF substantially above 5 (or Tolerance, which is \\(\\frac{1}{VIF}\\), below .20) suggests collinearity.\nLarge divergence between simple r and Beta can signal overlap among predictors.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assumptions</span>"
    ]
  },
  {
    "objectID": "glm_assumptions.html#assignment-7-putting-it-together-choose-two-datasets",
    "href": "glm_assumptions.html#assignment-7-putting-it-together-choose-two-datasets",
    "title": "\n12  Assumptions\n",
    "section": "\n12.3 Assignment 7 — Putting it together (choose two datasets)",
    "text": "12.3 Assignment 7 — Putting it together (choose two datasets)\nFiles: pick any two from the set\nTask:\n\nFor each dataset, run the standard diagnostic workflow from Assignment 1. - Summarize, in a short paragraph per dataset, which assumptions are reasonably met and which are doubtful, citing the specific plot or statistic you used.\n\nReminder:\n\nFocus on diagnosis only. Do not apply remedies or re-specify models here.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assumptions</span>"
    ]
  },
  {
    "objectID": "glm3binary.html",
    "href": "glm3binary.html",
    "title": "13  GLM-III: Binary Predictors",
    "section": "",
    "text": "13.1 Lecture\nWe can examine group differences in a continuous outcome variable using bivariate regression. To do this, group membership must be represented as a binary variable (e.g., gender or ethnicity). To ensure meaningful results, we use dummy coding to represent the binary variable. Dummy coding assigns the value 0 to one category, which serves as the reference category, and the value 1 to the other category. When we include this dummy variable as the predictor in a bivariate linear regression analysis, it will estimate the mean value of the reference category and test the difference between the means of the two categories.\nRegression with a binary predictor is completely equivalent to the independent samples t-test. The independent samples t-test is also used to compare the means of two independent groups. In regression, we estimate the slope (b) for the binary predictor, which represents the difference between the means of the two groups. This t-test of the slope in regression is the same as an independent samples t-test.\nBoth regression with a binary predictor and the independent samples t-test rely on certain assumptions. These include the linearity of the relationship between the binary predictor and the outcome variable, the normality of residuals (the outcome variable should be normally distributed within each group), homoscedasticity (equal variances in both groups), and independence of observations. To check for equal variances, we can use Levene’s test - but keep in mind that “assumption checks” are questionable. If you assume equal variances, report the normal t-test; if you do not assume equal variances, you can report a corrected t-test that allows for different variances. Both are included in SPSS output by default.\nTo determine the practical significance of a mean difference, we can calculate an effect size measure. Cohen’s d is a commonly used effect size for mean differences. It standardizes the difference between the two group means by the pooled standard deviation. A larger Cohen’s d indicates a greater magnitude of difference between the groups. As a rule of thumb, a small effect size is typically considered around d ≈ 0.2, a medium effect size around d ≈ 0.5, and a large effect size around d ≈ 0.8.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GLM-III: Binary Predictors</span>"
    ]
  },
  {
    "objectID": "glm3binary.html#formative-test",
    "href": "glm3binary.html#formative-test",
    "title": "13  GLM-III: Binary Predictors",
    "section": "\n13.2 Formative Test",
    "text": "13.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat is the purpose of dummy coding in regression with binary predictors?\n\nTo simplify the model by removing categorical predictors.To estimate the mean of the reference category and test the difference between categories.To increase the number of predictors in the model.To convert binary variables into continuous variables.\n\nQuestion 2\nWhat does the slope coefficient (b) represent in regression with a binary predictor?\n\nThe ratio of the two categories' means.The difference in means between the two categories.The mean of the second category.The intercept of the reference category.\n\nQuestion 3\nWhich assumption is not relevant for the independent samples t-test and bivariate linear regression with only a binary predictor?\n\nHomoscedasticityNormality of residualsIndependence of observationsLinearity of relationship between X and Y\n\nQuestion 4\nWhat does the Levene’s test check in the context of the independent samples t-test?\n\nNormality of residuals.Linearity of relationship between X and Y.Equality of variances in both groups.Normality of variances in both groups.\n\nQuestion 5\nHow is the independent samples t-test related to the t-test of the slope in regression with a binary predictor?\n\nThe t-test of the slope is a subset of the independent samples t-test.They are equivalent.They are not related.The independent samples t-test is a subset of the t-test of the slope.\n\nQuestion 6\nWhat does the p-value in the context of the independent samples t-test indicate?\n\nThe probability of observing a group difference at least as extreme as the one observed, if the null hypothesis is true.The probability that the null hypothesis is rejected.The probability that the null hypothesis is true.The probability of observing a statistically significant result.\n\nQuestion 7\nWhat does Cohen’s d represent?\n\nAn effect size for the difference between standardized group means.The difference between the two categories' means.A measure of explained variance for mean differences.An effect size for the mean difference, expressed in number of standard deviations.\n\nQuestion 8\nWhat is the recommended approach when assumption checks for homoscedasticity are significant?\n\nUse a robust t-test.It depends - in confirmatory analyses, you may switch to a robust test; in exploratory analyses, you would report the violation and proceed as planned.It depends - in exploratory analyses, you may switch to a robust test; in confirmatory analyses, you would report the violation and proceed as planned.Exclude outliers to ensure homoscedasticity.\n\nQuestion 9\nThe observed mean difference between two groups is 2.50, and Cohen’s D is 1.25. What is the pooled standard deviation?\n\n2.502Can't say based on this information.1.25\n\nQuestion 10\nA researcher accidentally coded a dummy variable as 0 and 2, instead of 0 and 1. The regression equation is Y = 5.66 + 3*D. What is the mean value of the group coded 2?\n\n11.6638.667.16\n\n\n\n\nShow explanations\n\nQuestion 1\nDummy coding allows regression to include binary predictors by assigning numerical values to each category, estimating the mean of the reference category, and testing the difference between categories.\nQuestion 2\nThe slope coefficient (b) in regression with a binary predictor represents the difference in means between the two categories, indicating how much the dependent variable changes when the binary predictor changes from 0 to 1.\nQuestion 3\nThe assumption of linearity is not relevant, because the difference between two binary values of the predictor is linear by definition.\nQuestion 4\nLevene’s test checks the assumption of equality of variances in both groups for the independent samples t-test.\nQuestion 5\nThe independent samples t-test and the t-test of the slope in regression with a binary predictor are equivalent tests that compare means between two independent groups.\nQuestion 6\nThe p-value indicates the probability of observing a group difference at least as extreme as the one observed, assuming that the null hypothesis is true.\nQuestion 7\nCohen’s d is an effect size that standardizes the difference between group means by the (pooled) standard deviation, making it interpretable on a meaningful scale.\nQuestion 8\nAssumption checks can alert you that important assumptions of the test are violated, but you should not blindly adapt analyses based on their results either - particularly in confirmatory research. You can always perform a sensitivity analysis in which you report both the planned analysis and the robust version.\nQuestion 9\nCohen’s D = mean difference/pooled sd.\nQuestion 10\nThe slope tells you how much the predicted value goes up for a 1-unit increase in the predictor D. Since D is coded 0 and 2, a 1-unit increase only gets you halfway!",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GLM-III: Binary Predictors</span>"
    ]
  },
  {
    "objectID": "glm3binary.html#in-spss",
    "href": "glm3binary.html#in-spss",
    "title": "13  GLM-III: Binary Predictors",
    "section": "\n13.3 In SPSS",
    "text": "13.3 In SPSS\n\n13.3.1 Independent Samples t-test\nAs a t-test and as regression with a dummy predictor:",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GLM-III: Binary Predictors</span>"
    ]
  },
  {
    "objectID": "glm3binary.html#tutorial",
    "href": "glm3binary.html#tutorial",
    "title": "13  GLM-III: Binary Predictors",
    "section": "\n13.4 Tutorial",
    "text": "13.4 Tutorial\n\n13.4.1 Independent Samples T-Test\nIn this assignment we will use the data file 5groups.sav. Download the file and open it in SPSS.\nThis time, we will compare the means of the variable y of two specific groups: group 1 and group 4. To test the difference between two sample means, we will use the t-test for independent samples.\nWhat is the null hypothesis of this test? And what is the alternative hypothesis?\n\n\nAnswer\n\n\\(H_0: \\mu_1=\\mu_4\\), against \\(H_1: \\mu_1\\neq \\mu_4\\)\n\nCreate the necessary syntax for the t-test that compares the means of group 1 and group 4.\nYou can find the dialog for the two-sample t-test under Analyze &gt; Compare Means &gt; Independent Samples T Test\nIn the SPSS dialog you have to specify which two groups you want to compare. In our case, it’s group 1 and group 4. After placing the variable in the box named “Grouping Variable”, click the button named “Define Groups” to define the groups.\nCompare your syntax to the correct syntax:\n\n\nAnswer\n\nT-TEST GROUPS=group(1 4) /MISSING=ANALYSIS /VARIABLES=y /CRITERIA=CI(.95).\n\nOne of the assumptions of the independent samples t-test is homoscedasticity (equal variances for all levels of the predictor). We can compare the sizes of the variances of the two groups with a simple F-test, which we call Levene’s test.\nHave a look at Levene’s test and try to interpret it. Discuss with your group what null-hypothesis is being tested here.\nWhat is the p-value of the Levene’s test? \nWhat do you conclude from this? What’s the practical use of the outcome of this test?\n\n\nExplanation\n\nLevene’s test is not significant. Remember that the null hypothesis of Levene’s test is that the population variances of the group are equal. As the p-value is not significant, we cannot reject the null hypothesis. Consequently, there is no evidence that the population variances of two groups are unequal. Thus, there is no reason to question the assumption.\n\nNow you will have to decide on the outcome of the actual t-test. SPSS reports two versions: one that assumes equal variances (top row) and one that relaxes this assumption (bottom row).\nYou should pick one of these. In principle, you should decide which one you will use before seeing the results - although if there is clear evidence of violation of assumptions, you might want to discuss in your report whether the results change if you use the robust version (bottom row).\nFor now remember: we assume equal variances.\nWhat is the two-sided p-value? \nDo you reject the null hypothesis of this t-test at alpha 0.05? \nYes\nNo\n\n13.4.2 Regression with dummies\nWe will now perform the exact same analysis, but with regression and dummies.\nTo test the difference between group 1 and group 4, we first create a dummy variable to distinguish these two groups. Use group 1 as reference category. You can use either Transform -&gt; Recode into different variables, or syntax:\nRECODE group (1=0) (4=1) INTO dgroup4.\nEXECUTE.\nNote that all other groups are coded as missing on this variable, which is exactly what we want!\nWe will use regression to perform our t-test. The hypothesis is the same as in the previous assignment, but you could also rewrite it in terms of regression coefficient(s). What is the null hypothesis of this test in terms of regression coefficient(s)? And what is the alternative hypothesis?\n\n\nAnswer\n\n\\(H_0: \\beta_{group1 vs group2}=0\\) which is the same as \\(H_0: \\mu_{group1} = \\mu_{group2}\\), versus \\(H_1: \\beta_{group1 vs group2} \\neq 0\\) which is the same as \\(H_0: \\mu_{group1} \\ne \\mu_{group2}\\)\n\nCreate the necessary syntax for a regression with the dummy variable that compares the means of group 1 and group 4.\nYou can find the dialog under Analyze &gt; Regression &gt; Linear\nIn the SPSS dialog you have to specify the Dependent and Independent variable. In our case, the independent variable is the dummy we created.\nCompare your syntax to the correct syntax:\nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT y\n  /METHOD=ENTER dgroup4.\nNote that, unlike the t-test interface, the regression interface does not provide a Levene’s test. This is one reason you might want to use the t-test interface. The regression interface provides a more generic way to test the assumption of homoscedasticity: a residual plot.\nGo back through the regression interface, but this time click the Plots button and plot the predicted value (X = ZPRED) against the residual value (Y = ZRESID).\nYour syntax will now say:\nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT y\n  /METHOD=ENTER dgroup4\n  /SCATTERPLOT=(*ZRESID ,*ZPRED)\nIf the assumption of homoscedasticity is met, we should see that the dots in this plot are equally distributed around the zero line for all values on the X-axis. In this case, we see much narrower spread on the right side than on the left side.\nWhat can you conclude from this, and does it match your conclusion from Levene’s test?\nNow you will have to decide on the outcome of the actual t-test.\nRemember that the t-test of the dummy variable should be the same as the t-test we conducted before. Verify that this is true.\nWhat is the two-sided p-value? \nWe see one more t-test: for the “(Constant)” or intercept. How do we interpret this?\n\nThe mean difference between both Groups is 7, and this differs significantly from zero.The mean value in Group 4 is 7, and this differs significantly from zero.The mean value across both Groups is 7, and this differs significantly from zero.The mean value in Group 1 is 7, and this differs significantly from zero.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GLM-III: Binary Predictors</span>"
    ]
  },
  {
    "objectID": "glm4anova.html",
    "href": "glm4anova.html",
    "title": "14  GLM-IV: ANOVA",
    "section": "",
    "text": "14.1 Lecture\nWe can also use regression analysis to examine mean differences between the categories of a nominal or ordinal predictor with more than two categories. Suppose we have a categorical predictor, such as socioeconomic status (SES), with three categories: Low, Medium, and High. We want to predict fathers’ involvement in child rearing based on these SES categories. We can use regression analysis to model this relationship by using dummy variables.\nWe previously discussed how bivariate linear regression allows us to model the effect of a binary categorical predictor by using dummy coding. We code one variable as the reference group (giving it the value 0), and estimate the mean difference between the reference group and the other category.\nWhen we have two or more categories, we can use the same principle - but we need to expand the model. For our example with SES, we can select one reference category (say, High SES), and we would create two dummy variables to estimate the mean differences between the reference category and the Medium and Low SES categories. Our regression model then includes both dummy variables as predictors, along with the intercept term.\nThis regression model is completely equivalent to one-way ANOVA (Analysis of Variance). Think of ANOVA as a different interface to the same analysis, which presents the results in a slightly different way that is more common in some subfields of social science.\nWhen we perform an ANOVA, we conduct an omnibus test of differences between group means. The default null hypothesis is that all group means are equal, and the alternative hypothesis suggests that at least two group means differ. We test this hypothesis with an F-test for the overall significance of the model. You are already familiar with this test from the lecture on sums of square.\nOne way to think about the F-test in the context of ANOVA is that it compares the size of the variance (differences) in group means, relative to the error variance in the data. If the differences between group means are large relative to the spread of the data, we observe a significant test. In ANOVA, the regression sum of squares is also called the “between-group sum of squares”, and the error sum of squares is also called “within-group sum of squares”.\nWhen interpreting the results of ANOVA, it’s common to use eta squared \\(\\eta^2\\) as an effect size. It is simply another name for the familiar \\(R^2\\). It reflects the proportion of variance in the outcome variable that can be explained by the categorical predictor.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM-IV: ANOVA</span>"
    ]
  },
  {
    "objectID": "glm4anova.html#formative-test",
    "href": "glm4anova.html#formative-test",
    "title": "14  GLM-IV: ANOVA",
    "section": "\n14.2 Formative Test",
    "text": "14.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nHow can you model a categorical predictor with more than 2 categories in regression?\n\nConvert the categorical predictor into a binary predictor.Use a single continuous variable to represent all categories.Create dummy variables for each category and include them in the regression equation.Exclude the categorical predictor from the regression model.\n\nQuestion 2\nWhat does the intercept term represent in a regression model with dummy variables?\n\nThe standard deviation of the predictor variable.The difference between the means of all categories.The mean difference between all categories.The mean value of the reference category.\n\nQuestion 3\nHow many dummy variables are created for a categorical predictor with three categories?\n\nThree dummy variables are created, each representing membership in a category.Two dummy variables are created, each representing membership in one of the non-reference categories.One dummy variable is created, representing membership in the reference category.For three categories, you don't need dummy variables.\n\nQuestion 4\nWhat does the F-value represent in ANOVA?\n\nAn overall test of the difference between group means.The proportion of variance explained by the predictor variable.How large the variance between group means is relative to variance within groups.How large the difference between the group means is, relative to the error variance.\n\nQuestion 5\nWhat is the purpose of follow-up analyses after a significant ANOVA?\n\nTo understand which specific group means differ significantly from each other.To calculate the effect size.To understand which groups are significant.To adjust the p-value for multiple comparisons.\n\nQuestion 6\nHow are the degrees of freedom calculated for the F-distribution in ANOVA?\n\nNumerator df: Total number of observations - Number of groups; Denominator df: Number of groups - 1Numerator df: Number of groups - 1; Denominator df: Total number of observations - Number of groupsNumerator df: Total number of observations - 1; Denominator df: Number of groupsNumerator df: Number of groups - 1; Denominator df: Total number of observations\n\nQuestion 7\nWhat is the correct interpretation of a small eta squared (η²) value in ANOVA?\n\nA small proportion of the total variance is due to error.A small proportion of the total variance is due to individual differences.A small proportion of the error variance is explained by the group differences.A small proportion of the total variance is explained by the group differences.\n\nQuestion 8\nGiven the regression equation Y = 20 + 5D1 - 3D2 for an ANOVA model with dummy coded predictors, what is the predicted value of Y when D1 = 2 and D2 = 1?\n\nY = 20 + 51 + 30 = 25Y = 20 - 50 - 31 = 17This cannot happen as the dummy variables are orthogonal.Y = 20 + 51 - 31 = 22\n\nQuestion 9\nIn an ANOVA model with 4 groups and 300 observations, calculate the degrees of freedom for the numerator and denominator for the F-test.\n\nNumerator df: 300 - 1 = 299; Denominator df: , 4 - 1 = 3Numerator df: 300 - 4 = 296; Denominator df: , 4 - 1 = 3Numerator df: 4 - 1 = 3; Denominator df: 300 - 4 = 296Numerator df: 4 - 1 = 3; Denominator df: 300 - 1 = 299\n\nQuestion 10\nIn an ANOVA model, the variation of individual observations with respect to the grand mean (SST) is 1200, and the variation of individuals with respect to group means (SSW) is 800. Calculate the proportion of variance explained by the group means (η²).\n\nη² = SSB/SSW = (1200 - 800)/800 = 0.5η² = SSB/SST = (1200 + 800)/1200 = 2.0η² = SSB/SST = (SST - SSW)/SST = (1200 - 800)/1200 = 0.333η² = SSW/SST = 800/1200 = 0.667\n\n\n\n\nShow explanations\n\nQuestion 1\nTo model a categorical predictor with more than 2 categories, you create dummy variables for each category and include them as predictors in the regression equation.\nQuestion 2\nThe intercept term in a regression model with dummy variables represents the mean value of the reference category.\nQuestion 3\nFor a categorical predictor with three categories, two dummy variables are created, each representing membership in one of the non-reference categories.\nQuestion 4\nThe F-test in ANOVA measures how large the variance in group means is relative to the error variance, helping us determine if there are significant differences between group means.\nQuestion 5\nFollow-up analyses are conducted after a significant ANOVA to understand which specific groups differ significantly from each other, as the omnibus ANOVA only tells us there are differences among groups but not which ones.\nQuestion 6\nThe degrees of freedom for the F-distribution in ANOVA are calculated as follows: Numerator df = Number of groups - 1; Denominator df = Total number of observations - Number of groups.\nQuestion 7\nA small eta squared (η²) value in ANOVA indicates that a small proportion of the total variance is explained by the group differences, suggesting weaker group effects.\nQuestion 8\nIt is not possible for an observation to score 1 on two dummies.\nQuestion 9\nThe degrees of freedom for the F-test are calculated as follows: Numerator df = Number of groups - 1; Denominator df = Total number of observations - Number of groups.\nQuestion 10\nThe proportion of variance explained by the group means (η²) is calculated as η² = SSB/SST = (SST - SSW)/SST = (1200 - 800)/1200 = 0.333.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM-IV: ANOVA</span>"
    ]
  },
  {
    "objectID": "glm4anova.html#in-spss",
    "href": "glm4anova.html#in-spss",
    "title": "14  GLM-IV: ANOVA",
    "section": "\n14.3 In SPSS",
    "text": "14.3 In SPSS\n\n14.3.1 ANOVA\nUsing the ANOVA interface and the regression interface:",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM-IV: ANOVA</span>"
    ]
  },
  {
    "objectID": "glm4anova.html#tutorial",
    "href": "glm4anova.html#tutorial",
    "title": "14  GLM-IV: ANOVA",
    "section": "\n14.4 Tutorial",
    "text": "14.4 Tutorial\n\n14.4.1 ANOVA\nFor this assignment, we will use the data file 5groups.sav. Please open it in SPSS.\nAs you have seen in the previous assignments, this file contains the measurements of the y variable for 5 different groups. So far, we have - at most - compared two groups at once. This time, we will compare the means of all 5 groups simultaneously using an Analysis of Variance (ANOVA).\nANOVA is often used to examine the results of experimental research where different groups receive different manipulations of an independent variable, and a continuous dependent variable is measured. In that case, rejecting the null hypothesis indicates a causal effect of the manipulated independent variable on the dependent variable.\nLet’s say the 5 groups in our data file have received 5 different types of training, and the dependent variable y measures the effect of the training.\nWe will now perform an ANOVA to see if these trainings have an equal effectiveness.\nThe null hypothesis of the ANOVA with 5 groups is as follows:\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5\\)\nThat is, the null hypothesis states that the population means of all five groups are the same. Rejecting the null hypothesis implies that at least one one of these means is different from the rest.\nLet’s run the analysis!\nNavigate to Analyze &gt; General Linear Model &gt; Univariate. Choose y as the dependent variable. Choose group as the fixed factor. Now click on the “Options” button and check the two boxes named “Descriptive statistics” and “Homogeneity tests”. Finally, click “Paste” to paste the syntax into the syntax editor, and run it from there.\nYou should end up with the following syntax:\n    UNIANOVA y BY group\n      /METHOD=SSTYPE(3)\n      /INTERCEPT=INCLUDE\n      /PRINT=HOMOGENEITY DESCRIPTIVE\n      /CRITERIA=ALPHA(.05)\n      /DESIGN=group.\nThe first table in the output we will inspect is the “Descriptive Statistics” table. This table displays the means and standard deviations of the variable y for each of the five groups.\nDo you think the population standard deviations are different for each group? If they are, why could that pose a problem for our analysis?\nOne of the assumptions of ANOVA is homoscedasticity. In this case, that means that the population of each group has the same variance (and hence, the same standard deviation). This assumption is also called “homogeneity of variance” (= translation of homoscedasticity). Of course the variance in each sample will differ somewhat. If these differences are significant, there is evidence to doubt our assumption. That’s why we asked SPSS to perform “Homogeneity tests”.\nNote that the SD’s of groups 1 and 2 are quite different from the SD’s of groups 3, 4, 5.\nThe next table shows the output of Levene’s test. You might remember using Levene’s test for comparing the variances of two groups in the context of the independent samples t-test. This time, it tests whether the variance of all 5 groups should be considered equal.\nIs there a reason to doubt the assumption of homoscedasticity based on Levene’s test? Note: Use the Levene’s test “Based on mean”. mcq(c(answer = \"Yes\", \"No\"))\nIf Levene’s test is significant, there is evidence that the population variances of at least 2 of the groups differ. This is evidence against our assumption. This could pose a problem for our analysis. We may choose to use a version of the analysis that is robust to violations of this assumption instead, but that makes our analysis dependent on the data (= no longer confirmatory). Instead, we could discuss the violation, and compare results with and without a robust test.\nFor now, we continue with interpreting the output of the final table. There’s a lot of information, but for now we are only interested in the Sig. value of the “Corrected Model” in the first row. This is the two-sided p-value we can use to test our null hypothesis.\nWhat is the two-sided p-value? Do you reject the null hypothesis? What does that mean?\n\n\nAnswer\n\nThe p-value is &lt;.001. This is smaller than 0.05. Therefore, we reject the null hypothesis, which was: \\(H_0: \\mu1=\\mu2=\\mu3=\\mu4=\\mu5\\)\nThis means that the means of at least two groups are different. Note that we do not yet know for which groups the means differ!\n\nFinally, there’s an interesting nugget of information below the final table. It’s called R Squared. This shows the total amount of variance in y that is explained by group membership.\nWhat is the value of R Squared? \nBy rule of thumb, what is the magnitude of this value (small, medium, or large)?\nCohen (1988) proposed the following guidelines for interpreting the magnitude of R2:\n\n\nSize\nR2\n\n\n\nSmall\n0.01\n\n\nMedium\n0.06\n\n\nLarge\n0.138\n\n\n\n14.4.2 ANOVA using regression\nThis time, we will conduct the ANOVA using the regression interface.\nWhen we conduct ANOVA using regression, we still test the null hypothesis mentioned before:\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5\\)\nA different way to phrase this when using regression is to state that all regression coefficients will be zero:\n\\(H_0: \\beta_0 =\\beta_1 = \\beta_2 = \\beta_3 = \\beta_4\\)\nOr to say that the explained variance will be zero:\n\\(H_0: \\rho^2 =0\\)\nAll of these hypotheses are interchangeable and imply that the means of all five groups are the same. If this is not the case, each of these null-hypotheses would be rejected. Rejecting these null hypothesis implies that at least one one of these means is different from the rest.\nTo test the differences between groups, we first create dummy variables. Let’s make them for all categories, but we will mostly use group 1 as reference category. When making dummies, it’s most convenient to use syntax:\nRECODE group (1=1) (2=0) (3=0) (4=0) (5=0) INTO dgroup1.\nRECODE group (1=0) (2=1) (3=0) (4=0) (5=0) INTO dgroup2.\nRECODE group (1=0) (2=0) (3=1) (4=0) (5=0) INTO dgroup3.\nRECODE group (1=0) (2=0) (3=0) (4=1) (5=0) INTO dgroup4.\nRECODE group (1=0) (2=0) (3=0) (4=0) (5=1) INTO dgroup5.\nEXECUTE.\nCreate the necessary syntax for a regression with the dummy variable that compares the means of group 1 against all other groups.\nYou can find the dialog for the regression under Analyze &gt; Regression &gt; Linear\nIn the SPSS dialog you have to specify the Dependent and Independent variable. In our case, the independent variables are all dummies we created, except for the reference category! Use category 1 as reference category.\nCompare your syntax to the correct syntax:\nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT y\n  /METHOD=ENTER dgroup2 dgroup3 dgroup4 dgroup5.\nAlso compare this syntax to the one we used for t-test using regression. What is the only difference?\nWhich test statistic do you use to determine the significance of your ANOVA? \nR2\nAll of the ts\nt for the Constant\nF\nWhat is the value of the test statistic? \nCompare this output to the output of your previous ANOVA!\nWhich parts are the same? Which parts are different?\n\n\nAnswer\n\nThe F-test is identical to the one from the ANOVA. What’s different is that the regression also gives us t-tests for the difference between each group and the reference group. By changing the reference group, we can make all possible comparisons.\n\nNote that, unlike the t-test interface, the regression interface does not provide a Levene’s test. This is one reason you might want to use the t-test interface. The regression interface provides a more generic way to test the assumption of homoscedasticity: a residual plot.\nGo back through the regression interface, but this time click the Plots button and plot the predicted value (X = ZPRED) against the residual value (Y = ZRESID).\nAlternatively, just add this line to your syntax (make sure to remove the period . from what was previously the last line):\n  /SCATTERPLOT=(*ZRESID ,*ZPRED).\nIf the assumption of homoscedasticity is met, we should see that the dots in this plot are equally distributed around the zero line for all values on the X-axis. In this case, we see some differences that could lead us to question the assumption. However, we don’t get an actual test, which is a pity. Thus, you could use the ANOVA interface if you want this test.\n\n14.4.3 One-Way ANOVA\nWe have prepared the following data file for this assignment: hiking.sav. Please download it and open it in SPSS.\nThe data file describes the result of a fictitious experiment in which a hiking guide has displayed five different types of behavior towards different groups of hikers. The treatment that each person received from the guide is recorded in the variable behavior.\nThe dependent variable of this experiment is feeling. Higher scores on this variable indicate a more positive attitude of a participant towards the guide. In this assignment, we will use ANOVA to determine whether the mean score on the dependent variable differs between the five experimental conditions.\nWhat type of design do we use for this experiment? \nWithin-subjects design\nBetween-subjects design\nCombination of the two\nAs you will have noticed, the data file contains a third variable named weather, which can be either good or bad. For now, we will only look at the results obtained during good weather. Hence, we will use “Select cases” to select only those participants with a value of 1 on the weather variable.\nClick Data &gt; Select Cases and select “If condition is satisfied” and click the “If”-button. Now enter the following condition into the equation box:\nweather = 1\nNow click “Continue” and “Paste” to paste the resulting syntax into the syntax editor. Select Run &gt; All to run it.\nVerify that half of the participants have been crossed out in the Data View.\nWe are now ready to perform an ANOVA with the 50 remaining participants.\nTo run an ANOVA in SPSS there are multiple options. We will use the module “General Linear Model”, which encompasses ANOVA and all of its extensions which we will discuss later on (factorial ANOVA, ANCOVA, and repeated measurements).\nAnyway, let’s first create the basic syntax.\nAnalyze &gt; General Linear Model &gt; Univariate Choose feeling as the dependent variable and behavior as the fixed factor Click on the “Options” button and check the two boxes named “Descriptive” and “Homogeneity tests”. After clicking “Paste” you should get the following syntax:\nUNIANOVA feeling BY behavior    \n    /METHOD=SSTYPE(3)   \n    /INTERCEPT=INCLUDE   \n    /PRINT=DESCRIPTIVE HOMOGENEITY    \n    /CRITERIA=ALPHA(.05)   \n    /DESIGN=behavior.   \nWhat is the p-value of the Levene’s test? Use the Levene’s test “Based on mean” again. \nDo we have reason to question the assumption of equal population variances? \nYes\nNo\nIn ANOVA, we distinguish between three sources of variation: the Sums of Squares total (SSt), the Sums of Squares between (SSb, or SSR) and the Sums of Squares within (SSw, or SSE).\nWhat does the Sum of squares total mean (phrase your answer in your own words)? Look up the value of the SSt in the ANOVA output and write it down as well.\nWhat does the Sum of squares between entail (phrase your answer in your own words)? Look up the value of the SSb in the ANOVA output, and write it down as well.\nWhat does the Sums of squares within entail (phrase your answer in your own words)? Look up the value of the SSw in the ANOVA output, and write it down as well.\n\n\nAnswer\n\nThe Between Groups Sum of squares or SSb is equal to 18.330 and simply give the squared distance of individual scores to the mean, summed together. In other words, it shows how much variability there is in the group means. If all group means would be equal to each other, the SSb equals 0.\nThe SSw here is 38.405. It tells us how much the individual scores within a group deviate from the group mean. In other words, it shows how much variability there is within the groups. The is the variation that is independent from the experimental effect (because variation within groups cannot be caused by differences in experimental conditions).\nThe SSt tells us how much the invidual scores deviate from the grand mean. In other words, it shows how much variability there is in the dependent variable in total.\nRecall that SSB is the same as SSR; it can be found in the row “Corrected Model”, column Type III Sums of Squares.\nRecall that the SSW is the same as SSE; it islabeled “Error” in the column Type III Sums of Squares.\n\nHow do we use the different types of Sum of Squares to calculate the F statistic?\n\n(SSB)/(SSW)(SSW/dfw)/(SSB/dfb)(SSB/dfb)/(SSW/dfw)(SSW)/(SSB)\n\n\n\nAnswer\n\nWe can calculate F using the following formula:\n\\(F = \\frac{MSb}{MSw}\\)\nThe MSb and MSw give the between group variance and within group variance, respectively. They can be calculated using the following formula’s:\n\\(MSb = \\frac{SSb}{k-1}\\), \\(MSw = \\frac{SSw}{N-k}\\)\n\nAgain, consider the table Tests of Between Subjects, which represents the results of ANOVA.\nWhat is the F-value of the ANOVA? \nThe degrees of freedom between (dfb) are  and the degrees of freedom within (dfw) are .\ndfb = k-1\ndfw = N-k\nAgain consider the table Tests of Between Subjects\nWhat is the p-value of the ANOVA? \nYou can find the p-value of the ANOVA in the table named “Tests of Between-Subjects Effects”. The p-value is equal to the Sig.-value in the first row of this table.\nWhat can you conclude from this?\nWrite down a statistical conclusion and a conclusion within the context of this research example.\n\n\nAnswer\n\nThe p-value is smaller than our alpha level (0.05). Therefore, we can conclude that there was a statistically significant difference in positive attitude between the groups, based on the behaviour the guide displayed towards them, (F(4,45) = 5.369, p = .001).\n\nWhat is the proportion of variance explained by behavior? \nHow would you describe this number in words? So what does it mean?\nRemark: Cohen formulated some rules of thumb for interpreting the \\(R^2\\) How would you qualify the strength of the effect based on Cohen’s rules of thumb? And why should we not take the rules of thumb too seriously?\nCohen (1988) proposed the following guidelines for interpreting the magnitude of \\(R^2\\)\n\n\nSize\n\\(R^2\\)\n\n\n\nSmall\n0.01\n\n\nMedium\n0.06\n\n\nLarge\n0.14\n\n\n\nNote that, in ANOVA, \\(R^2\\) is also called \\(\\eta^2\\) (eta squared) is a measure of effect size, it indicates the amount of variance in thedependen t variable that is explained by the independent variable(s). In our case, 32.3% of the variance in feeling is explained by behaviour. According to Cohen’s guidelines this is a large effect size (see slide 28). However, these guidelines are rather arbitrary, which Cohen himself also stresses.\nThe correct conclusion so far is that the five groups differ significantly on the dependent variable feeling. However, we do not yet know which groups differ.\n\n14.4.4 One-Way ANOVA using regression\nIn this assignment, we will conduct the same ANOVA using the regression interface.\nTo test the differences between groups, we first create dummy variables. Let’s make them for all categories, but we will mostly use group 1 as reference category. When making dummies, it’s most convenient to use syntax. Let’s give the dummies informative names this time:\nRECODE behavior (1=1) (2=0) (3=0) (4=0) (5=0) INTO rushing.\nRECODE behavior (1=0) (2=1) (3=0) (4=0) (5=0) INTO stories.\nRECODE behavior (1=0) (2=0) (3=1) (4=0) (5=0) INTO insulting.\nRECODE behavior (1=0) (2=0) (3=0) (4=1) (5=0) INTO jokes.\nRECODE behavior (1=0) (2=0) (3=0) (4=0) (5=1) INTO singing.\nEXECUTE.\nCreate the necessary syntax for a regression with the dummy variable that compares the means of the rushing group against all other groups.\nYou can find the dialog for the regression under Analyze &gt; Regression &gt; Linear\nIn the SPSS dialog you have to specify the Dependent and Independent variable. In our case, the independent variables are all dummies we created, except for the reference category! Use category 1 as reference category.\nCompare your syntax to the correct syntax:\nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT feeling\n  /METHOD=ENTER stories insulting jokes singing.\nCan you find all three of the aforementioned sources of variation in the output? The Sums of Squares total (SSt), the Sums of Squares between (SSb, or SSR) and the Sums of Squares within (SSw, or SSE).\nCompare the results to those of the One-Way ANOVA interface.\n\n\nAnswer\n\nIn the ANOVA table, the Regression Sum of Squares is identical to the “Between Groups Sum of Squares” from the One-Way ANOVA interface. The Residual Sum of Squares is identical to the “Within Groups Sum of Squares” from the One-Way ANOVA interface. And the Total Sums of Squares are also the same.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM-IV: ANOVA</span>"
    ]
  },
  {
    "objectID": "glm5multiple.html",
    "href": "glm5multiple.html",
    "title": "15  GLM-V: Multiple regression",
    "section": "",
    "text": "15.0.1 Multiple regression\nMultiple regression is a statistical technique that allows us to examine the relationship between one outcome and multiple predictors. It extends the concept of bivariate linear regression, where we model the relationship between two variables, to include more predictors. In the context of social science research, multiple regression helps us answer the question: What is the unique effect of one predictor, while controlling for the effects of all other predictors?\nAs a matter of fact, last week’s analyses for categorical variables with more than two categories were already an example of multiple regression. We included two dummy variables to represent a categorical variable with three categories. All that’s new today is that we also consider the case where the multiple predictors are continuous variables. An important realization is that a regression model can be expanded to include as many predictors as needed. The general formula for multiple regression is \\(\\hat{Y} = a + b_1X_1 + b_2X_2 + \\ldots + b_KX_K\\), where \\(\\hat{Y}\\) represents the predicted value of the dependent variable Y, \\(a\\) is the intercept, and \\(b_{1 \\ldots K}\\) are the slopes for each predictor.\nWhen interpreting the regression coefficients, the intercept (a) represents the expected value of the dependent variable when all predictors are equal to 0. For dummy variables, this is the mean value of the reference category, while for continuous predictors, it represents the expected value for someone who scores 0 on all predictors. The regression coefficients (b1, b2, …, bK) indicate how many units the dependent variable Y is expected to change when the corresponding predictor X increases by 1 unit, while holding all other predictors constant.\nCentering predictors can be useful in multiple regression. By centering, we shift the zero-point of the predictor to a meaningful value, such as the mean value on that predictor. This helps in interpretation, because the intercept now gives us the mean value on the outcome for someone who has an average score on all predictors.\nAs previously explained, standardized regression coefficients drop the units of the predictor and outcome variable. They are calculated by transforming the predictors and outcome variable into z-scores with a mean of 0 and a standard deviation of 1, and performing the (multiple) regression analysis on those z-scores. Because the units of the variables are dropped, standardized coefficients make the effects of predictors comparable across different studies or variables with different measurement units. They represent the change in the dependent variable in terms of standard deviations when the corresponding predictor increases by 1 standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM-V: Multiple regression</span>"
    ]
  },
  {
    "objectID": "glm5multiple.html#lectures",
    "href": "glm5multiple.html#lectures",
    "title": "15  GLM-V: Multiple regression",
    "section": "\n15.1 Lectures",
    "text": "15.1 Lectures",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM-V: Multiple regression</span>"
    ]
  },
  {
    "objectID": "glm5multiple.html#formative-test",
    "href": "glm5multiple.html#formative-test",
    "title": "15  GLM-V: Multiple regression",
    "section": "\n15.2 Formative Test",
    "text": "15.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat is the primary advantage of using multiple regression analysis?\n\nTo group predictors based on their significanceTo find a single predictor that explains all the varianceTo understand the unique effect of each predictor while accounting for othersTo identify the most significant predictor\n\nQuestion 2\nWhat is the purpose of centering in multiple regression analysis?\n\nTo ensure the assumption of normality is metTo choose a meaningful zero-point for predictorsTo remove outliers from the dataTo standardize all predictors\n\nQuestion 3\nWhat does the intercept (a) in a multiple regression model represent?\n\nThe difference between the two groupsExpected value when all predictors are equal to 0The effect of the first predictorThe effect of the last predictor\n\nQuestion 4\nWhat do the b-coefficients in a multiple regression model represent?\n\nThe change in the dependent variable for a one-unit change in the predictor while other predictors are held constantThe change in the dependent variable for a one-unit change in the predictor without considering other predictorsThe average of all predictor valuesThe change in the predictor for a one-unit change in the dependent variable while other predictors are held constant\n\nQuestion 5\nWhen is multicollinearity a concern in multiple regression analysis?\n\nWhen there are multiple outcome variablesWhen predictor variables are independentWhen there are outliers in the dataWhen predictor variables are highly correlated with each other\n\nQuestion 6\nWhat is the role of standardized regression coefficients in multiple regression analysis?\n\nTo compare the effect sizes of predictors on a common scaleTo determine the unique effect of each predictorTo calculate the interceptTo convert categorical predictors to continuous ones\n\nQuestion 7\nWhat is the potential bias introduced when controlling for a collider in multiple regression analysis?\n\nIt biases the estimate of the association between the two variables that cause the colliderIt creates a causal relationship between the variables that cause the colliderIt has no effect on the causal inference between the variables that form the colliderIt biases the estimate of the association between the two variables that are caused by the collider\n\nQuestion 8\nGiven the multiple regression equation: Y = 12.5 + 2.3X1 + 1.8X2 - 0.5*X3, calculate the predicted value of Y when X1 = 5, X2 = 3, and X3 = 2.\n\n27.620.528.423.1\n\nQuestion 9\nIn a multiple regression model, if the coefficient of determination (R²) is 0.75 and the SSE is 150, what is the value of SST?\n\n4500.25112.5600\n\nQuestion 10\nGiven the following standardized regression equation: Y = 0.6 + 0.35X1 + 0.25X2 - 0.15*X3, what is the correct conclusion about the effect of X2?\n\nX2 is associated with a 0.25 increase in Y, keeping all other predictors constant.A one unit increase in X2 is associated with a 0.25 unit increase in Y.A one SD increase in X2 is associated with a 0.25 SD increase in Y, keeping all other predictors constant.A one unit increase in X2 is associated with a 0.25 increase in Y, keeping all other predictors constant.\n\n\n\n\nShow explanations\n\nQuestion 1\nMultiple regression analysis helps to understand the unique effect of each predictor while controlling for the effects of other predictors.\nQuestion 2\nCentering is used in multiple regression analysis to choose a meaningful zero-point for predictors.\nQuestion 3\nThe intercept (a) in a multiple regression model represents the expected value when all predictors are equal to 0.\nQuestion 4\nThe b-coefficients in a multiple regression model represent the change in the dependent variable for a one-unit change in the predictor while other predictors are held constant.\nQuestion 5\nMulticollinearity is a concern in multiple regression analysis when predictor variables are highly correlated with each other.\nQuestion 6\nStandardized regression coefficients are used to compare the effect sizes of predictors on a common scale, especially when the units of predictors are not meaningful.\nQuestion 7\nControlling for a collider in multiple regression analysis can introduce bias in the estimated association between the variables that form the collider.\nQuestion 8\nPlug in the values of X1, X2, and X3 into the regression equation: Y = 12.5 + 2.35 + 1.83 - 0.5*2 = 28.4.\nQuestion 9\nUse the formula R2 = 1-(SSE/SST) or R2 = SSR/SST\nQuestion 10\nThe correct interpretation is in the original units of the variables, and emphasizing the fact that other predictors were controlled for.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM-V: Multiple regression</span>"
    ]
  },
  {
    "objectID": "glm5multiple.html#in-spss",
    "href": "glm5multiple.html#in-spss",
    "title": "15  GLM-V: Multiple regression",
    "section": "\n15.3 In SPSS",
    "text": "15.3 In SPSS\n\n15.3.1 Multiple Regression",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM-V: Multiple regression</span>"
    ]
  },
  {
    "objectID": "glm5multiple.html#tutorial",
    "href": "glm5multiple.html#tutorial",
    "title": "15  GLM-V: Multiple regression",
    "section": "\n15.4 Tutorial",
    "text": "15.4 Tutorial\n\n15.4.1 Multiple Regression\nSocial science students were asked about their opinion towards Tilburg’s nightlife, number of Facebook friends, and some other characteristics. The data are in the SocScSurvey.sav file.\nIn a previous assignment we predicted Facebook friends by extraversion.\nIn this question we will add another predictor, peer pressure.\nThe variable peer pressure refers to the tendency to be influenced by close friends. Higher scores reflect higher sensitivity to peer pressure.\nBefore we proceed with the regression analysis, we will first look at the correlations between the variables.\nAnalyze &gt; correlate &gt; bivariate.\nNow choose as variables: Facebook Friends, Extraversion and Peer Pressure, and click OK.\nWhat is the correlation between peer pressure and number of Facebook friends? \nSuppose three researchers test the significance of the correlation between peer pressure and Facebook friends. Researcher I tests at the 10% level, researcher II tests at the 5% level, and researcher III at the 1% level.\nWhich researcher will reject the null hypothesis? \nAll three researchers\nOnly researcher II\nOnly researcher I\nOnly researcher III\nNow, run the regression analysis in which the number of Facebook friends is regressed on extraversion and peer pressure.\nProceed as follows: via analyze &gt; regression &gt; linear. Choose Facebook friends as dependent and extraversion and peer pressure as independents.\nConsult the output and write down the regression equation.\n\n\nAnswer\n\n\\(\\text{Friends}_i = -158.012 + 26.560*\\text{Extraversion}_i + 12.056*\\text{Peer}_i + e_i\\)\n\nConsider the second person in the sample. The person had an extraversion score of 11 and a score of 9 on peer pressure.\nWhat is the predicted number of Facebook friends for this person? \nConsult the output.\nResearchers conclude that – in the sample – as peer pressure increases with one unit, the predicted number of Facebook friends increases with 12.056 units.\nIs this a valid conclusion? \nYes\nNo\nIn multiple regression, the regression coefficients show us the expected changes in the dependent variable, while keeping the other independent variables constant.\nWith this in mind, what is the correct conclusion?\n\nAs peer pressure increases with one unit the predicted number of Facebook friends increases with 12.056 units.As peer pressure increases with one unit the predicted number of Facebook friends increases with 12.056 units. This is added to the constant of -158.01.As peer pressure increases with one unit the predicted number of Facebook friends increases with 12.056 units, while keeping extraversion constant.As peer pressure increases with one unit the predicted number of Facebook friends increases with 12.056 units, while extraversion changes with 26.56 units.\n\nConsult the table Coefficients. The table shows the results of t-tests.\nWhat are the null hypotheses and alternative hypotheses that are tested here?\n\n\nAnswer\n\nThe t-tests test significance of the individual regression coefficients. In particular, for each coefficient we can use the t-tests to test the following hypotheses:\n\\(H_0: \\beta = 0\\), \\(H_1: \\beta \\ne 0\\)\n\nWhat is the value of the test-statistic for the significance test for extraversion? \nConsider the t-tests for the regression coefficients again.\nHow many degrees of freedom do the t-tests have? \n\n\nExplanation\n\nNote that:\nDegrees of freedom = N - p N = number of participants; p = number of parameters in the model (intercept + two regression slopes)\n\nSuppose three researchers test the significance of peer pressure as a predictor of Facebook friends, while controlling for extraversion. Researcher I tests at the 10% level, researcher II tests at the 5% level, and researcher III at the 1% level.\nWhich researcher(s) will reject the null hypothesis? \nOnly researcher III\nOnly researcher I\nAll three researchers\nOnly researcher II\nWhat percentage of the total variance in Facebook friends can be explained by both extraversion and peer pressure? \nCompare the \\(R^2\\) of the regression model with both predictors with the \\(R^2\\) of a model with only extraversion as the predictor.\nWhat is the difference? \nIn the previous step we compared the \\(R^2\\) of two so called nested models.\nTwo models are nested if the larger model (i.e., the model with the most predictors) contains all predictors of the smaller model.\nIn the next lecture we will learn more about nested models, model comparisons, and how useful they are for researchers!\n\n15.4.2 Multiple Regression II\nFor this assignment we need the file HealthyFood.sav.\nThis file contains hypothetical data on three variables:\nEating healthy food (the higher the score, the healthier a person’s diet) Knowledge about food (the higher the score, the more a person knows about healthy food and risks of unhealthy food) Income (higher scores = more income).\nLet’s first look at the associations (correlations) between the three variables.\nCompute the correlations and summarize the relationships between all pairs of variables. Include in your answer the strength of the relationship (i.e., weak, moderate, or strong), the direction of the relationship (i.e., positive or negative), and generalizability to the population (i.e., is the correlation significant at the 5% level).\nCohen’s rules of thumb:\n\nr = 0.00-0.30 (none to weak)\nr = 0.30-0.50 (weak to moderate)\nr = 0.50-0.70 (moderate to strong)\nr = 0.70-0.90 (strong to very strong)\nr = 0.90-1.00 (very strong)\n\n\n\nAnswer\n\n\nIncome and eating have a weak positive correlation, which is significant at the 5% level.\nIncome and knowledge have a weak to moderate positive correlation, which is significant at the 5% level.\nKnowledge and eating have a moderate to strong positive correlation, which is significant at the 5% level.\n\n\nResearchers may be interested in explaining differences in eating healthy food: in other words, they want to know why some people eat very healthy, while others tend to eat unhealthy.\nOne of the hypotheses is that healthy food is on average more expensive than unhealthy food, so one of the explanatory variables may be income.\nRun a regression analysis using eating as the dependent variable and income as the independent variable.\nConsult the output.\nWhich of the following conclusions is correct?\n\nThe effect of Income on eating healthy food is positive and significantly different from zero.The effect of Income on eating healthy food is positive but not significantly different from zero.\n\nSimple regression analysis suggests a positive relationship between income and healthy food.\nHowever, other researchers (say Team B) came up with an alternative explanation. They hypothesized that the relationship between income and healthy food can be explained by a confounder; knowledge. People with more knowledge will have better jobs (on average), and, as a result more, income. As the result of their knowledge they also prefer to eat healthy food. I.e., Team B thinks knowledge is a common cause of income and eating healthy food.\nIn other words, the researchers of Team B hypothesize that the relationship between income and eating healthy food is \nIndirect\nSpurious\nDraw (on a piece of paper) the conceptual model that reflects the hypotheses of the researchers.\n\n\nAnswer\n\n\n\nNow let’s see if the data support the hypotheses of the researchers.\nRun a multiple regression analysis using eating healthy food as the dependent variable and income and knowledge as independent variables.\nConsult the output. Look at the effect of income, controlled for knowledge (both the coefficient and the significance test).\n\nWhat happened with the effect of income if you control for knowledge?\nDoes knowledge predict eating healthy food (controlled for income)?\nDo the data support the hypothesis that the relationship between income and healthy food is confounded by knowledge?\n\nWhat is the p-value of Income when you control for Knowledge? \nWhat is the p-value of Knowledge, controlling for Income? \nDo the data support the hypothesis that the relationship between income and healthy food is confounded by knowledge? \nYes\nNo\nFinally, interpret the output. Write down the answers to the following questions:\n\nHow well can we predict the variance in healthy eating with the predictors income and knowledge? Interpret R2, report the appropriate test and its significance\nInterpret the regression coefficients (size, direction, significance)\nWhich predictor is the most important predictor of healthy eating behavior? Inspect the standardized regression coefficients\n\n\n\nAnswer\n\nIncome and Knowledge together predict 44.9% of the variance in Eating healthy food, which is significantly different from zero, \\(R^2 = .45, F(2, 347) = 141.179, p &lt;.001\\).\nControlling for Knowledge, Income has a positive, but non-significant effect on Eating healthy food, t(347) = .283, p = .778.\nControlling for Income, Knowledge has a positive, significant effect on Eating healthy food, t(347) = 15.287, p &lt;.001.\nKnowledge is the most important predictor (\\(\\beta\\) = .665) (compare it with \\(\\beta\\) = .012 of Income).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM-V: Multiple regression</span>"
    ]
  },
  {
    "objectID": "glm6nested.html",
    "href": "glm6nested.html",
    "title": "16  GLM-VI: Nested models",
    "section": "",
    "text": "16.1 Lecture\nNested models refer to models that are identical, except for the fact that some parameters are constrained to zero in one of them, while all parameters are free in the other. The smaller or constrained model model is said to be “nested in” the larger or unconstrained model, meaning that all predictors in the smaller model are also present in the larger model. By definition, the unconstrained model always provides a better fit than the constrained model.\nIncremental F-tests are used to determine whether the increase in \\(R^2\\) between two nested models is statistically significant. Recall that \\(R^2\\) represents the proportion of variance in the dependent variable explained by the predictors in the model. The incremental F-test compares the variation in the dependent variable explained by an unconstrained model with the variation explained by a constrained model. The F-test assesses whether the increase in R-squared is greater than what would be expected by chance alone, indicating the significance of adding additional predictors to the model.\nHierarchical regression refers to an approach where predictors are added to a regression model in blocks, allowing us to assess the additional variance explained by each block of predictors with an incremental F-test. Each block represents a set of predictors, and they are added to the model in a stepwise manner. This approach is useful when we want to determine the significance of adding a single categorical predictor which is represented by multiple dummy variables, or when we want to test whether adding some predictors (e.g., theoretically relevant variables) explain significant variance above and beyond other variables (e.g., demographic covariates). By conducting incremental F-tests at each step, we can assess the significance of adding each block of predictors and their contribution to the overall model.\nIn summary, nested models allow us to compare models with different levels of complexity, incremental F-tests help us determine the significance of adding predictors to a model, and hierarchical regression enables the examination of additional variance explained by different blocks of predictors. These concepts are valuable tools in statistical analysis and can provide insights into the relationships between variables and the predictive power of a model.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>GLM-VI: Nested models</span>"
    ]
  },
  {
    "objectID": "glm6nested.html#formative-test",
    "href": "glm6nested.html#formative-test",
    "title": "16  GLM-VI: Nested models",
    "section": "\n16.2 Formative Test",
    "text": "16.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhich of the following statements is true about nested models?\n\nModel A is nested in Model B if they are identical, except Model A uses a subsample of the sample used to estimate Model B.Model A is nested in Model B if they are identical, except some parameters in model A are constrained.Nested models always have the same number of predictors.Model A is nested in Model B if they are identical, except some parameters in model B are constrained.\n\nQuestion 2\nWhat can we say about the relative fit of Model A and Model B if Model A is nested in Model B?\n\nModel A will always have better fit than Model B.In nested models, one model always has better fit than the other.Nested models always have equal fit.Model B will always have better fit than Model A.\n\nQuestion 3\nIn the context of nested models, what does ‘constrained’ mean?\n\nSome variables are fixed to 0.Some parameters are fixed to 0.The R-squared value of the smaller model is constrained to be lower than that of the larger model.The set of predictors is constrained.\n\nQuestion 4\nWhat is the purpose of performing a nested model test?\n\nTo compare two different models.To check if the predictors are correlated with each other.To determine if the models are independent of each other.To determine whether adding specific predictors significantly improves the model's fit.\n\nQuestion 5\nGiven the nested models: Y = a + b1X1 and Y = a + b1X1 + b2*X2, which model is the ‘constrained’ one?\n\nNeither is constrained.Y = a + b1X1 + b2X2It cannot be determined from the information given.Y = a + b1*X1\n\nQuestion 6\nHow does the F-test for the R-squared of a single model relate to the F-test for the delta R-squared of two nested models?\n\nThe similarity is only superficial; although both are F-tests, the calculation is different.Both can be seen as R-squared tests; both compare the model of interest to a model with only an intercept, which is a nested model test.Both can be seen as delta R-squared tests; the former compares the model of interest to a model with only an intercept, which is a nested model test.Both are F-tests; these are ratios comparing the explained variance to the unexplained variance.\n\nQuestion 7\nWhat can we conclude from an incremental F-test in nested models?\n\nWhether the predictors of the nested models are significantly different.Whether the smaller model is significantly better than the larger model.Whether the larger set of predictors has significant multicollinearity.Whether adding specific predictors significantly improves the model's fit.\n\nQuestion 8\nWhen performing an incremental F-test, what does the numerator in the F ratio represent?\n\nThe reduction in residual mean square when adding predictors to the model.The reduction in the number of predictors from the larger to the smaller model.The increase in the number of degrees of freedom when adding predictors to the model.The increase in residual mean square when adding predictors to the model.\n\nQuestion 9\nWhich scenario is suitable for using hierarchical regression?\n\nTo test whether each dummy of a categorical predictor with &gt;2 categories explains more variance than the other dummies.To determine if theoretically relevant factors explain variance beyond demographic characteristics.To check the multicollinearity of the predictors.To compare two unrelated models.\n\nQuestion 10\nGiven two nested models, one has regression sum of squares = 283.96 and residual sum of squares = 958.58; the other has regression sum of squares = 202.76 and residual sum of squares = 1039.78. What is the delta R-squared between these models?\n\n.065Can't say with this information.085.078\n\nQuestion 11\nIn a nested model test, numerator degrees of freedom for the F ratio is 4, how many parameters were added to the model?\n\n534Depends on the model\n\nQuestion 12\nIn a nested model test, one model has regression sum of squares = 202.76 and 3 parameters and residual sum of squares = 1039.78, the other model has regression sum of squares = 283.96 and 4 parameters and residual sum of squares = 958.58. There are 387 participants. What is the F-value for the nested model test?\n\n28.29Can't say based on this information.32.4424.90\n\n\n\n\nShow explanations\n\nQuestion 1\nNested models are identical except for some constrained parameters.\nQuestion 2\nThe bigger model always has a better fit than the smaller model in a nested pair.\nQuestion 3\nIn the context of nested models, a ‘constrained’ model has some parameters fixed to 0.\nQuestion 4\nA nested model test is used to determine whether adding specific predictors significantly improves the model’s fit.\nQuestion 5\nThe model Y = a + b1*X1 is the ‘smaller’ or ‘constrained’ model.\nQuestion 6\nBoth F-tests can be seen as nested model tests; the F-test for the R-squared of a single model tests against the variance explained by a model including only the intercept.\nQuestion 7\nThe incremental F-test determines if adding specific predictors significantly improves the model’s fit.\nQuestion 8\nThe numerator in the F ratio of an incremental F-test represents the reduction in residual mean square when adding predictors to the model.\nQuestion 9\nHierarchical regression is suitable for determining if theoretically relevant factors explain variance beyond demographic characteristics.\nQuestion 10\nThe delta R-squared is the difference in regression sums of squares, divided by the total sum of squares. The latter can be calculated by adding the error sum of squares to the regression sum of squares for either model.\nQuestion 11\nThe numerator degrees of freedom is equal to the difference in number of parameters.\nQuestion 12\nThe F-value is obtained by first calculating the regression mean square: dividing the difference in regression sum of squares by the difference in model DF: (283.957-202.758)/1, and calculating the residual mean square for the larger model: 958.58/(387-4). The F-value is then ((283.957-202.758)/1)/(958.58/(387-4)).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>GLM-VI: Nested models</span>"
    ]
  },
  {
    "objectID": "glm6nested.html#in-spss",
    "href": "glm6nested.html#in-spss",
    "title": "16  GLM-VI: Nested models",
    "section": "\n16.3 In SPSS",
    "text": "16.3 In SPSS\n\n16.3.1 Multiple Regression",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>GLM-VI: Nested models</span>"
    ]
  },
  {
    "objectID": "glm6nested.html#tutorial",
    "href": "glm6nested.html#tutorial",
    "title": "16  GLM-VI: Nested models",
    "section": "\n16.4 Tutorial",
    "text": "16.4 Tutorial\n\n16.4.1 Multiple Regression\nThis assignment revolves around multiple regression with more than two predictors.\nUse the data file called Work.sav. These data were about work characteristics.\nOpen the date file in SPSS to get started!\nConsider the following research question(s):\n“To what extent do variety at work, learning possibilities, and independence at work explain pleasure at work, and which of these explanatory characteristics is most important?”\nWhat are the independent variables in this research question?\n\nvariety, learning possibilities, independence, pleasurevariety, learning possibilities, independencevarietypleasure\n\nLet’s now run a multiple regression analysis to get the output we need to answer our research question.\nNavigate to Analyze &gt; Regression &gt; Linear. Enter the dependent variable (pleasure at work). Enter the independent variables (variety at work, learning possibilities, independence at work). Paste and run the syntax.\nIn the next steps we will go over the output from this analysis.\nWhat percentage of the total variance in work pleasure is explained by variety at work, learning possibilities, and independence at work altogether? \nWhich of the following statements correctly summarizes the F-test for the R2 in this analysis?\n\nR-square is significant, F(3, 121) = 14.455, p &lt; .001R-square is not significant, F(3, 121) = 14.455, p &lt; .001R-square is not significant, F(3, 118) = 14.455, p &lt; .001R-square is significant, F(3, 118) = 14.455, p &lt; .001\n\nConsider the “Coefficients” table.\nWhat is the unstandardized regression coefficient for the effect of variety at work on work pleasure? \nDescribe the effect of variety at work on work pleasure as precise as possible.\nIn other words, how should we interpret the unstandardized regression coefficient for variety at work?\n\n\nAnswer\n\nWhen variety at work increases with one unit and the other two variables stay constant, pleasure at work increases with .272 units.\n\nNow, consider the effect of independence at work on work pleasure.\nWhat is the standardized regression coefficient of the effect of independence at work on work pleasure? \nDescribe the effect the effect of independence at work on work pleasure as precise as possible using the standardized regression coefficient.\nIn other words, how should we interpret the standardized coefficient for this predictor?\n\n\nAnswer\n\nWhen independence at work increases with one SD and the other two variables stay constant, pleasure at work increases with .107 SDs.\n\nWhich of the predictors has a significant partial effect on pleasure at work when using \\(\\alpha\\) =.10? \nVariety at work\nVariety at work and Learning possibilities\nIndependence at work\nNone\nLearning possibilities\nIn the final couple of steps we went through the output displayed in the “Coefficients” table. Before you proceed with the next assignment, please review the following aspects in the answers your gave:\n\nWhen you wrote down the effect of variety at work on pleasure at work in unstandardized form, did you include that it is the effect of the IV on the DV controlled for the other two variables? If not, please keep in mind that this is very important! In multiple regression, all effects are partial (unique) effects, controlling for the other predictors.\nWhen you wrote down the effect of independence at work on pleasure at work in standardized form, did you indicate that it is the effect in standard deviations, and, again, did you include that it is the effect of IV on the DV controlled for the other two variables (i.e., 1 SD change in independent variable independence at work leads to a .107 SDs change in the predicted score for pleasure at work, controlled for the other variables)?\n\n16.4.2 Unique Contributions\nUse the data file called Work.sav. These data were about work characteristics.\nIn this assignment, we will look more closely at how to evaluate the “added value” of one predictor in addition to the other predictors in an analysis explaining the dependent variable. Or, put the other way around, how much we lose if we would remove a certain predictor from the model.\nWe will again consider the analysis in which we predicted pleasure at work by the three predictors learning possibilities, independence at work, and variety at work.\nWe want to know how much the variable variety at work adds to predicting work pleasure.\nRun the two analyses specified below:\nRun the regression analysis using all three predictors.\nRun the regression analysis analysis with only learning possibilities and independence at work as predictors.\nInspect the R2 of both regression analyses.\nWhat happened with the R2 when variety was removed?\n\n\nAnswer\n\nWith all three predictors included, R-square equals .269 (see slide 3) With Variety at Work removed, R-square equals .248. So, R-square decreased with .021, indicating that Variety at Work uniquely explains 2.1% of the total variance in Pleasure at Work.\n\nLet’s do the same for the other two predictors; that is, check how much the R2 changes if you would remove the predictor from the model.\nWrite down the changes below.\n\n\nAnswer\n\nWith Learning Possibilities removed, R square equals .211. R-square decreased with .058, which means that Learning Possibilities uniquely explains 5.8% of the total variance in Pleasure at Work. With Independence at Work removed, R square equals .260. R-square decreased with .009, which means that Independence at Work uniquely explains 0.9% of the total variance in Pleasure at Work.\n\nWhich predictor explained the least unique variance, controlling for the other two? \nVariety at work\nIndependence at work\nLearning possibilities\n\n16.4.3 Hierarchical Regression Analysis\nIn this assignment we will carry out a hierarchical regression analysis, which is a structured way to compare nested models in linear regression analysis.\nWith hierarchical regression analysis we are able to compare two (or more) nested models. Consider the model below:\n\\(Y′ = b0 + b1X1 + b2X2 + b4X4 + b5X5 + b7X7\\)\nWhich of the following models is/are nested within the larger model presented above?\n\n\\(Y′=b0+b1X1+b3X3+b4X4\\)\\(Y′=b0+b1X1+b2X2+b4X4\\)\\(Y′=b0+b1X1+b2X2+b6X6\\)\n\nWe will now get back to our data on work characteristics using the data file Work.sav.\nConsider the following situation in which researchers are interested in clusters of variables:\nResearchers are interested in the effect of “Health” (i.e., mental pressure, physical demands, and emotional pressure) and “Social Environment” (i.e., relationship with coworkers, and relationship with supervisors) on the Need for recovery.\nWe will address this research question in a number of steps.\nFirst, we will look at the full model (i.e., the model that has all health and social environment predictors).\nRun the regression analysis in which you regress Need for recovery on the three health variables (i.e., mental pressure, physical demands, and emotional pressure) and the two social environment variables (i.e., relationship with coworkers, and relationship with supervisors).\nWhat is the R2 of the full model? \nNow we want to test the significance of the clusters of variables. We will first look at the Health variables (emotional pressure, physical demands, & mental pressure). We want to test whether Health has a direct effect on Need for recovery controlled for the social environment predictors.\nTo accomplish this, we have to do a hierarchical regression analysis, in which we compare two nested models: a small one, and a larger one.\nWrite down what these two models look like.\nHow many predictors does the smaller model contain? \nHow many predictors does the larger model contain? \n\n\nAnswer\n\n\nSmall model: \\(Recover_i = b0 + b1 * sccowork_i + b2 * scsuperv_i + e_i\\)\n\nLarge model: \\(Recover_i = b0 + b1 * sccowork_i + b2 * scsuperv_i + b3 *scmental_i + b4* scphys_i + b5* scemoti_i + e_i\\)\n\n\nThe small model has 2 predictors and the large model has 5 predictors. The large model has three predictors more than the small model.\n\nNow run the hierarchical regression analysis in SPSS!\nNavigate to Analyze &gt; Regression &gt; Linear\nClick on “Reset” to start from scratch.\nSelect Need for Recovery as the dependent variable.\nSelect only the Social Environment variables (i.e., Relationship with coworkers and Relationship with supervisors) as the independent variables (this is the small model). This is block 1 of 1.\nClick on “Next”. Select the three Health variables (i.e., emotional pressure, physical demands, and mental pressure) and add those to this second block.\nImportant: Now click on the button “Statistics” to request the R2 change.\nPaste and run the syntax. Note the following new elements:\n  /STATISTICS COEFF OUTS R ANOVA CHANGE\n  /METHOD=ENTER sccowork scsuperv\n  /METHOD=ENTER scmental scphys scemoti.\nThe CHANGE command asks for \\(R^2\\) change statistics, and the two rows of /METHOD=ENTER sequentially add blocks of predictors to the model (enter them into the model).\nLet’s inspect the table labeled “Model Summary” first.\nWhat percentage of the total variance in Need for recovery is explained by the small model? \nHow much of the total variance in Need for recovery is explained by the large model? \nWhat is the R2-change from Model 1 to Model 2? \nThe Model Summary table also reports the results of the F-tests, which tests whether the change in R2 is significant.\nWrite down the null and alternative hypothesis that are evaluated by these F-tests, then check your answer?\n\n\nAnswer\n\n\\(H_0: \\Delta R^2 = 0\\)\n\\(H_0: \\Delta R^2 \\ne 0\\)\n\nSuppose three researchers use the output to see whether the R2-change is significant. Researcher I tests at the 10% level, Researcher II tests at the 5% level, and Researcher III tests at the 1% level.\nWhich of the researchers will conclude that there is a significant result? \nOnly researcher III\nOnly researcher I\nOnly researcher II\nAll three researchers\nSuppose the researchers summarized the result as follows:\n“Health variables explain 11.3% of the variance in need for recovery.” \nYes\nNo\n\n\nExplanation\n\nThe correct interpretation is:\n“The health variables explain an additional 11.3% of the total variance in Need to recover, on top of what is already explained by the social environment variables.”\nAlternatively, you may summarize the result as:\n“The health variables uniquely explain 11.3% of the total variance in Need to recover, while controlling for the social environment variables.”\nSince the R2-change was significant, we have evidence that “Health” had a direct effect on the Need to recover.\n\nNow test whether the social environment variables have an effect on need to recover (while controlling for the health variables). Summarize the results (include all relevant statistics: test statistics, degrees of freedom, p-values).\nInclude in your answer:\nThe R2 of the Small and Full model.\nThe R2-change and whether or not it is significant (use a significance level of .05).\nA substantive interpretation of the R2-change (within the context of this assignment).\nThe first step is to run the hierarchical regression analysis, with the health variables in Block 1 and the social environment variables in Block 2.\n\n\nAnswer\n\nThe model with social environment variables explains 14.2% of the total variance in Need for Recovery. Health variables explain an additional 7.8% of variance in Need for Recovery, and this difference significantly differs from zero, \\(\\Delta R^2 = .08, F(2,116) = 5.800, p = .004\\). This means that the social environment variables uniquely explain 7.8% of the total variance in Need for Recovery, while controlling for the health variables. The total explained variance in Need for Recovery is 22.0%, which is significantly different from zero, \\(F(5, 116) = 6.536, p &lt; .001\\).\n\n\n16.4.4 Dummies and Continuous Predictors\nIn this assignment we will predict a continuous outcome variable from a dichotomous predictor (i.e. gender).\nThe data file that we will use is PublicParticipation.sav.\nThis data file contains data on the following variables:\n\nincome (higher score = higher income)\npublic participation (i.e. being a member of school boards, municipal councilor, etc.)\neducation\nage\ngender (0 = females; 1 = males)\n\nWe want to test the following research question:\n“Are there gender differences in Public Participation?”\nTo answer this research questions, we can use an independent samples t-test.\nRun an independent samples t-test to answer the research question (consult the information button in case you forgot how to run an independent samples t-test.)\nConsult the output to answer the questions in the next steps.\nAnalyze &gt; Compare means &gt; Independent samples t-test.\nChoose Public Participation as the dependent variable.\nThe grouping variable is Gender. Define the groups: group 1 = 0 (i.e., women) and group 2 = 1 (i.e., men).\nPaste and run the syntax.\nWhat is the mean difference between men and women? \n\nConsult the table Group Statistics.\n“In the sample, men score on average  points higher on public participation than women.”\nWhich of the following statements correctly summarizes the results of Levene’s test (use \\(\\alpha\\)=.05)?\n\nLevene's test is not significant, no evidence against the assumption of homoscedasticityLevene's test is significant, evidence against assumption of homoscedasticity.\n\nThe researchers conclude:\n“We have convincing evidence that the population means of public participation differs between males and females.”\nIs this a valid conclusion? \nYes\nNo\nReport the test results, then check your answer.\n\n\nExplanation\n\nThe mean level of public participation differs between males (\\(M = 16.54\\)) and females (\\(M = 9.95\\)), \\(t(41) = -4.942, p &lt;.001\\).\n\nWhat is the (absolute) value of the t-statistic? \nFrom the results of the t-test the researchers would conclude that males on average have higher levels of public participation than females.\nBut, of course, we don’t know for sure, because we did not test the entire population. Which error type could the researchers have made? \nType I error \nType II error\nType I or Type II error \n\n16.4.4.1 Examining Income\nIn the previous steps, we used the independent sample’s t-test to test for mean differences in public participation and found a significant result. Results suggested that males show higher levels of public participation. However, males and females may differ not only in gender, but also in other characteristics that explain differences in public participation. In particular, the researchers hypothesize that income differences may play a role as well. Research has shown that income is positively associated with public participation. So if males and females differ in income (on average), the gender differences in public participation may be partly due to income differences.\nIn other words, according to the researchers income may be a \nconfounder\nmoderator\npredictor\ncollider of the effect of gender.\nBefore we proceed, let’s first check whether the groups differ on income. Request the means for the variable income for both males and females separately.\nDo the males and females in the sample differ in average income?\nTake your existing syntax for gender differences in Participation, and change it to test for income differences instead:\nT-TEST GROUPS=Gender(0 1)\n  /MISSING=ANALYSIS\n  /VARIABLES=Income\n  /ES DISPLAY(TRUE)\n  /CRITERIA=CI(.95).\nIs there a difference in income? \nYes\nNo\nThe next step is to study gender differences controlled for income differences. This is not possible via the independent samples t-test interface, but it is possible when specifying our model using regression analyses. Regression analysis allows us to study gender differences controlling for other variables.\n\n16.4.4.2 Add continuous predictor\nBefore we proceed with the regression analysis including income (we will do this in the next assignment), we will first see what the regression model looks like if we would only use gender as the only predictor in the model.\nRun a regression analysis using Public Participation as the dependent variable, and gender as the independent variable (Analyze &gt; regression &gt; linear). Note that the variable gender is already dummy coded.\nInspect the table with Coefficients.\nWhat is the regression slope of gender? \nCompare the value to the mean difference from ; what do you see?\nRecall that the independent samples t-test is the exact same test as the t-test for the regression slope of a dummy variable.\nCompare the value of the t-statistic in the regression with the value of t-statistic in the independent samples t-test (you may ignore the minus signs). What do you see?\nThe regression coefficient of gender is 6.502. This is the same as the difference in means of Public Participation between males and females. The t-statistic of the regression coefficient is 4.942. The t-statistic in the independent samples t-test was -4.942. As we can ignore the minus sign (just make sure to check which group is higher than the other), we can conclude they are equal.\nWhat percentage of the total variance in public participation is explained by gender? \nRecall that the independent samples t-test and regression analysis with a dummy variable predictor are completely equivalent; SPSS just offers different interfaces to the same linear model.\nWe need to use the regression interface when we want to add other predictors (e.g., income) to see if the gender differences are confounded by other characteristics, and we can calculate the R2 value to evaluate the size of the effect of gender.\nNext, we want to know whether there are differences in public participation between men and women with the same income level.\nIn other words: let’s look at gender differences in public participation controlled for income.\nRun a regression analysis of public participation on income and gender (analyze -&gt; regression -&gt; linear). Consult the output and answer the following questions:\nGender and income jointly explain % of the variance in public participation.\nWhich of the following alternatives correctly report the F-statistic testing the significance of the R2? \nF(2,40)=26.845, p &lt; 0.001\nF(2,42)=26.845, p &lt; 0.001\nThe researchers conclude:\n“On average, women score 4.818 points higher on public participation than men.”\nIs this a valid conclusion? \nNo\nYes\n\n\nExplanation\n\nI hope you concluded that the researchers’ conclusion is incorrect.\nThey should have said:\n“On average, women score 4.818 points less on public participation than men of an equal income level”\nSo, if we take two groups of men and women with the same income (= “controlling for income”), then we expect a mean difference of 4.818 in public participation.\n\nIs there evidence of gender differences, while controlling for income differences when testing at \\(\\alpha = .01\\)? \nYes\nNo\nWrite down the complete regression equation, then check your answer below.\n\n\nAnswer\n\nThe regression equation is as follows:\n\\(\\text{Participation}_i = 5.762 + 3.475*\\text{Income}_i+ 4.818 * \\text{Gender}_i+e_i\\)\n\nSuppose we have a man with an income of 3.0 and a woman with an income of 2.0, what is the predicted difference in public participation? \n\n\nExplanation\n\nMan with an income of 3 has a predicted score of:\nPublic Participation′=5.762+3.475∗3+4.818 ∗1\nPublic Participation′=21.005\nWoman with an income of 2 has a predicted score of:\nPublic Participation′=5.762+3.475∗2+4.818 ∗ 0\nPublic Participation′=12.712\nDifference = 21,005 – 12,712\nDifference = 8.293\n\nWe previously computed the mean difference in public participation for men and women, which was 6.50.\nWhy is this previous mean difference not the same as the effect of gender that we find in this most recent regression analysis?\n\n\nAnswer\n\nThe mean difference as observed in the sample is the mean difference without controlling for income differences.\nHowever, as we have seen, men and women also differ in the average income, and income differences between males and females may also explain differences in public participation. In the regression analyses, we controlled for income differences, which means that we “filtered out” the effect of income (the correct way to say it: we partialed out income effects). After controlling for income the effect of gender is somewhat smaller. Hence, the difference in public participation in the sample means is partly due to income differences.\n\nOne researcher argues that there are gender differences in public participation, and gender differences in income, and that income additionally has an effect on public participation.\nIn this theoretical model, income is a \nCommon cause\nModerator\nConfounder\nMediator\n\n16.4.5 Nested Models\nFinally, we might want to know how much unique variance is explained by gender, after controlling for income. To this end, we would perform a nested model test.\nConduct this test as you have done before, using the graphical interface or syntax. Remember: Enter Income first, then Gender - and ask for the \\(R^2\\) change statistics.\nLet’s inspect the Model Summary table.\nWhat percentage of the total variance in Participation is explained by Income (only)? \nHow much of the total variance in Participation is explained by both Income and Gender? \nWhat is the R2-change (in percentage) from Model 1 to Model 2? \nThe Model Summary table also reports the results of the F-tests, which tests whether the change in R2 is significant.\nSuppose three researchers use the output to see whether the R2-change is significant. Researcher I tests at the 10% level, Researcher II tests at the 5% level, and Researcher III tests at the 1% level.\nWhich of the researchers will conclude that there is a significant result? \nOnly researcher III\nOnly researcher II\nAll three researchers\nOnly researcher I\nReport the results of your analysis (include all relevant statistics: test statistics, degrees of freedom, p-values).\nInclude in your answer:\n\nThe R2 of the Small and Full model.\nThe R2-change and whether or not it is significant (use a significance level of .05).\nA substantive interpretation of the R2-change (within the context of this assignment).\n\n\n\nAnswer\n\nIncome explains 39.1% of variance in Participation, which is significantly greater than zero, \\(F(1,41) = 26.32, p &lt; .001\\). Gender explains an additional 18.2% of variance in Participation, and this difference significantly differs from zero, \\(\\Delta R^2 = .18, F(1,40) = 17.06, p &lt; .001\\). The total explained variance in Participation by Income and Gender is 55.2%, which is significantly different from zero, \\(F(2, 40) = 26.85, p &lt; .001\\).\n\n\n16.4.6 One more Categorical Variable\nLet’s finish this assignment by adding one more categorical variable, and seeing whether it has a significant unique effect after controlling for Income and Gender. We will use Education level, which has three categories. We thus have to create two dummy variables to account for this variable.\nFirst, make the dummy variables, using low Education status as reference category.\nRemember that these dummies together code for one variable, so you must add them to the model in the same step together.\nWhat is the F test statistic for the \\(\\Delta R^2\\) test for adding Education to a model that already contains Income and Gender? \nWhat is the difference in expected Participation between someone with Low education status and High education status?",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>GLM-VI: Nested models</span>"
    ]
  },
  {
    "objectID": "glm7interaction.html",
    "href": "glm7interaction.html",
    "title": "17  GLM-VII: Interaction",
    "section": "",
    "text": "17.0.1 Introducing Interactions\nThe foundation of the regression model is the equation that describes the relationship between two variables. In a simple bivariate linear regression, we use the equation:\n\\(Y_i = a + b \\cdot X_i + e_i\\)\nWhere:\nThe regression line provides us with predicted values based on the model. It can be represented as:\n\\(\\hat{Y}_i = a + b \\cdot X_i\\)\nWhere \\(Y^i\\) is the predicted score for individual \\(i\\) on the dependent variable \\(Y\\).\nNow, let’s take our regression analysis to the next level by introducing the concept of interactions. An interaction implies that the effect of one predictor variable depends on the level of another predictor variable.\nTo incorporate interactions, we add a special building block to our regression equation:\n\\(Y = a + b_1X_1 + b_2X_2 + b_3(X_1 \\times X_2)\\)\nTo include an interaction term in your regression model:\nIn summary, interactions add a new layer of complexity to regression analysis by acknowledging that the relationships between variables can be contingent on other factors. By incorporating interactions, we can gain deeper insights into the nuanced dynamics between predictors and outcomes in our statistical models.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>GLM-VII: Interaction</span>"
    ]
  },
  {
    "objectID": "glm7interaction.html#lecture",
    "href": "glm7interaction.html#lecture",
    "title": "17  GLM-VII: Interaction",
    "section": "\n17.1 Lecture",
    "text": "17.1 Lecture",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>GLM-VII: Interaction</span>"
    ]
  },
  {
    "objectID": "glm7interaction.html#formative-test",
    "href": "glm7interaction.html#formative-test",
    "title": "17  GLM-VII: Interaction",
    "section": "\n17.2 Formative Test",
    "text": "17.2 Formative Test\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nAn interaction effect is when…\n\nThe effect of one predictor is added to the effect of another predictor.The effect of two predictors is cumulative.You interact with your partipants.The effect of one predictor depends on the value of another predictor.\n\nQuestion 2\nWhat is the purpose of centering continuous predictors in interaction analysis?\n\nCentering continuous predictors is necessary for regression analysis to provide correct results.Centering continuous predictors prevents multicollinearity.Centering continuous predictors increases interpretability.Centering continuous predictors helps in enhancing interpretability and reducing multicollinearity.\n\nQuestion 3\nWhen two continuous predictors interact, how many unique regression lines are generated?\n\nTwo unique regression lines are generated, one for each predictor.Only one unique regression line is generated regardless of the interaction.The number of unique regression lines depends on the number of unique values of the predictors.A theoretically infinite number of unique regression lines, as both predictors can take on an infinite number of values.\n\nQuestion 4\nWhat is the concept of simple slopes in interaction analysis?\n\nSimple slopes are the partial effects of two predictors, controlling for their interaction effect.Simple slopes are the slopes of predictors before they are centered.Simple slopes refer to the linear effects of a predictor on the outcome variable.Simple slopes refer to the effect of one predictor variable, evaluated at specific levels of another variable.\n\nQuestion 5\nIn a multiple regression model with two continuous predictors, how would you assess the effect of one predictor at different levels of the other predictor?\n\nBy standardizing the predictors.By excluding the interaction term from the model.By centering one predictor at a specific level and re-computing the interaction term.By computing the mean of both predictors.\n\nQuestion 6\nWhat does it mean when an interaction term in a regression model is not significant?\n\nAn insignificant interaction term indicates that the model is overfitting.When an interaction term is not significant, it suggests that the effect of one predictor variable on the outcome variable is consistent across all levels of the other predictor variable.An insignificant interaction term means that the outcome variable is not related to the predictor variables.An insignificant interaction term indicates a problem with the data collection process.\n\nQuestion 7\nWhat is the predicted value for an individual with a score of 2.5 on X1 and score of 35 on X2, given the following regression equation: Y = 10 + 2 * X1 - 0.5 * X2 + 0.1 * (X1 * X2)?\n\n6.25-3.75-2.48.75\n\nQuestion 8\nWhat is the effect of a one-unit increase in X1 on the predicted value for an individual who scores 40 on X2, given the following regression equation: Y = 8 + 1.2 * X1 - 0.3 * X2 + 0.05 * (X1 * X2)?\n\n3.21.50.11.2\n\nQuestion 9\nAssume that X1 and X2 are centered around the mean. Given the following regression equation: Y = 6 + 1.5 * X1 - 0.4 * X2 + 0.08 * (X1 * X2), what is the simple slope of X1 on Y for people who score 1SD above the mean on X2, if the SD of X2 is 0.5?\n\n1.581.51.541.46\n\n\n\n\nShow explanations\n\nQuestion 1\nAn interaction effect involves the combined influence of two or more predictor variables on the outcome variable, which is different from their individual effects.\nQuestion 2\nThe two purposes of centering are to avoid artificial multicollinearity between the interacting variables and their product, and aids the model’s interpretability.\nQuestion 3\nIn interactions involving continuous predictors, the relationship between the predictors and the outcome variable can vary infinitely, leading to an infinite number of possible regression lines.\nQuestion 4\nSimple slopes allow us to assess how the relationship between two predictors changes at different levels of the moderator variable, helping to understand conditional effects.\nQuestion 5\nCentering and re-computing the interaction term allows us to obtain the slopes of the predictor of interest at different levels of the moderator, helping us understand its conditional effects.\nQuestion 6\nWhen an interaction term is not significant, it implies that the relationship between the predictors and the outcome remains relatively constant regardless of the values of the interacting predictors.\nQuestion 7\nPlug in the values: Y = 10 + 2 * 2.5 - 0.5 * 35 + 0.1 * (2.5 * 35)\nQuestion 8\nYou have to add the interaction term to the effect of X1; when X2 has the value 0, the effect of X1 is 1.2. When X2 has the value 40, add 0.05*40 to that effect.\nQuestion 9\nCalculating the simple slope works just the same as calculating the effect of X1 for specific values of X2, so you calculate the effect for +1 SD of .05: 1.5X1 + 0.08.5X1 = 1.54X1.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>GLM-VII: Interaction</span>"
    ]
  },
  {
    "objectID": "glm7interaction.html#in-spss",
    "href": "glm7interaction.html#in-spss",
    "title": "17  GLM-VII: Interaction",
    "section": "\n17.3 In SPSS",
    "text": "17.3 In SPSS\n\n17.3.1 Multiple Regression",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>GLM-VII: Interaction</span>"
    ]
  },
  {
    "objectID": "glm7interaction.html#tutorial",
    "href": "glm7interaction.html#tutorial",
    "title": "17  GLM-VII: Interaction",
    "section": "\n17.4 Tutorial",
    "text": "17.4 Tutorial\n\n17.4.1 Interaction\nIn this assignment we work with the PublicParticipation.sav data. It contains (fictional) data on the following variables: income (higher scores, more income), public participation, education, age, and gender (0 = females; 1 = males). Public participation involves being member of school boards, municipal councillor, etc.\nIn this assignment we will see how we can model interaction between a continuous predictor and a dichotomous predictor.\nSuppose we are interested in relationship between age and public participation, and we want to know if the relationship is moderated by gender. An interaction model is conceptually represented as follows (these two diagrams are interchangeable):\n\n\n\n\n\n\n\n\n\n\nModeling Interactions\nThe regression model for testing the interaction is:\n\\(Y' = b_{0} + b_{1}X + b_{2}D_{g} + b_{3}XD_{g}\\)\nwhere X = age, and D_g = gender (0 = women; 1 = men). Notice that women are our reference group.\nTo model interaction we need to create a new variable, which is the product of the dummy variable (gender in our case) and (age in our case).\nThis is best done via syntax, but to use the graphical interface proceed as follows:\nvia Transform &gt; compute variable\nGive the new variable a name (i.e., the target variable), say GenderTAge.\nThen specify the product at the right (see more information button). Click on Paste, select and run the code. Check in Data View whether the product term was added correctly.\nAlternatively, the syntax is:\nCOMPUTE GenderTAge = Gender * Age.\nEXECUTE.\nNow run the regression analysis that includes the interaction effect.\nImportant: Just like with dummies you must include all dummies that belong to the same variable in the model together, with an interaction term, you must always include its constituent variables as well. This is because the interaction term only modifies the effect of its constituent variables; the effect of those constituent variables must thus also be in the model.\nSo, if you add variable intXTZ into the model, you must also include X and Z.\nVia analyze &gt; regression &gt; linear; choose age, gender and GenderTAge as the independent variables, and public participation as the dependent variable.\nConsult the table Regression coefficients. Write down the general estimated model.\nFinish the following equation, then check your answer.\n\\(\\text{Public Participation' = .....}\\)\n\n\nAnswer\n\n\\(\\text{Public Participation}′ = 3.252 + 0.137*\\text{Age} + 12.439*\\text{Gender} − 0.116*\\text{Gender}*\\text{Age}\\)\n\nNow write down the estimated models down for women and men separately. Hint: fill in 0 and 1 in the general estimated model mentioned in the previous step, then simplify the formula.\nComplete the equations for women (W) and men (M):\n\\(\\text{PP}_W'=\\) \\(+\\) \\(*\\text{Age}\\)\n\\(\\text{PP}_M'=\\) \\(+\\) \\(*\\text{Age}\\)\nNow draw (on a piece of paper) a graph of the results. That is, put age on the x-axis, the predicted public participation on the y-axis, and draw separate regression lines for males and females.\nTrue or false\nIn the sample, age has a positive effect on public participation for women but a negative effect for men? \nTRUE\nFALSE\nThe researchers tested at the 5% level and concluded:\n“We have convincing evidence that the population effect of age on public participation is different for men and women.” \nTRUE\nFALSE\nThe estimated regression model was:\n\\(Y'= 3.252 + 0.137Age + 12.439D_g - 0.116(Age \\times D_g)\\)\nWhat would the regression equation look like if we would have used the men as the reference group? Use logic to answer this question, instead of re-running the analysis.\n\\(Y'=\\) \\(+\\) \\(*\\text{Age}+\\) \\(*D_g+\\) \\(*(\\text{Age} \\times D_g)\\)\n\nTo verify our answer to the previous question, we will recode the variable Gender such that males are scored 0 (= reference group) and females are scored 1.\nProceed as follows:\n\nvia Transform &gt; Recode into different variables\nSelect Gender.\nGive a name to the new output variable (say GenderFem), give a label (say: “Gender (ref=males)” click on change.\nSpecify old and new values: old value 0 becomes 1 and old value 1 becomes 0 (don’t forget to click on add in between).\nClick OK. Verify that SPSS added a new column with a dummy variable where males are the reference group.\nCompute the product variable for the interaction between age and gender but now use the dummy having males as reference group.\nRerun the regression analysis, but now using the new gender variable and interaction term. If you’re answer in the previous step is correct you should find the values back in the table Regression Coefficients.\n\n17.4.2 Categorical Predictors with Three or more Categories\nThe categorical predictor Education has three levels (low, middle, high). If we want to include such a variable we need to use dummies.\nCode the dummy variables as follows:\n\n\nValue\nD1\nD2\n\n\n\nLow\n0\n0\n\n\nMiddle\n1\n0\n\n\nHigh\n0\n1\n\n\n\nWhich group is the reference group according to this coding? \nLow\nMiddle\nHigh\nUse syntax to create the dummies.\nWe are now ready for the regression analysis.\nRun a hierarchical regression analysis with public participation as dependent variable. Model 1 only includes age. Model 2 includes age and the dummies. So we have the following nested models:\nThis model does not include the interaction effects yet! This means that we assume that the regression lines are parallel to one another. In the next assignment we check whether this assumption is reasonable.\nProceed as follows:\n\nvia analyze &gt; regression &gt; linear.\nSelect public participation as the dependent variable and only age as the independent variable. Click on next.\nNow select the two dummies we have created in the previous step. The two dummies together represent education. Always enter dummies into the model together!\nVia Statistics ask for the R-change statistics.\n\nConsult the output and answer the questions in the next few steps.\nEducation and age together explain  % of the total variance.\nWhat is the value of the test statistic that tests the unique effect of education, controlled for age? \nReport the results for the unique effect of education, then check your answer.\n\n\nAnswer\n\nEducation does not have a significant unique effect on public participation after controlling for age, \\(\\Delta R^2 = .04, F(2,38) = 0.895, p = .417\\).\n\nConsult the table with the regression coefficients.\nWrite down the estimated regression equation of Model 2.\n\n\nAnswer\n\n\\(PublicParticipation'\\:=\\:10.478\\:+\\:.097\\:\\cdot \\:Age\\:-\\:2.042\\:\\cdot \\:D1\\:-\\:3.071\\:\\cdot \\:D2\\)\n\nWrite down the estimated model for each of the three groups.\nThen make a graph of the regression equations. Put age on the x-axis, the predicted public participation on the y-axis, and draw the lines for each education group.\n\n\nAnswer\n\nThe models were:\n\\(PP'_l = 10.478 + .097Age\\)\n\\(PP'_m = (10.478-2.042) + .097Age = 8.436 + .097Age\\)\n\\(PP'_h = (10.478-3.071) + .097Age = 7.407 + .097Age\\)\nDid you get it right?\n\nSuppose we have two persons, both are 40 years old, but one had middle level education and the other had high-level education.\nWhat is the expected (absolute) difference in public participation between these two persons? \nThe researchers conclude:\n“Controlled for age, low educated people in the sample show highest level of public participation”.\nIs this a valid conclusion? \nTRUE\nFALSE\n\n17.4.3 Interaction with more than Two Categories\nIn the previous assignment, we assumed that the effect of Age on Public participation was equal for each of the education level groups. However, we do not know whether this assumption is reasonable. In this assignment, we will check whether the interaction effect between Age and Education level is statistically significant or not.\nCreate the two interaction terms using syntax, with the Compute variable command. Note that we need two interaction terms: D1Tage and D2Tage.\nWe are now ready for the regression analysis.\nRun a hierarchical regression analysis. Model 1 only includes age and the two dummy variables. Model 2 additionally includes the interaction terms.\nWrite down the formulas for the two nested models, then check your answer.\n\n\nAnswer\n\n\nModel 1: \\(Y'= b_0 + b_1Age + b_2D_1 + b_3 D_2\\)\n\nModel 2: \\(Y'= b_0 + b_1Age + b_2D_1 + b_3 D_2 + b_4D_1Age + b_5 D_2Age\\)\n\n\n\nProceed as follows (or, preferably, use syntax):\n\nvia analyze &gt; regression &gt; linear.\nSelect public participation as the dependent and age, D1 and D2 as the independent variables. Click on next.\nNow select the two interaction terms we have created in the previous step. The two interaction terms together represent the interaction effect between education and age.\nVia Statistics ask for the R-change statistics.\n\nConsult the output and answer the questions in the next few steps.\nBefore we carry out any of the significance tests, let’s take a look at the coefficients table. Look at the unstandardized coefficients in Model2. First, write down the entire estimated model.\nComplete the following equation:\n\\(Y'=\\) \\(+\\) \\(*Age+\\) \\(*D_{middle}+\\) \\(*D_{high}+\\) \\(*(D_{middle}*Age)+\\) \\(*(D_{high}*Age)\\)\nNext, write down the estimated model for each of the three education groups.\nRemember, fill in 0 and 1 for the dummy variables, then simplify:\n\\(Y_{low}'=\\) \\(+\\) \\(*Age\\)\n\\(Y_{middle}'=\\) \\(+\\) \\(*Age\\)\n\\(Y_{high}'=\\) \\(+\\) \\(*Age\\)\nNow, answer the following questions.\nTrue or False?\nThe effect of Age on Publication Participation in the sample is positive for all education groups. \nTRUE\nFALSE\nFor which group is the effect of Age on publication participation the strongest? \nLow\nMiddle\nHigh\nWe inspected the estimated model. But is there a significant interaction effect to begin with? To answer that question we inspect the Model Summary Table.\nFirst of all, write down the \\(R^2\\) for the model without- and with interactions. What do these numbers mean?\nWithout interactions:  With interactions: \nFinish the following sentence:\nModel 2 with the interaction effects explains an additional  % of the variance in Public Participation compared to Model 1 (on top of what was already explained by the main effects of Age and Education).\nWe will now carry out the F-change test. Write down the null hypothesis and alternative hypothesis that we test with this F-change test.\n\n\nAnswer\n\n\\(H_0:\\:R^2\\:=\\:0\\)\n\\(H_1:\\:R^2 \\ne 0\\)\n\nWrite down the F-value, the df and the p-value.\n\nF-value: \n\ndf: ( ,  )\np-value: \n\n\nTrue or false: there is a significant interaction effect: \nTRUE\nFALSE\nTrue or False: As a follow-up analysis, we should perform a simple effects analysis. \nTRUE\nFALSE\nInterpret the results of Model 1 (without interaction) and report your results.\n\n\nAnswer\n\nThere is no evidence for a significant effect of Age and Education on Participation, \\(R^2 = .10, F(3, 38) = 1.36, p = .27\\).\n\n\n17.4.4 Interaction Effects\nIn this assignment, you will examine whether the effect of relationship with coworkers (sccowork; higher score = better relationship) on the emotional pressure at work (scemoti) has an interation effect with gender (0 = male, 1 = female).\nIf there is an interaction effect, the effect of sccowork on scemoti depends on the value of the variable gender.\nOpen Work.sav.\nTo be able to examine the interaction effect, you should first create a product variable.\n\nGo to Transform &gt; Compute Variable\nGive a name to the new product variable in Target Variable (GenderTRelco for example).\nIn Nummeric Expression you need to specify how the new variable should be computed. You have to enter gender * sccowork to compute the product of gender and sccowork.\nPaste and run the syntax, and check whether the product variable was added\n\nConduct a multiple regression analysis (using Analyze &gt; Regression &gt; Linear) with scemoti as dependent variable. The independent variables are the main effects (gender and sccowork) and the interaction effect (genderTsccowork).\nWhat is the p-value of the interaction effect? \nTrue or false: The interaction effect is significant at \\(\\alpha = .10\\) \nTRUE\nFALSE\nThe regression equation for the entire sample is:\n\\(\\text{scemoti}'=\\) \\(+\\) \\(*\\text{Gender}+\\) \\(*\\text{Relationship}+\\) \\(*(\\text{Gender}*\\text{Relationship})\\)\nFor males, the value of Gender is 0. That means that GenderTRelco is also 0. The regression equation for males then becomes:\n\\(\\text{scemoti}'=\\) \\(*\\text{Relationship}\\)\nFor females, the value of Gender is 1. What is the regression equation for females?\n\\(\\text{scemoti}'=\\) \\(+\\) \\(*\\text{Relationship}\\)\nDraw (on paper, not in SPSS) a schematic graph of the interaction effect. Put relationship with coworkers on the X-axis, and emotional pressure on the Y-axis. Draw a schematic regression line for each group.\nIn what group is the effect of relationship with coworkers on emotional pressure the strongest: males or females? \nMales\nFemales\nIn practice, you’d often want to know whether the effects within the groups are significant.\nCan you use the output of this regression analysis to draw conclusions about the significance of the effect within each group? \nNo\nYes, but only for the group of males\nYes, but only for the group of females\nYes, for both groups\nAt this moment, we don’t have enough information in the output yet to test the effect within the female group. But we can test the effect within the male group!\nWhat is the p-value of the effect of sccowork on scemoti within the male group? \nTo test the significance within the the group of females, we can simply switch the reference groups.\n\nMake a new dummy variable called male, on which males score 1, and females 0\nCompute a new product variable: COMPUTE maleTsccowork = male * sccowork.\n\n\nPerform a new regression analysis with these predictors. This is exactly the same analysis, but now with women as reference group instead of men.\nLook at the table with the estimated coefficients. What is the p-value of the effect of sccowork on scemoti within the female group?",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>GLM-VII: Interaction</span>"
    ]
  },
  {
    "objectID": "logistic.html",
    "href": "logistic.html",
    "title": "18  GLM VIII - Logistic Regression",
    "section": "",
    "text": "18.0.1 Introducing the logit\nWhen dealing with binary dependent variables (nominal or ordinal), we could model the probability of observing the outcome (coded as 0 or 1) with a conventional linear regression model. The problem with this approach is that linear regression predicts probabilities outside the range of [0, 1], and will have heteroscedastic and non-normal residuals because there are only two discrete values for the dependent variable.\nLogistic Regression overcomes these limitations of linear regression. The core idea behind logistic regression is to predict a transformation of the dependent variable, Y, rather than the raw scores. Specifically, we model the log odds of the probability of Y being one category (e.g., 1) versus the other category (e.g., 0). The logit function, denoted as log(p/(1-p)), models the probability p using an s-shaped curve bounded by 0 and 1. Furthermore, logistic regression assumes a Bernoulli error distribution, instead of a normal error distribution, which accounts for the fact that observed outcomes can only take the values 0 or 1.\nLogistic regression predicts the logit function of the individual probabilities of observing the outcome, pi. The logit of the probability (pi) is given by log(pi/(1-pi)), ensuring that the predicted values remain within the valid probability range. The outcome, Yi, is assumed to follow a Bernoulli distribution with an individual probability of success, pi. The logit of this success probability is modeled as a linear function of the predictors, allowing us to use the familiar linear regression model to predict the logit of pi.\nUnderstanding the distinction between probability, odds, and the logit is crucial in logistic regression. Probability is defined as the long-run proportion of outcomes of a random experiment in which a particular outcome is observed. Odds describe the ratio of the probability of an event occurring relative to the probability of it not occurring. Finally, the logit transforms odds into a linear function, enabling us to use the regression model. The transformations from probability to odds and logit, and back, are given by:",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>GLM VIII - Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#maximum-likelihood-estimation-mle",
    "href": "logistic.html#maximum-likelihood-estimation-mle",
    "title": "18  GLM VIII - Logistic Regression",
    "section": "\n18.1 Maximum Likelihood Estimation (MLE)",
    "text": "18.1 Maximum Likelihood Estimation (MLE)\nIn traditional linear regression, we use “ordinary least squares” (OLS) estimation to obtain the model parameters. This method involves simple matrix algebra and always yields a unique solution. However, for logistic regression, there is no OLS solution due to the binary nature of the dependent variable. Instead, we turn to Maximum Likelihood Estimation (MLE). A complete explanation is beyond the scope of this course, but here is a basic intuitive explanation of the procedure:\n\nStart with random values for the coefficients (a and b)\nUsing those parameter values, calculate the individual probabilities predicted by the logistic regression formula\nFor each individual, calculate the likelihood of observing their true outcome in a Bernoulli distribution with model-implied probability pi\nMultiply these probabilities across all individuals to get the overall likelihood of observing these data, given the chosen coefficient values\nAdjust the values of a and b a little bit\nCheck if the likelihood has become larger\nRepeat steps 2-6 until until we find the coefficient values that maximize the likelihood and no further improvement can be found.\n\nIn other words, we look for the values of a and b that maximize the likelihood of observing the observed outcome values.\n\n18.1.1 Interpreting Coefficients\nThe intercept (a) represents the log odds of the outcome (Y) for someone who scores 0 on all predictors. We can convert this log odds to the probability of the outcome for an individual who scores 0 on all predictors using the formula P = e^(a) / (1 + e^(a)). We can also solve for the inflection point at which the model stops predicting 0 and starts predicting 1, or vice versa, using \\(X_{p = .5} = \\frac{-a}{b}\\).\nThe slope (b) of the logistic regression equation determines how steeply the logistic function switches from predicting 0 to predicting 1 as the predictor variable (Xi) increases. Larger absolute values of b indicate a steeper transition between the two outcomes. If the slope is positive, the function ascends (starts at 0 and goes to 1), resulting in an S-shaped curve. Conversely, if the slope is negative, the function descends (starts at 1 and goes to 0), resulting in a Z-shaped curve.\n\n18.1.2 Odds Ratio\nThe odds ratio is another important concept when interpreting logistic regression coefficients. It represents the odds of the outcome occurring given a one-unit increase in the predictor variable (Xi), relative to the odds of the outcome occurring when Xi remains unchanged. For binary predictors (e.g., conditions), the odds ratio provides a sensible effect size. For continuous predictors, the odds ratio is a multiplyer by which the odds increase when the predictor increases by one unit.\nTo calculate the odds ratio for a logistic regression coefficient (b), we use the formula OR = e^(b). A value greater than 1 indicates that the predictor is associated with higher odds of the outcome, while a value less than 1 indicates lower odds of the outcome. For example, if the odds ratio for the test score coefficient (b = 2.12) is 8.35, it means that for each unit increase in the test score, the odds of the outcome are multiplied by 8.35.\n\n18.1.3 Model Fit\nTo assess how well the logistic regression model fits the data, we can use the likelihood obtained from maximum likelihood estimation (MLE). By multiplying the log likelihood by -2, we obtain the \\(-2LL\\), which is a chi-square distributed test statistic. Performing a chi-square test allows us to determine if the overall model is significant. The null hypothesis is that the model does not significantly differ from a model with no predictors. If the chi-square test is significant, it indicates that the model provides a better fit than a null model.\n\n18.1.4 Likelihood Ratio Test\nIn logistic regression, we can also conduct a Likelihood Ratio (LR) test, which is a chi-square test for the difference in log likelihood between two nested models, \\(-2LL_0 - -2LL_1\\). The first model is the restricted model with fewer parameters, and the second is the full model with more parameters. The LR test helps us compare the two models and determine if the additional predictors in the full model significantly improve its fit. The degrees of freedom for the LR test are equal to the difference in the number of parameters between the two models.\n\n18.1.5 Pseudo R2\nUnlike linear regression, logistic regression doesn’t have a traditional R-squared to measure explained variance. However, researchers have proposed several Pseudo R2 statistics to approximate the concept of explained variance in logistic regression. These Pseudo R2 statistics rescale the -2 log likelihood of the model. Two common Pseudo R2 statistics are Cox & Snell and Nagelkerke.\nCox & Snell is a generalization of the “normal” R2, which provides the same value for ordinary least squares regression. For logistic regression, however, its value can never reach 1; it will be somewhere between 0 and &lt; 1. Nagelkerke aims to “fix” this property by rescaling Cox & Snell to a range of [0, 1], by dividing it by its maximum possible value. While these statistics can provide a measure of relative model fit and help compare models on the same dataset, they do not represent absolute model fit or effect size.\n\n18.1.6 Classification Accuracy\nOne way to evaluate the model’s predictive performance is by using a classification table. The classification table compares the predicted outcomes with the actual outcomes to determine how well the model predicts true positives and true negatives. The table is constructed by calculating the predicted probability for each individual and then dichotomizing these probabilities using a specific cutoff, typically 0.5. Individuals with predicted probabilities above the cutoff are classified as “1,” and those below as “0.” The observed outcomes are then cross-tabulated against the dichotomized predictions.\nBy examining the classification table, we can assess the accuracy of the model’s predictions and identify areas of improvement. Researchers could choose a different cutoff to optimize the trade-off between false positives and false negatives, depending on the specific goals of the analysis.\nIn summary, evaluating logistic regression models involves assessing model fit through chi-square tests, using Pseudo R2 statistics for relative fit comparison, and examining classification accuracy to understand the model’s predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>GLM VIII - Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#lecture",
    "href": "logistic.html#lecture",
    "title": "18  GLM VIII - Logistic Regression",
    "section": "\n18.2 Lecture",
    "text": "18.2 Lecture",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>GLM VIII - Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#formative-test",
    "href": "logistic.html#formative-test",
    "title": "18  GLM VIII - Logistic Regression",
    "section": "\n18.3 Formative Test",
    "text": "18.3 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat type of dependent variable is suitable for logistic regression?\n\nCategoricalBinaryOrdinalInterval\n\nQuestion 2\nWhat is the primary purpose of maximum likelihood estimation in logistic regression?\n\nTo maximize the likelihood of observing the data given the modelTo minimize the sum of squared errorsTo maximize the variance explained by the predictorsTo maximize the parameter values\n\nQuestion 3\nHow is the likelihood ratio test used in logistic regression?\n\nTo test for normality of residualsTo assess the effect size of predictorsTo check for multicollinearityTo compare the fit of nested models\n\nQuestion 4\nWhich of the following is a Pseudo R2 statistic commonly used in logistic regression?\n\nAdjusted R2Pearson's R2Cox & Snell R2-2LL\n\nQuestion 5\nWhat does the logit function do in logistic regression?\n\nConverts continuous predictors to categoricalStandardizes the dependent variableCalculates the Wald test statisticTransforms the predicted probabilities to log odds\n\nQuestion 6\nWhat is the range of the Cox & Snell Pseudo R2 statistic in logistic regression?\n\n0 to 10 to &lt; 10 to Infinite-Infinite to 1\n\nQuestion 7\nHow is the classification table used in logistic regression evaluation?\n\nTo test for multicollinearityTo assess the model's predictive accuracyTo determine effect size of predictorsTo compare model fit using likelihood ratio test\n\nQuestion 8\nWhat does a significant chi-square test in logistic regression indicate?\n\nThe model's residuals are normally distributedThe model provides a better fit than a null modelThe model's predicted probabilities are accurateThe model's predictors are collinear\n\nQuestion 9\nWhich parameter represents the odds ratio associated with a one-unit increase in the predictor?\n\nThe logit functionThe exponent of the coefficient b, e^bThe p-valueThe coefficient b\n\nQuestion 10\nWhen calculating the likelihood in logistic regression, what does a high value of likelihood imply?\n\nThe observed outcome values are very likely given the parametersThe model's residuals are normally distributedThe model has a high R-squared valueThe model is overfitting the data\n\nQuestion 11\nWhat is the main purpose of the likelihood ratio test in logistic regression?\n\nTo compare model fit between two nested modelsTo test the significance of individual predictorsTo assess the normality of residualsTo evaluate multicollinearity among predictors\n\nQuestion 12\nWhat can help researchers optimize the trade-off between false positives and false negatives in logistic regression?\n\nThe classification tableThe odds ratioThe -2LLThe pseudo R2\n\nQuestion 13\nWhat is the range of the Nagelkerke Pseudo R2 statistic in logistic regression?\n\n0 to &lt; 10 to 1-Infinity to 1-1 to 1\n\n\n\n\nShow explanations\n\nQuestion 1\nLogistic regression is used for binary categorical outcomes.\nQuestion 2\nMaximum likelihood estimation aims to maximize the likelihood of observing the data given the model.\nQuestion 3\nThe likelihood ratio test compares the fit of nested models to determine if additional predictors significantly improve the model.\nQuestion 4\nCox & Snell R2 is a Pseudo R2 statistic used in logistic regression.\nQuestion 5\nThe logit function transforms the predicted probabilities to odds.\nQuestion 6\nThe Cox & Snell Pseudo R2 statistic never reaches 1 for logistic regression, so it ranges from 0 to &lt; 1.\nQuestion 7\nThe classification table helps assess the model’s predictive accuracy by comparing predicted outcomes with actual outcomes.\nQuestion 8\nA significant chi-square test indicates that the model provides a better fit than a null model.\nQuestion 9\nThe exponent of the coefficient b represents the odds ratio associated with a one-unit increase in the predictor.\nQuestion 10\nA high value of likelihood indicates that the observed outcome values are very likely given the parameters.\nQuestion 11\nThe likelihood ratio test is used to compare model fit between two nested models.\nQuestion 12\nThe classification table helps researchers optimize the trade-off between false positives and false negatives in logistic regression.\nQuestion 13\nThe Nagelkerke Pseudo R2 statistic rescales the Cox & Snell statistic to range from 0 to 1.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>GLM VIII - Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#tutorial",
    "href": "logistic.html#tutorial",
    "title": "18  GLM VIII - Logistic Regression",
    "section": "\n18.4 Tutorial",
    "text": "18.4 Tutorial\n\n18.4.1 Probability, Odds, and Logits\nWe will start this session on logistic regression with some theoretical exercises. This way, you will learn how to work with probability, odds, and logits.\nThe given probability is P = 0.36.\nWhat are the corresponding odds? \nPlease find the formula’s below:\n\n\nGoal\nFunction\n\n\n\nProbability -&gt; Odds\n\\(odds = P/(1-P)\\)\n\n\nOdds -&gt; Probability\n\\(P = odds/(1+Odds)\\)\n\n\nOdds -&gt; Logit\n\\(logit = \\ln(odds)\\)\n\n\nLogit -&gt; Odds\n\\(odds = e^{logit}\\)\n\n\n\nAgain, the given probability is P = 0.36.\nWhat is the corresponding logit? \nThe given logit is – 2.7.\nWhat are the corresponding odds? \nAgain, the given logit is – 2.7. \nDiscuss with your group mates when and why we should carry out logistic regression analysis.\n\n\nExplanation\n\nWe carry out logistic regression analysis when we want to carry out regression analysis and we have a dichotomous outcome variable (i.e., a dependent variable with two answer categories). In that case we cannot use linear regression because several of the assumptions of linear regression are violated.\n\nIn a study concerning the smoking behavior amongst adolescents, a logistic regression analysis is conducted to check the effect of the image of smoking (Image) on smoking behavior (Smoking).\nImage: to what degree the young adult thinks smoking is perceived as “cool”. The variable image is measured on a scale from 10 to 30, in which higher scores indicate that smoking is perceived as cooler.\nSmoking: whether or not the adolescent smokes.\n\n0 = the adolescent is a non-smoker\n1 = the adolescent is a smoker\n\nBelow you can find part of the output the researchers retrieved.\n\nFirst, write down the estimated regression model.\n\n\nAnswer\n\n\\(Logit(Smoking = 1) = -5.65 + 0.28*Image\\)\n\nWhat is the probability that an adolescent with a score of 15 on image smokes? \nTake a look at the regression coefficient.\n\nImagine that one’s score on Image will increase from 15 to 16.\nTo what degree will the logit and odds change?\nAnd what can you conclude about the increase in probability?\n\n\nAnswer\n\nThe logit increases with 0.284. So −1.387+0.284=−1.103\nThe odds increase with a factor 1.328. So, the odds become 0.25×1.328=0.332\nBy just looking at the output, it is unclear what exactly will happen with the probability. We DO know that the probability will increase because the logits and the odds increase.\nIf we would calculate the probability by hand, we would see that the probability increases with 0.05.\n\nAs you can see, the logit follows a linear function, the odds follow an exponential function, and probability follows a logistic function (perhaps less clear from the example but see the graphs below for an illustration). It is nice to work with a linear model (i.e., work with the logits), but from an interpretation point of view odds and probabilities are nicer to work with.\n\n\n18.4.2 Logistic Regression\nToday we will study whether the probability to pass a Statistics course depends on exam fear and the math grade obtained in high school (prior math ability).\nOpen LAS BE LR.sav.\nIn the table below you find the variables included in this fictional data set.\nWe want to study the following research question:\nDoes the probability of passing the statistics exam depend on exam stress and math ability?\nTo answer this question, we first need to dichotomize the dependent variable. This is an unusual step, because dichotomizing variables loses information! Still, for the purpose of the present exercise, we will do so. We will create the new dependent variable Passed (0 = fail, 1 = pass), where a 5.5 or higher counts as a pass. Use the following steps:\nNavigate to Transform → Recode into different variables. Select GradeStats as input variable and use Passed as the name for the new recoded variable. Do not forget to click on CHANGE. Click on Old and new values and specify the recoding in such a way that all grades of 5.5 or higher will get the new value 1 and all grades lower than 5.5 (i.e., all other grades) the value 0. Paste and run the syntax. Go to variable view and specify the value labels.\nObtain the frequency distribution for Passed (via Analyze –&gt; Descriptive Statistics –&gt; Frequencies).\nWhat percentage of students passed? %\nWe will use logistic regression to study the effect of exam stress and math ability on the probability to pass. Take the following steps:\nNavigate to Analyze –&gt; Regression –&gt; Binary logistic regression Select Passed as dependent variable and ExamStress and Math as independent variables (in SPSS called “covariates”). Paste and run the syntax.\nInspect the output corresponding to Block 1. Take a look at the table “Variables in the Equation”.\nWhat would be good description of the effects on the sample level? (i.e., what the effect of ExamStress and Math on the probability to pass looks like).\n\n\nAnswer\n\nTurns out Exam Stress has a negative effect on the probability to pass (controlled for Math grade), and Math a positive effect (controlled for Exam Stress).\n\nWhat is the value of the regression coefficient for the variable Exam Stress? \nGive a detailed interpretation of this number.\n\n\nAnswer\n\nIf we would increase one unit in Exam Stress the logits to pass the exam will decrease with 0.025, while keeping the variable Math constant.\n\nTrue or false: The independent variables Math and Exam Stress together have a significant effect on the probability to pass. \nTRUE\nFALSE\n\n\nExplanation\n\nWhen we inspect the model test, we see that, together, Exam Stress and Math grade have a significant effect on the probability to pass, χ2(2) = 81.043, p &lt; .001.\n\nSecond, carry out the significance tests for the individual predictors as well.\nTrue or false: There is a significant effect of Math on the probability to pass, controlled for Exam stress. \nTRUE\nFALSE\nTrue or false: There is a significant effect of Exam stress on the probability to pass, controlled for Math. \nTRUE\nFALSE\n\n\nExplanation\n\nWhen we inspect the separate regression coefficients together, we see that:\nControlled for Exam stress, Math grade has a significant effect on the probability to pass, χ2(1)=54.500, p&lt;.001. Controlled for Math grade, Exam stress has no significant effect on the probability to pass, χ2(1)=0.040, p=.841.\n\n\n18.4.3 Logistic Regression with Categorical Predictor\nNow, we will carry out a logistic regression analysis to study whether the probability to pass can be predicted from the amount of preparation (Preparation), controlled for prior math ability (Math).\nBut… the variable Preparation is a nominal (categorical) variable. Use dummy coding to include it in the model. The variable Preparation has three categories. Create all three dummy variables!\nTake the following approach to carry out the analysis.\nNavigate to Analyze –&gt; Regression –&gt; Binary Logistic\nSelect Math as covariate.\nSelect the dummies for Preparation as covariates. At this stage, use “only reading the book” as the reference group.\nPaste and run the syntax.\nTake a look at the table Variables in the Equation.\nInspect the estimated regression coefficients. Consider the results on the sample level, ignoring inferential statistics for now.\nWhich group has the highest estimated probability to pass? \nStudent who only read the book.\nStudents who read the book and did the exercises.\nStudents who only did the exercises.\nWhich group has the lowest estimated probability to pass (controlled for math ability)? \nStudents who only did the exercises.\nStudents who read the book and did the exercises.\nStudent who only read the book.\nWrite down the full model equation, filling in the values of coefficients. Then check your answer.\n\n\nAnswer\n\n\\(logit_{passed} = -5.833 + .300*D_{exercises} + .946*D_{both} + .999 * Math\\)\n\nControlled for Math Grade, what is the difference in predicted odds between people who only read the book and people who read the book and made the exercises? \n\n\nExplanation\n\nTake the exponent of the regression coefficient for the dummy for people who read the book and made the exercises (see table).\n\nBy hand, calculate the probability that the third person in the file (respID=3) passes the exam. \nImagine that you do not know whether this person passed the exam or not.\nTrue or false: Based on your answer in the previous step and using a decision threshold of .5, you would predict this student to pass. \nTRUE\nFALSE\nSPSS can easily calculate the probability to pass.\nEither use the visual interface:\nClick on the SAVE button in the menu for logistic regression.\nSelect Probabilities under the header Predicted Values.\nClick on continue.\nOr add this line of code to your model:\n  /SAVE=PRED\nSPSS adds a new variable to the data file, which gives the predicted probability for each person. Check whether you calculated the predicted probability for person 3 correctly.\nImagine that we would have used this model to predict the probability to pass for the students in this sample before they even made the exam. Use a decision threshold of .5 to classify students as those likely to pass vs fail.\nOut of all students that were classified as “pass”, which proportion actually failed? \n\n\nExplanation\n\nOf the 370 students who were predicted to pass, 52 failed the exam (.141). As you can see, the model is not completely flawless in making predictions.\n\nWhat is worse in your opinion? Incorrectly predicting that students will pass (when they end up failing), or incorrectly predicting that students will fail (when they end up passing)? Why?\nIf you chose a different decision criteria to predict passing - say .7, what would happen to the proportion of students that were classified as “pass”, but actually failed? \ngets smaller\ncan’t say\nstays the same\ngets bigger\n\n18.4.4 Hierarchical Logistic Regression\nIn this assignment we will compare the following two models against each other.\n\nModel1: \\(Logit(Pass) = b0+b1∗Math\\)\n\nModel2: \\(Logit(Pass) = b0+b1∗Math+b2∗ExamStress+b3∗Evaluation\\)\n\n\nLater on, we want to carry out a model comparison test to check whether the larger model predicts the probability of passing the exam better than the smaller model.\nWhat would be the regression df for the model comparison test, in which we compare the larger model 2 to the smaller model 1? \nNow we will carry out the model comparison test. Take the following steps.\nNavigate to Analyze -&gt; Regression -&gt; Binary Logistic\nSelect Math as predictor (covariates).\nClick on Next (upper right); we can now indicate which block of predictors we like to add in addition to the variables added in the first model.\nEnter ExamStress and Evaluation as predictors (covariates).\nPaste and run the syntax.\nInspect the output. SPSS organizes the output in three blocks. The results in Block 0 refer to the null model without any predictors. Note that this null model also exists when you use the “Linear Regression” interface, but it’s not featured in the output.\nBlock 1 refers to the results of Model 1 and Block to the results of Model 2.\nTake a look at the results of the model comparison test.\nWhat test statistic is used to compare nested logistic regression models? \nZ\nChi squared\nF\nt\nWhat is the value of the appropriate test statistic for comparing models 1 and 2? \nTrue or false: Adding predictors ExamStress and Evaluation lead to significantly better predictions, compared to a model with only high school math grade as predictor. \nTRUE\nFALSE",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>GLM VIII - Logistic Regression</span>"
    ]
  },
  {
    "objectID": "glm_contrasts.html",
    "href": "glm_contrasts.html",
    "title": "19  GLM: Contrasts",
    "section": "",
    "text": "19.0.1 Effects Coding\nRecall that in one-way ANOVA, we have a categorical predictor and a continuous outcome variable. The overall F-test of an ANOVA provides an omnibus (overall) test of differences between group means. The null hypothesis tested in this case is that all \\(k\\) groups have the same mean, \\(H_0: \\mu_1 = \\mu_2 = \\ldots \\mu_k\\). Today, we delve deeper into tests you can perform when you have more complex hypotheses about group means, or want to perform follow-up tests to determine which groups differ significantly.\nFirst, we already covered that we can use dummy variables to incorporate this categorical predictor in a regression model. We pick one reference category and create dummy variables to indicate membership in the other groups. These dummy variables take binary values (0 or 1) to represent group membership.\nThe formula for regression with dummies is:\n\\(\\hat{Y}_i = a + b_1D_{1i}+ b_2D_{2i}+ b_3D_{3i}+ b_4D_{4i}\\)\nThe intercept represents the mean value of the reference category, and the coefficients \\(b\\) represent the differences between each group and that reference category.\nA different way to specify the same model is to omit the intercept, and include a dummy for each of the \\(k\\) groups, so we represent \\(k\\) groups with \\(k\\) dummies:\n\\(\\hat{Y}_i = b_1D_{1i}+ b_2D_{2i}+ b_3D_{3i}+ b_4D_{4i}+ b_5D_{5i}\\)\nBoth of these models are mathematically identical. The advantage of estimating all group means is that this model provides a standard error for each group mean, allowing us to test each group mean against hypothesized values.\nAnother way to include a categorical predictor is via effects coding. Effects coding compares each group to the grand mean. When we have unequal group sizes, the coding scheme should account for relative group size.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>GLM: Contrasts</span>"
    ]
  },
  {
    "objectID": "glm_contrasts.html#lecture",
    "href": "glm_contrasts.html#lecture",
    "title": "19  GLM: Contrasts",
    "section": "\n19.1 Lecture",
    "text": "19.1 Lecture",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>GLM: Contrasts</span>"
    ]
  },
  {
    "objectID": "glm_contrasts.html#tutorial",
    "href": "glm_contrasts.html#tutorial",
    "title": "19  GLM: Contrasts",
    "section": "\n19.2 Tutorial",
    "text": "19.2 Tutorial\n\n19.2.1 Group Means\nIn this tutorial, you will learn to use the general linear model to estimate means and to test the difference between two group means, the difference between individual group means and the overall mean, and between groups of means.\nOpen hiking_long.sav in SPSS.\nThe data file describes the result of a fictitious experiment. A hiking guide has displayed five different types of behavior towards different groups of hikers. The treatment that each person received from the guide is recorded in the variable behavior.\nThe dependent variable of this experiment is feeling. Higher scores on this variable indicate a more positive attitude of a participant towards the guide. In this assignment, we will use ANOVA to determine whether the mean score on the dependent variable differs between the five experimental conditions.\nThe data file contains a third variable named weather which can be either good or bad. For now, we will only look at the results obtained during good weather. Hence, we will use “Select cases” to select only those participants with a value of 1 on the weather variable.\nAdditionally, the data contains a variable named balanced which distinguishes between data resulting from a balanced experiment (with equal sample sizes in all groups), and from an unbalanced experiment (with unequal group sizes). For now, just ignore this variable.\nClick Data &gt; Select Cases and select “If condition is satisfied” and click the “If”-button. Now enter the following condition into the equation box:\nweather = 1\nNow click “Continue” and “Paste” to paste the resulting syntax into the syntax editor. Select Run &gt; All to run it. You should now see in the Data View tab that half of the participants have been crossed out.\nFirst, let’s compute the overall mean of feeling and tabulate the group means.\nWhat is the overall mean of feeling? \nWhat are the group means:\n\nWhat is the mean of the rushing group? \n\nWhat is the mean of the stories group? \n\nWhat is the mean of the insulting group? \n\n\n\n\nAnswer\n\nTo get the mean of feeling, use Analyze -&gt; Descriptive Statistics -&gt; Descriptives.\nTo get the group means, use Analyze -&gt; Compare Means -&gt; Means\nDESCRIPTIVES VARIABLES=feeling \n  /STATISTICS=MEAN STDDEV MIN MAX.\n  \nMEANS TABLES=feeling BY behavior\n  /CELLS=MEAN COUNT STDDEV.\n\nYou have previously learned to include categorical variables in a linear model by using dummy coding. Today, we will build upon this principle of encoding the information from a categorical variable into several numerical variables.\nFirst, recall that a linear model with a five-group nominal predictor can be written as follows:\n\\(\\hat{Y} = b_0 + b_1*D_1 + b_2*D_2 + b_3*D_3 + b_4*D_4\\)\nWhat is \\(b_0\\) in this equation?\n\nThe intercept; it is the mean of the reference category.The average of the group meansThe overall sample mean.The slope of the reference category.\n\nTo estimate the model above using regression, you could code dummy variables as follows:\n\n\nbehavior\nD1\nD2\nD3\nD4\n\n\n\nrushing\n1\n0\n0\n0\n\n\ntelling stories\n0\n1\n0\n0\n\n\ninsulting\n0\n0\n1\n0\n\n\nmaking jokes\n0\n0\n0\n1\n\n\nsinging\n0\n0\n0\n0\n\n\n\nWhat is the reference category in the coding scheme above? \nsinging\nrushing\nnone\njokes\nSpecify the dummies as described in the table, and estimate the model.\n\n\nAnswer\n\nTo get the mean of feeling, use Analyze -&gt; Descriptive Statistics -&gt; Descriptives.\nTo get the group means, use Analyze -&gt; Compare Means -&gt; Means\nRECODE behavior (1=1) (2=0) (3=0) (4=0) (5=0) INTO rushing.\nRECODE behavior (1=0) (2=1) (3=0) (4=0) (5=0) INTO stories.\nRECODE behavior (1=0) (2=0) (3=1) (4=0) (5=0) INTO insulting.\nRECODE behavior (1=0) (2=0) (3=0) (4=1) (5=0) INTO jokes.\nEXECUTE.\n\n  \nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT feeling\n  /METHOD=ENTER rushing stories insulting jokes.\n\nWhat’s the value of the intercept? \nIn this analysis, the intercept is the mean value on feeling for the reference category (singing). Verify that this is true by comparing the intercept of this regression to the Means table you made previously.\nWhat is the value of the coefficient for stories? \nHow can we interpret this coefficient?\n\nThe difference between the mean of the singing group and the mean of the stories group.The mean of the stories group.\n\n\n19.2.2 More Dummies\nAs we’ve previously established, dummy variables allow us to test the significance of mean differences between one reference group and all other groups.\nNow, imagine we expect rushing to have a negative effect on behavior, and we want to know which other behaviors are “better” (i.e., result in a higher score on behavior) than rushing.\nSpecify your hypotheses, then check your answer.\n\n\nAnswer\n\n\\(H_0: \\mu_{rushing} \\leq (\\mu_{stories}, \\mu_{insulting}, \\mu_{joking}, \\mu_{singing})\\)\n\\(H_1: \\mu_{rushing} &lt; (\\mu_{stories}, \\mu_{insulting}, \\mu_{joking}, \\mu_{singing})\\)\n\nUse dummy variables to test this hypothesis. Note: you will need to specify one additional dummy.\n\n\nAnswer\n\nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT feeling\n  /METHOD=ENTER stories insulting jokes singing.\n\nWhat is the \\(R^2\\) of this model? \nCompare this to the \\(R^2\\) of your previous model. They should be identical, as should be the overall F-test and p-values. Changing the reference category doesn’t change what information the dummies convey.\nDo we perform one-sided or two sided tests? \none-sided\ntwo-sided\nUse this information to test your hypotheses.\nWhich behaviors are “better” than rushing?\n\njokes and singingno behaviorsall behaviorsstories, jokes, and insulting\n\nYou can use this technique any time you want to test the significance of the difference between one reference group and other groups.\nNote that, in the first assignment, we used a different set of dummy variables than in the second assignment. This means that you can always use different sets of dummy variables when want to compare against multiple reference groups.\n\n19.2.3 Estimating group means\nIn the first assignment, we computed the group means. But remember that the ANOVA model allows us to estimate them using the general linear model. In this assignment, we will do so by hand. After the previous assignment, you should have five dummy variables to represent the five groups of behavior.\nUntil now, you’ve always included four dummy variables to represent five categories, as the last category is represented by the intercept.\nHowever, it is also possible to represent five groups as follows:\n\\(\\hat{Y} = b_1*D_1 + b_2*D_2 + b_3*D_3 + b_4*D_4 + b_5*D_5\\)\nWhat is \\(b_5\\) in this equation?\n\nThe mean value of group 5The intercept; it is the mean of the reference category.The slope of the reference category.The overall sample mean.\n\nTo estimate the model above using regression, you could code dummy variables as follows (note that you should already have all these dummies from the previous assignment):\n\n\nbehavior\nD1\nD2\nD3\nD4\nD5\n\n\n\nrushing\n1\n0\n0\n0\n0\n\n\ntelling stories\n0\n1\n0\n0\n0\n\n\ninsulting\n0\n0\n1\n0\n0\n\n\nmaking jokes\n0\n0\n0\n1\n0\n\n\nsinging\n0\n0\n0\n0\n1\n\n\n\nNow, go to Analyze -&gt; Regression -&gt; Linear, and add all five dummies as predictors.\nThen, click the Options button, and notice the option titled “Include Constant in Equation”.\nTurn this option off to remove the intercept from the regression equation, then paste your syntax. Notice a new line that says /ORIGIN instead of /NOORIGIN. This command removes the intercept.\nRun your syntax, and examine the results.\nYou might notice that the \\(R^2\\) and F-test changed. This is because these are computed relative to a null-model with only the intercept - but you told SPSS not to include an intercept, so it can’t compute that null model here. It’s not a big deal. As soon as you estimate models with an intercept again, the \\(R^2\\)s will be identical again, regardless of the dummy coding.\nWhat’s the value of the dummy for singing? \nCompare all coefficients to the table of means from the first assignment. They should all be identical.\nThis is how you estimate means using the linear model!\nNow, what do the t-tests and p-values in the Coefficients table tell us?\n\nWhether the means are significantly different from the reference category.Whether the means are significantly different from each other.Whether the means are significantly different from zero.They are not meaningful.\n\nKeep in mind that you can use the standard errors from the coefficients table to perform t-tests against other values than 0; for example, what would the test statistic be when testing whether the mean of the insulting group is significantly different from 5, so \\(H_0: \\mu_{insulting} = 5\\)? t = \nIs the difference significant? \nYes\nNo\nYou can use regression without an intercept any time you wish to estimate all group means in a single analysis and/or test the group means against specific hypothesized values.\n\n19.2.4 Comparing to Overall Mean\nUntil now, we’ve represented levels of a categorical variable using dummy variables with values 0 or 1.\nIn this assignment, we introduce an alternative coding system: effects coding.\nThe main difference with dummy coding is that the reference category does not receive 0 values on all indicator variables, but instead, receives a negative value. In a balanced design (with equal sizes for each group), this value is -1.\nSo in a balanced design, with equal group sizes, the coding scheme for effects coding is (I now use the letters E1-E4 to clarify that these are not dummies but effect coded indicators):\n\n\nbehavior\nE1\nE2\nE3\nE4\n\n\n\nrushing\n1\n0\n0\n0\n\n\ntelling stories\n0\n1\n0\n0\n\n\ninsulting\n0\n0\n1\n0\n\n\nmaking jokes\n0\n0\n0\n1\n\n\nsinging\n-1\n-1\n-1\n-1\n\n\n\nThe reference category is still “singing”.\nThe resulting model will give us the following information:\n\nThe overall sample mean for feeling\nThe difference between each group mean, except for singing, compared to the overall mean\n\nMost of the time, however, we will not have balanced designs. With this in mind, it is more useful to learn the general way to construct effect coding.\nSpecifically, the weights assigned for the singing category (reference category) differ for each dummy, and are computed as:\n\\(-1 * n_{\\text{this category}} / n_{\\text{reference category}}\\)\nCheck the group sizes in the output from assignment 1.\nWhat is the sample size for the rushing group? \nWhat is the sample size for the reference category? \nWith this in mind, what should the weight be for the singing group, on the dummy that codes for membership of the rushing group? \nComplete the following syntax, then run it:\nRECODE behavior (1=1) (2=0) (3=0) (4=0) (5= …) INTO Erushing. RECODE behavior (1=0) (2=1) (3=0) (4=0) (5=) INTO Estories. RECODE behavior (1=0) (2=0) (3=1) (4=0) (5=) INTO Einsulting. RECODE behavior (1=0) (2=0) (3=0) (4=1) (5=…) INTO Ejokes. EXECUTE.\nNote the correct answer for the effect code for the stories group. Compare the number of people in the stories group and in the reference group. Then recall that I explained that In a balanced design (with equal sizes for each group), this value is -1. You see that this is true now, and why.\nCalculate the effect indicators, then specify a regression model with these four effect indicators. Make sure to include the intercept again!\n\n\nCheck correct syntax\n\n\nRECODE behavior (1=1) (2=0) (3=0) (4=0) (5= -1.18) INTO Erushing.\nRECODE behavior (1=0) (2=1) (3=0) (4=0) (5=-1) INTO Estories.\nRECODE behavior (1=0) (2=0) (3=1) (4=0) (5=-1.18) INTO Einsulting.\nRECODE behavior (1=0) (2=0) (3=0) (4=1) (5=-1.18) INTO Ejokes.\nEXECUTE.\n\n\nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT feeling\n  /METHOD=ENTER Erushing Estories Einsulting Ejokes.\n\n\nRun the syntax. What is the F-value of the model? \nVerify that this is the same value you got before in models with an intercept.\nWhat is the value of the intercept? \nVerify that this is identical to the overall mean of the dependent variable.\nWhich group means differ significantly from the overall mean?\n\nno behaviorsinsultingall behaviorsjokes and insulting\n\nUsing the coefficients table, calculate the mean of the jokes group. What value do you get? \nThis should be identical to the mean you observed in the previous assignment (using regression to estimate means), and in the first assignment (just computing the means).\n\n19.2.5 Comparing Groups of Means\nExtending the methods above, it is also possible to compare groups of means. For example, we might wonder whether negative behaviors (rushing and insulting) differ significantly from positive behaviors (stories, jokes, and singing).\nThis approach builds upon the logic of effects coding, where the weights for the reference category were based on the relative sample size of the reference category. This time, however, the weights for the category to be compared to the reference category are also based on a sample size.\nWe are going to perform several steps, as explained in the lecture.\n\n19.2.5.1 Step 1: Plan Contrasts\nKeep in mind these rules:\n\nThe possible values of each indicator variable must sum to 0.\nEach group must be uniquely identified by a particular combination of the contrast variables.\n\nAssume for a moment that we have equal group sizes and want to compare groups 1 and 2 to groups 3, 4, and 5.\nAppropriate contrasts would then be (I’m using the letter C to indicate that these are not dummies or effect indicators):\n\n\nbehavior\nC1\nC2\nC3\nC4\n\n\n\nrushing\n1\n1\n0\n0\n\n\ninsulting\n1\n-1\n0\n0\n\n\ntelling stories\n-\\(\\frac{2}{3}\\)\n\n0\n2\n0\n\n\nmaking jokes\n-\\(\\frac{2}{3}\\)\n\n0\n-1\n1\n\n\nsinging\n-\\(\\frac{2}{3}\\)\n\n0\n-1\n-1\n\n\n\nNote that:\n\nEach column sums to 0\nEvery level of behavior is uniquely identified by some combination of contrasts\n\nIn this case, we only care about C1; we created C2, C3 and C4 to ensure that every level of behavior is uniquely identified. But what do C2-C4 test?\nC2 compares the two negative behaviors; C3 compares stories against jokes and singing. C4 compares jokes and singing.\n\n19.2.5.2 Step 2: Account for Group Size\nNow, we have to account for the relative sample sizes of these groups to ensure that we can interpret the coefficients as the difference between the means of those combinations of groups.\nUse the descriptive statistics you previously obtained to weight the contrasts from step 1.\nE.g., contrast C3 below is already completed. Which other contrasts do you still need to change? \nC1\nC1, C2, C4\nnone\nC2 and C4\n\n\nbehavior\nC1\nC2\nC3\nC4\n\n\n\nrushing\n1\n1\n0\n0\n\n\ninsulting\n1\n-1\n0\n0\n\n\ntelling stories\n-\\(\\frac{2}{3}\\)\n\n0\n1\n0\n\n\nmaking jokes\n-\\(\\frac{2}{3}\\)\n\n0\n-13/(11+13)\n1\n\n\nsinging\n-\\(\\frac{2}{3}\\)\n\n0\n-11/(11+13)\n-1\n\n\n\n19.2.5.3 Step 3: Do Matrix Algebra\nEnter the complete matrix into a spreadsheet program. Add one column before the contrasts with an intercept for each group, equal to \\(1/k\\).\nWhat’s the value of this intercept for this study? \n\nClick an Empty cell\nPaste =MINVERSE(TRANSPOSE(\n\nSelect your contrast matrix\nFinish the formula by typing closing brackets ))\n\n\nThese are the values you will use for your indicators!\nNow, write syntax to create the contrasts using the values you calculated in a spreadsheet. Give these contrasts informative names to help remind yourself of their interpretation. Here is one example; complete the rest yourself:\nRECODE behavior (1=.6) (2=.6) (3=-.4) (4=-.4) (5= -.4) INTO posVneg.\n\n\nAnswer\n\nRECODE behavior (1=.6) (2=.6) (3=-.4) (4=-.4) (5= -.4) INTO posVneg.\nRECODE behavior (1=.5) (2=-.5) (3=0) (4=0) (5= 0) INTO rushVinsult.\nRECODE behavior (1=-.01) (2=-.01) (3=.67) (4=-.33) (5= -.33) INTO storyVjokesing.\nRECODE behavior (1=.02) (2=.02) (3=.02) (4=.48) (5= -.53) INTO ? .\nEXECUTE.\n\nWhat does the final contrast encode? \npositive versus negative\nall levels\njoke versus singing\nstory vs singing\n\n19.2.6 Run the Analysis\nCreate the indicator variables and run the regression analysis.\nWhat is the mean difference in feeling between rushing and insulting behaviors? \nWhich effects are significant? \nall contrasts\npositive V negative behaviors\nstory V jokes, singing\nrushing V insulting\n\n19.2.7 Adjusting for Multiple Comparisons\nIn these assignments, we have been conducting many tests. You have learned that the significance level \\(\\alpha\\) indicates the probability of drawing a false-positive conclusion (Type I error). However, these probabilities add up for multiple tests! So when you perform many tests, you can be in a situation where you have a very high probability of comitting at least one Type I error.\nWe call the total probability of committing at least one Type I error across multiple tests in the same study the “family-wise” or experiment-wise Type I error. You compute it as:\n\\(P(1+ Type I error) = 1 − (1 − \\alpha)^{\\text{number of tests}}\\)\nSo if we perform 3 comparisons, the probability of committing at least one Type I error is: \nAnd if we perform 10 tests? \nIf this makes you uncomfortable - you’re not alone! People often seek to maintain a low risk of drawing any false-positive conclusions, and we can do so simply by lowering \\(\\alpha\\).\n\n19.2.7.1 Bonferroni correction\nBonferroni proposed a simple correction of \\(\\alpha = \\alpha_{EW}/m\\), where \\(\\alpha_{EW}\\) is the desired experiment-wise Type I error rate (e.g., .05), and \\(m\\) is the number of tests.\nWhat alpha level would you use per test if you want to achieve an experiment-wise alpha of .05 and conduct 7 tests? \nThe Bonferroni correction is quite conservative; in other words - although Bonferroni helps you avoid false-positive conclusions, it becomes much harder to detect true effects.\n\n19.2.8 Compare All Groups\nThrough the ANOVA interface, you can compare all groups to one another. This is equivalent to repeating a regression analysis multiple times, making each category the reference category in turn.\nGo to Analyze -&gt; Compare Means -&gt; One Way ANOVA. Enter Feeling as dependent variable and behavior as Factor.\nNow, click post-hoc. Note that you can select many different tests. Select LSD; this corresponds to “normal” p-values.\nThe other tests in this menu will either apply a penalty to the p-value, or compute the test statistic in a different way, with the purpose of adjusting for multiple comparisons.\nWe will manually apply the correction for multiple comparisons instead, because the fact that SPSS performs the correction behind the scenes has a high risk of user error.\nAssuming we perform two-sided tests at \\(\\alpha = .05\\), how many significant differences between group means are there? \nNow, apply a Bonferroni correction to the alpha level. How many tests are you performing? \nWhat is the new alpha level? \nHow many comparisons are still significant when using this new alpha level?",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>GLM: Contrasts</span>"
    ]
  },
  {
    "objectID": "factorial.html",
    "href": "factorial.html",
    "title": "20  GLM: Factorial ANOVA",
    "section": "",
    "text": "20.1 Lecture\nIn previous chapters, we have explored various regression techniques, including bivariate linear regression, independent samples t-tests, and analysis of variance (ANOVA). We’ve also delved into the concept of interaction between binary and continuous predictors. In this chapter, we build upon that prior knowledge when explaining the Factorial ANOVA.\nFactorial ANOVA is used to examine the effects of multiple categorical predictors and their interactions on a continuous outcome variable. Despite its historical development as a separate method, factorial ANOVA can be conceptualized as a special case of multiple regression. It combines the concepts of dummy coding and interaction that we’ve previously encountered. Each factor is represented via dummy coding, and creating interaction terms are computed by multiplying those dummies.\nThe reason ANOVA is often considered a separate technique is that it has different historical roots from regression, and because of those roots, researchers typically focus on different output when reporting factorial ANOVA versus regression. For example, ANOVA focuses more on variance explained by each factor, and overall tests of the effect of each Factor across all of its levels.\nA factorial ANOVA involves two or more factors, each with multiple levels. Each combination of factor levels creates a unique condition or group in the study. For instance, if we have Factor A with 3 levels and Factor B with 2 levels, the factorial design will have a total of 3 × 2 = 6 groups.\nFactorial designs can be visually represented in a matrix-like structure, where each cell represents a unique combination of factor levels. This representation helps us understand the experimental conditions and the interactions between factors.\nIn factorial ANOVA, we examine main effects and interaction effects. As in multiple regression, main effects represent the influence of each factor on the dependent variable, controlling for all other predictors. Interaction effects capture how the effects of one factor depend on the levels of another factor.\nBecause each factor may be represented by multiple dummies, we can’t use individual t-tests for the dummies to determine the siginficance of the Factor they belong to. Instead, we use F-tests to test the significance of the effect of a Factor. These F-tests compare the variance explained by the factor to the unexplained variance.\nIn addition to significance tests, effect size measures help us understand the practical importance of the effects observed in factorial ANOVA. Eta squared and partial eta squared are common effect size measures that quantify the proportion of variance explained by each factor or interaction, relative to the total variance or unexplained variance, respectively. Eta squared is just a different name for R squared; partial eta squared is something different, and typically only reported for ANOVA.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>GLM: Factorial ANOVA</span>"
    ]
  },
  {
    "objectID": "factorial.html#formative-test",
    "href": "factorial.html#formative-test",
    "title": "20  GLM: Factorial ANOVA",
    "section": "\n20.2 Formative Test",
    "text": "20.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat is the primary objective of factorial ANOVA?\n\nTo examine the effects of multiple categorical predictors and their interactions on a continuous outcome variable.To analyze the variance between different levels of a single categorical predictor.To determine the correlation between two continuous variables.To compare the means of two independent groups.\n\nQuestion 2\nIn a 2x2 factorial design, how many unique conditions or groups are there?\n\n2634\n\nQuestion 3\nWhat does a main effect represent in factorial ANOVA?\n\nThe unexplained variance in the model.The interaction between two factors on the dependent variable.The combined effect of all factors on the dependent variable.The influence of a single factor on the dependent variable, controlling for other factors.\n\nQuestion 4\nWhat does an interaction effect capture in factorial ANOVA?\n\nHow the effects of one factor depend on the levels of another factor.The influence of a single factor on the dependent variable, controlling for other factors.The unexplained variance in the model.The combined effect of all factors on the dependent variable.\n\nQuestion 5\nWhich effect size measure quantifies the proportion of variance explained by each factor or interaction relative to the total variance?\n\nEtaPartial eta squaredSum of SquaresEta squared\n\nQuestion 6\nWhat does it mean when lines representing different conditions on a means plot cross each other?\n\nThe model is poorly specified.There is a significant interaction between the factors.The main effects of the factors are significant.The factors have equal effects on the dependent variable.\n\nQuestion 7\nIn a 3x2 factorial design, how many dummies would be required in total to represent both factors?\n\n3645\n\nQuestion 8\nWhich term describes the variance remaining unexplained by the factors and interactions in factorial ANOVA?\n\nResidual varianceInteraction varianceExplained varianceTotal variance\n\nQuestion 9\nWhat does a partial eta squared measure in factorial ANOVA?\n\nThe interaction between two factors.The proportion of variance explained by each factor relative to the unexplained variance.The proportion of variance explained by each factor, controlling for other factors.The total variance explained by each factor.\n\n\n\n\nShow explanations\n\nQuestion 1\nFactorial ANOVA allows us to explore how multiple categorical predictors and their interactions impact a continuous outcome variable.\nQuestion 2\nIn a 2x2 factorial design, there are 2 levels for Factor A and 2 levels for Factor B, resulting in a total of 2 x 2 = 4 unique conditions.\nQuestion 3\nA main effect reflects the impact of a specific factor on the dependent variable while considering the influence of other factors.\nQuestion 4\nAn interaction effect in factorial ANOVA describes how the effects of one factor change based on the levels of another factor.\nQuestion 5\nEta squared is an effect size measure in factorial ANOVA that indicates the proportion of variance explained by a factor or interaction relative to the total variance.\nQuestion 6\nCrossing lines on a means plot indicate a significant interaction between the factors, suggesting that the effect of one factor depends on the levels of another factor.\nQuestion 7\nFor a 3x2 factorial design, Factor A would require 2 dummies, and Factor B would require 1 dummy. Therefore, the total number of dummies needed is 2 + 1 = 3.\nQuestion 8\nResidual variance refers to the unexplained variability in the dependent variable after accounting for the effects of factors and interactions in factorial ANOVA.\nQuestion 9\nPartial eta squared quantifies the proportion of variance explained by a specific factor while controlling for the influence of other factors in the model.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>GLM: Factorial ANOVA</span>"
    ]
  },
  {
    "objectID": "factorial.html#tutorial",
    "href": "factorial.html#tutorial",
    "title": "20  GLM: Factorial ANOVA",
    "section": "\n20.3 Tutorial",
    "text": "20.3 Tutorial\n\n20.3.1 Factorial ANOVA\nConsider the following research situation: A psychologist wants to study if and to what extent different behavior of waiters affect the amount of tip money they get, and whether it matters if the behavior is shown by a waiter or waitress.\nThe researcher distinguishes the following types of behavior: neutral behavior, drawing a smiley on the bill, or making small talk.\nThey ran a fully crossed experiment with 3 (behaviors) x 2 (gender: waiter or waitress) = 6 conditions. For each condition they collected data for 10 customers who were helped by a waiter showing neutral behavior, 10 helped by a waiter drawing a smiley on the bill, and so forth.\nIs the design balanced? \nYes\nNo\nWhat is/are the independent variable(s) in this experiment? \nBehavior\nGender\nTip size\nDraw the conceptual model of the experiment. Then, check your answer.\n\n\nAnswer\n\n\nlibrary(tidySEM)\nlibrary(ggplot2)\nlo &lt;- get_layout(\"\", \"Gender\", \"\",\n                 \"Behavior\", \"\", \"Tip size\", rows = 2)\nedges &lt;- data.frame(from = \"Behavior\", to = \"Tip size\")\np &lt;- prepare_graph(layout = lo, edges= edges)\nplot(p) + geom_segment(aes(x = p$nodes$x[p$nodes$name == \"Gender\"], xend =  p$nodes$x[p$nodes$name == \"Gender\"], y = p$nodes$node_ymin[p$nodes$name == \"Gender\"], yend = p$nodes$y[p$nodes$name == \"Behavior\"]), arrow = arrow(length = unit(0.03, \"npc\"), type = \"closed\"))\n\n\n\n\n\n\n\nAs you can see, the dependent variable is Tip money, and the independent variables are Type of behavior and Gender. More specifically. Gender is the moderator, as is expected to influence the relationship between Behavior and Tip money.\n\nNow, open the dataset WaiterBehavior.sav.\nWe will run the analysis to find out what the results of the experiment are. We can do so with a factorial ANOVA.\nFirst, create all the dummy variables you need, using syntax. Check your answer below.\n\n\nAnswer\n\n\nlibrary(tidySEM)\nlibrary(ggplot2)\nlo &lt;- get_layout(\"\", \"Gender\", \"\",\n                 \"Behavior\", \"\", \"Tip size\", rows = 2)\nedges &lt;- data.frame(from = \"Behavior\", to = \"Tip size\")\np &lt;- prepare_graph(layout = lo, edges= edges)\nplot(p) + geom_segment(aes(x = p$nodes$x[p$nodes$name == \"Gender\"], xend =  p$nodes$x[p$nodes$name == \"Gender\"], y = p$nodes$node_ymin[p$nodes$name == \"Gender\"], yend = p$nodes$y[p$nodes$name == \"Behavior\"]), arrow = arrow(length = unit(0.03, \"npc\"), type = \"closed\"))\n\n\n\n\n\n\n\nAs you can see, the dependent variable is Tip money, and the independent variables are Type of behavior and Gender. More specifically. Gender is the moderator, as is expected to influence the relationship between Behavior and Tip money.\n\n\n20.3.2 Regression with dummies\nFirst, we analyze these data using regression with dummies.\nYou will need to dummy code both categorical predictors.\nDo this in the familiar way, then check your syntax.\n\n\nAnswer\n\nRECODE behavior (1=1) (2=0) (3=0) INTO talking.\nRECODE behavior (1=0) (2=1) (3=0) INTO smiling.\nRECODE behavior (1=0) (2=0) (3=1) INTO neutral.\n\nRECODE Gender (1=1) (2=0) INTO waitress.\nRECODE Gender (1=0) (2=1) INTO waiter.\nEXECUTE.\n\nEstimate a regression model with a main effect for gender and behavior. Create the syntax, then check your answer below.\n\n\nAnswer\n\nUsing neutral behavior and waiter as reference categories, the code is:\nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT Tip\n  /METHOD=ENTER waitress smiling talking.\n\nNow, we want to examine whether there is an interaction between gender and behavior. To do so, we want to specify a “full factorial” model. This simply means that we want to know if there is an interaction effect between the two categorical variables. The way to include an interaction effect is to multiply the predictor. As each predictor is represented by multiple dummies, we have to multiply each dummy for each variable with all dummies from the other variable.\nIn this case, that means multiplying the waiter dummy with the smiling and talking dummies.\nDo this via syntax, then check your work.\n\n\nAnswer\n\nCOMPUTE smilingXwaitress = smiling*waitress.\nCOMPUTE talkingXwaitress = talking*waitress.\nEXECUTE.\n\nConduct a hierarchical regression analysis that includes these new interaction terms.\nWhat proportion of the total variance in Tip money is explained by the full factorial model? \nTrue or false: The full factorial model explains a significant amount of variance. \nTRUE\nFALSE\nIs there a significant interaction effect? \nYes\nNo\nCan’t tell\n\n\n\nExplanation\n\nEven though we see significant interaction terms in the coefficients table, determining whether there is a significant interaction between categorical variables requires more than just “eyeballing” whether those terms are significant or not. You need to perform a nested model test. \n\nBased on the output, complete the following table:\n\n\n\n\n\n\n\nBehavior\nGender\nMean\n\n\n\nNeutral\nWaiter\n4.600\n\n\nNeutral\nWaitress\n\n\n\nSmiley\nWaiter\n5.100\n\n\nSmiley\nWaitress\n7.600\n\n\nSmall talk\nWaiter\n\n\n\nSmall talk\nWaitress\n10.000\n\n\n\nDraw a rough plot of these means on a piece of paper.\nPut the type of behavior on the x-axis and draw separate lines for waiters and waitresses.\nTrue or false: The graph suggests a potential interaction. \nTRUE\nFALSE\nIf so, describe the interaction effect (i.e., what can we say about the effect of behavior on amount of tip money for waiters and waitresses?).\n\n\nAnswer\n\nThe lines are not parallel. Hence, also from the graph we see that there is interaction. The effect of type of behavior on the amount of tip money depends on the gender of the waiter/waitress.\nIt seems that for waiters the tip money does not depend much on the behavior.\nFor waitresses the effect is stronger; neutral behavior produces the least amount of tip money, whereas small talk is most beneficial.\n\nSee if you can answer the following question by yourself:\nTrue or false: The interaction effect is significant. \nTRUE\nFALSE\n\n\nAnswer\n\nTo answer this question, you need to perform a nested model test.\nThe syntax is:\nREGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA CHANGE\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN \n  /DEPENDENT Tip\n  /METHOD=ENTER smiling talking waitress\n  /METHOD=ENTER smilingXwaitress talkingXwaitress.\nNote two things: we request CHANGE statistics, and we enter all “interaction effects” in a separate step, so we can use the R-squared change test to determine overall significance.\n\nWhat is the value of the test statistic for the significance of the interaction effect? \nReport the effect, then check your answer.\n\n\nAnswer\n\nThere was a significant interaction effect between waiters’ sex and behavior, F(2,54) = 18.315, p &lt; .001.\nThis means that the effect of Type of behavior on Tip money depends on the Gender of the waiter/waitress.\n\nOut of curiosity - how much variance is explained by the main effects only? \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n20.3.3 ANOVA interface\nSPSS also has a dedicated “ANOVA interface”. It specifies exactly the same model as we have been investigating, but its output is more focused on information that is typically requested when estimating a model with only categorical predictors and (optionally) interactions between them.\nWe will now use the ANOVA interface and pay special attention to output it gives us that is not directly available via the Regression interface.\nYou can either use the graphical user interface, or copy-paste this syntax. Either way, pay particular attention to the following:\n\nUnder Options, ask for Homogeneity tests: /PRINT=HOMOGENEITY\n\nUnder Options, ask for Estimates of effect size: /PRINT=ETASQ\n\nUnder Options, ask for Parameter Estimates: /PRINT=PARAMETER\n\nUnder EM Means (expected marginal means), ask for the means for the interaction, and compare Main effects: /EMMEANS=TABLES(Behavior*Gender) COMPARE(Behavior) ADJ(LSD)\n\n\nUNIANOVA Tip BY Behavior Gender\n  /METHOD=SSTYPE(3)\n  /INTERCEPT=INCLUDE\n  /PLOT=PROFILE(Behavior*Gender)\n  /PRINT=ETASQ HOMOGENEITY DESCRIPTIVE PARAMETER\n  /CRITERIA=ALPHA(.05)\n  /EMMEANS=TABLES(Behavior*Gender) COMPARE(Behavior) ADJ(LSD)\n  /EMMEANS=TABLES(Behavior*Gender) COMPARE(Gender) ADJ(LSD)\n  /DESIGN=Behavior Gender Behavior*Gender.\nCopy and run this syntax.\nNote that the table labelled Parameter Estimates is identical to the Coefficients table from the previous regression analysis.\nAlso note that, for example, the F-value for the explained variance for the interaction between Gender an Behavior is identical to the F-value for the \\(\\Delta R^2\\) test you performed in the previous hierarchical regression analysis.\n\n20.3.3.1 Homoscedasticity\nRegression models have an assumption of homoscedasticity. If all predictors are categorical, that means that we assume equal variances in all groups.\nThe ANOVA output offers us a Levene’s test for homogeneity of variance. Find it, and answer the following question:\nTrue or false: There is reason to doubt the assumption of homogeneity. \nTRUE\nFALSE\nWhat is the value of the appropriate test statistic? \nTrue or false: As a rule, the assumption of homogeneity is more likely to be violated in a factorial ANOVA with an unbalanced design (as compared to a balanced design). \nTRUE\nFALSE\n\n20.3.3.2 Effect size\nWe can also calculate effect sizes \\(\\eta^2\\) or partial \\(\\eta^2\\), for the two factors and the interaction effect separately.\nRecall that \\(\\eta^2\\) is just the explained variance, \\(R^2\\). In other words: What proportion of the total sum of squares is explained by the factor of interest?\nWe obtain \\(\\eta^2\\) for Factor A by dividing the sum of squares for factor A, \\(SS_A\\), by the \\(SST\\), which is labeled “Corrected total” in the ANOVA output: \\(\\eta^2 = \\frac{SS_A}{SST}\\)\nWhat is \\(\\eta^2\\) for the interaction effect? \nGo back to your previous nested model test, where you determined whether adding the interaction terms led to a significant improvement in explained variance, \\(\\Delta R^2\\). Verify that this number is identical to the \\(\\eta^2\\) for the interaction! They are the same thing.\nAnother measure of effect size is the partial \\(\\eta^2\\). It tells us what proportion of the variance not explained by other factors is explained by the factor of interest.\nWe obtain \\(\\eta_p^2\\) for Factor A by dividing the sum of squares for factor A, \\(SS_A\\), by \\(SS_A\\) plus the residual sum of squares \\(SSE\\): \\(\\eta_p^2 = \\frac{SS_A}{SS_A+SSE}\\).\nNote that SPSS allows us to request partial \\(\\eta^2\\). We did so by including the line /PRINT=ETASQ in our syntax.\nWhat is the value of partial \\(\\eta^2\\) for the factor Behavior? \n\n20.3.3.3 Pairwise comparisons\nAnother unique feature of the ANOVA interface is that it gives us all pairwise comparisons when we ask for them using the code /EMMEANS=TABLES(Behavior*Gender) COMPARE(Behavior) ADJ(LSD)\nNote that this line asks for comparisons of the three levels of behavior for each gender separately. You can also ask for comparisons of the two levels of gender for each behavior separately by specifying COMPARE(Gender) instead.\nInspect the table Pairwise Comparisons. The three experimental conditions are compared in a pairwise manner, split over the factor Gender.\nFor which pair of groups do the means differ significantly from one another (at the 5% level)? \nWaiter: Small talk-Smiley\nWaitress: Neutral-Smiley\nWaiter: Neutral-Smiley\nWaiter: Neutral-Small talk\nLook at the note under the table. True or false: the p-values in this table are adjusted for multiple comparisons. \nTRUE\nFALSE\nWhy do we need to apply a correction like the Bonferroni correction?\n\n\nAnswer\n\nWhen doing multiple tests on one sample, like doing these pairwise comparisons, the level of risk of a Type I error increases. To correct for this, we should use an adjusted alpha-level, such as the Bonferroni correction.\nAlthough it is possible to ask for SPSS to adjust the p-values in the table, I have expressly not instructed you to do so. The reason for this is that, strictly speaking, Bonferroni is an adjustment of the significance level \\(\\alpha\\) - not of the p-values. Moreover, sometimes you conduct many more tests in a study aside from the pairwise comparisons performed here. In that case, you might want to include those in your Bonferroni correction too, and SPSS does not know about their existence when it applies a Bonferroni correction to the p-values.\nIn other words: It is better to set the alpha level yourself, and apply it consistently across all tests in a study.\n\n\n20.3.3.4 Simple Effects test\nThe significant interaction effect tells us that the effect of behavior differs by gender, or conversely, that gender differences vary across the three behavoirs.\nSimple effects analysis allows us to test the significance of these marginal effects.\nSimply put: They give us an overall test of the mean differences across levels of one factor, within each level of a different factor.\nConsult the table Univariate Tests (still for the contrast /EMMEANS=TABLES(Behavior*Gender) COMPARE(Behavior) ADJ(LSD)). This table displays the results of the simple effects tests.\nDoes the Behavior of waiters have an influence on the amount of tip money people give?\nWhat is the appropriate p-value? \nReport the simple effect test for waiters and interpret the finding, then check your answer.\n\n\nAnswer\n\nThere is no significant difference among the three behaviors within waiters. Hence, for waiters we don’t have evidence that the behavior has an effect on average tips received, F(2,52) = 0.591, p = .557.\n\nAgain, consult the table Univariate Tests. This table gives the results of the simple effects tests.\nWhat p-value do we see here? \nTrue or false: the type of behavior of waitresses has an influence on the tip people give. \nTRUE\nFALSE\nDoes it make sense to adjust your behavior as a waiter/waitress if you want to increase your tip?\n\nFor waiters behavior does affect amount of tip money received, but for waitresses it does not.For both waiters and waitresses behavior does not affect amount of tip money received.For waitresses behavior does affect amount of tip money received, but for waiters it does not.For both waiters and waitresses behavior does affect amount of tip money received.\n\nReport your results, then check the answer.\n\n\nAnswer\n\nWe do see a significant effect of behavior on average tip money for waitresses, F(2,52) = 46.385, p &lt; .001. Hence, we have convincing evidence that the type of behavior by waitresses affects the average amount of tips. Results suggest that in order to have high tips, waitresses best can make small talk.\n\n\n20.3.4 Optional: do it yourself!\nOpen the datafile hiking.sav. The data file also contains data on weather.\nExamine the effect of weather and behavior and their potential interaction, with feelings as the dependent variable.\nWhat is the explained variance of the main effects and interaction effects together? \nTrue or false: the explained variance of the whole model is significant. \nTRUE\nFALSE\nTrue or false: the interaction effect is significant. \nTRUE\nFALSE\nRequest a plot from SPSS that allows you to describe what the effects look like.\nDescribe the trends in your own words, then check your answer.\n\n\nAnswer\n\nThe lines in the plot are not parallel, also pointing to an interaction effect.\n\nPerform a simple effects analysis of the effect of behavior by weather.\nReport your findings and conclusion, then check your answer.\n\n\nAnswer\n\nWe see a significant effect of behavior when the weather was good, F(4,90) = 3.864, p = .006, while the effect of behavior is not significant when the weather was bad, F(4,90) = 1.320, p = .269.\nResults suggest that when the weather is good, joking and singing significantly improves the participants’ feelings about the guide.\nWhen the weather is bad, the behavior of the guide does not have much influence.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>GLM: Factorial ANOVA</span>"
    ]
  },
  {
    "objectID": "ancova.html",
    "href": "ancova.html",
    "title": "21  GLM: ANCOVA",
    "section": "",
    "text": "21.0.1 Covariates and Their Role\nANCOVA, which stands for Analysis of Covariance, is an extension of the concepts we’ve covered in bivariate linear regression and multiple regression. It is essentially a multiple regression with a categorical predictor and one or more continuous predictors. What’s “special” about this technique is that it is commonly used when the predictor of interest is that categorical variable, and the continuous predictor(s) are so-called “covariates”: predictors that are only included to improve our estimate of the effect of the categorical predictor of interest.\nYou will often see this technique used to analyze data from experiments or “natural experiments”, where participants self-select into a treatment group.\nWhile ANCOVA is a useful technique, it comes with some serious pitfalls: any time control variables are used, we are making assumptions about causality. If these assumptions are incorrect, our estimates of the effect of interest will be (severely) biased.\nCovariates are variables that have a relationship with the dependent variable but are not the primary focus of the study. They are often referred to as control variables, as they help control for unwanted variability and improve the precision of the analysis. Examples of common covariates include age, gender, education level, or any other variables that might influence the dependent variable.\nIn terms of causality, it’s crucial to consider the relationships between covariates, predictors, and the outcome variable. Control variables should ideally be confounders – variables that influence both the predictor of interest and the outcome. It’s essential to avoid controlling for colliders, which are variables caused by both the predictor and the outcome. A thorough understanding of causal relationships is crucial for proper interpretation.\nOne reason why researchers use control variables in ANCOVA is because they reduce the residual variance in the outcome variable, which in turn increases the power to detect the effect of the predictor of interest. Another reason to use covariates is when the goal is making causal inferences, especially in quasi-experimental designs. The proper selection of covariates that enable causal inference requires careful consideration and is beyond the scope of this course.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM: ANCOVA</span>"
    ]
  },
  {
    "objectID": "ancova.html#lecture",
    "href": "ancova.html#lecture",
    "title": "21  GLM: ANCOVA",
    "section": "\n21.1 Lecture",
    "text": "21.1 Lecture",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM: ANCOVA</span>"
    ]
  },
  {
    "objectID": "ancova.html#formative-test",
    "href": "ancova.html#formative-test",
    "title": "21  GLM: ANCOVA",
    "section": "\n21.2 Formative Test",
    "text": "21.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat is a valid reason for using covariates in ANCOVA?\n\nTo create more complex modelsTo replace categorical predictorsTo increase the total varianceTo increase statistical power\n\nQuestion 2\nWhat is an example of a ‘good control’ variable in ANCOVA?\n\nA variable unrelated to the predictorA confounderA variable related to the predictorA variable affected by the outcome\n\nQuestion 3\nWhich of the following is an example of a ‘bad control’ variable in ANCOVA?\n\nA variable affected by the outcomeA confounderA variable that causes the predictorA variable unrelated to the predictor or outcome\n\nQuestion 4\nIn a randomized controlled experiment, are there benefits to controling for any observed differences between the two experimental groups?\n\nNo, random assignment breaks the predictor's relationships with confoundersYes, controlling for relevant covariates is essentialOnly if the covariates are significantly related to the outcomeOnly if the covariates are significantly related to the predictor\n\nQuestion 5\nWhat is the purpose of calculating adjusted means in ANCOVA?\n\nTo create new variablesTo account for covariate effectsTo improve the model's fitTo replace the original means\n\nQuestion 6\nWhich of the following is a key factor in choosing appropriate covariates for ANCOVA?\n\nCovariates with the smallest p-valuesCausal relationshipsCovariates with the highest correlationsNumber of covariates\n\nQuestion 7\nWhat is the distinguishing feature of ANCOVA relative to multiple regression?\n\nANCOVA controls for confounding variablesMultiple regression includes only covariatesANCOVA always includes a categorical predictor and control variable(s)Multiple regression includes only continuous predictors\n\nQuestion 8\nIn which of these situations should you avoid controlling a particular covariate in ANCOVA?\n\nWhen the covariate is perfectly correlated with another covariateWhen the covariate is a mediatorWhen the covariate is irrelevantWhen the covariate is a confounder\n\nQuestion 9\nAn ANCOVA model compares statistics grades for two classes of students, controlling for number of hours studied per week. The regression model is \\(\\text{Grade} = -1.27 + 0.50*D_{class2} + .30*Hours\\). The unadjusted mean grades are 5.2 for class 1 and 7.3 for class 2. The mean hours studied were 21.56 for class 1 and 30.23 for class 2. The overall mean hours studied was 25.90. What are the adjusted means?\n\n6.50 and 6.00-1.27 and -1.779.35 and 2.97\n\n\n\n\nShow explanations\n\nQuestion 1\nCovariates can be used in ANCOVA to reduce error variance caused by factors other than the predictor of interest.\nQuestion 2\nA confounder is a ‘good control’ variable in ANCOVA – a variable that influences both the predictor of interest and the outcome.\nQuestion 3\nA variable that is affected by the outcome is a ‘bad control’ variable in ANCOVA and can introduce biases.\nQuestion 4\nIn a randomized controlled experiment, random assignment breaks the relationships between confounders and the treatment. Therefore, controlling for covariates related to the predictor is unnecessary.\nQuestion 5\nAdjusted means in ANCOVA are controlled for the effects of covariates, allowing us to understand how groups would differ on the outcome variable if they had equal scores on the covariate.\nQuestion 6\nCausal relationships are crucial when selecting covariates for ANCOVA. It’s important to choose covariates that are related to the outcome and predictor in a meaningful way.\nQuestion 7\nANCOVA is one example of the broader category of models known as multiple regression; it is characterized by having a categorical predictor of interest and (continuous) covariates.\nQuestion 8\nAvoid controlling for covariates that are mediators, as controlling for mediators could lead to overcontrol bias and distort the relationships between the predictor and outcome.\nQuestion 9\nUse the formula \\(\\bar{Y}_g^{adj} = \\bar{Y}_g - b(\\bar{X}_g-\\bar{X})\\)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM: ANCOVA</span>"
    ]
  },
  {
    "objectID": "ancova.html#tutorial",
    "href": "ancova.html#tutorial",
    "title": "21  GLM: ANCOVA",
    "section": "\n21.3 Tutorial",
    "text": "21.3 Tutorial",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM: ANCOVA</span>"
    ]
  },
  {
    "objectID": "ancova.html#bivariate-regression-recap",
    "href": "ancova.html#bivariate-regression-recap",
    "title": "21  GLM: ANCOVA",
    "section": "\n21.4 Bivariate Regression (RECAP)",
    "text": "21.4 Bivariate Regression (RECAP)\nResearchers are interested in the relationship between age and depression.\nThey hypothesized that older people are more vulnerable to depressive thoughts than younger people.\nTo test their research hypothesis, they collected data in a random sample of 164 persons from the general population. Open the dataset HADShealthyGroup.sav.\nRun a linear regression analysis using age as the independent variable and depression as the dependent variable.\nProceed as follows:\nNavigate to Analyze &gt; Regression &gt; Linear\nSelect the correct dependent and independent variable.\nPaste and run the syntax.\nHow much of the total variance in Depression is explained by Age? \nWhat can you say about the effect size? Would you say it’s a lot?\n\n\nAnswer\n\nTo me, 2% of explained variance does not seem like a lot. There are probably better predictors of depression (i.e., predictors that explain more of the variance in depression).\n\nTrue or false: The explained variance is significant at the 10% level. \nTRUE\nFALSE\n\n\nAnswer\n\nTo conclude anything about the significance of the proportion explain variance of this model, we can look at the ANOVA table or ask SPSS to show the R2-change. This shows us that the explained variance in this model is not significant compared to an empty model (i.e. including no predictors), F(1,140) = 2.836, p = .094.\n\nWrite down the estimated regression line using the unstandardized coefficients.\nY’=  +  *Age\nHow can we interpret the constant?\n\nThe predicted level of depression for the average age.The predicted level of depression when age would be 0.The average age in the sample.The average level of depression.\n\nConsult the table with the coefficients again\nTrue or false: We can conclude from this table that the effect of age is significant at the 10% level. \nTRUE\nFALSE\n\n\nAnswer\n\nTo conclude whether the effect of Age on Depression is significant, we look at the t-test for the estimated coefficient.\nWe should conclude that the effect of Age on Depression is significant when using \\(\\alpha\\)=.10, t(140) = 1.684, p = .094.\n\nOne of the assumptions of bivariate regression analysis is that the relationship between the independent and dependent variable is linear.\nState in your own words what this assumption entails.\n\n\nAnswer\n\nThe assumption of a linear relationship entails that the relationship between the variables can be described with a straight line.\n\nHow would you evaluate the assumption of linearity graphically? Do it for the data at hand.\nTrue or false: The relationship is linear. \nTRUE\nFALSE\nIf the assumption is not met, speculate about other possible relationships between age and depression.\n\n\nAnswer\n\nThe scatter plot does not have the shape of a cigar, so it does not unambiguously suggest a linear relationship. Thus, you may doubt whether the relationship between age and depression is best described by a linear model. Perhaps the relationship is quadratic. Especially persons in middle ages may be vulnerable to depressive thoughts. Next to the plot, you find both the estimated linear trend and a non-linear trend. It seems that the quadratic curve fits better with the data. Moreover, the quadratic model explains 6% of the variance, whereas the linear model only 2%.\n\nRun a regression analysis using Age as the independent variable and Anxiety as the dependent variable.\nSummarize the results.\nInclude in your answer the proportion of explained variance (R-square), a description of the effect based on the estimated regression coefficients, and evaluate the significance of the effect of age on anxiety.\n\n\nAnswer\n\nThe proportion explained variance in Anxiety by Age is .071. The explained variance in this model is not significant compared to an empty model (i.e. including no predictors), F(1,140) = 0.710, p = .401.\nThe effect is Age on Anxiety is negative (\\(\\beta\\) = -0.013), meaning that anxiety decreases with age. However, the effect of Age on Anxiety is not significant when using \\(\\alpha\\)=.05, t(140) = -0.842, p = .401.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM: ANCOVA</span>"
    ]
  },
  {
    "objectID": "ancova.html#anova-recap",
    "href": "ancova.html#anova-recap",
    "title": "21  GLM: ANCOVA",
    "section": "\n21.5 ANOVA (RECAP)",
    "text": "21.5 ANOVA (RECAP)\nConsider the following hypothetical research situation…\nResearchers are interested in effects of stereotyping on cognitive performance. For their research they performed a quasi-experiment. They selected three schools and asked girls from eight grade to do a math test. However, the teacher in School A says that boys do particularly well on the test (i.e., negative stereotyping for girls). In School B the teachers says that girls do particularly well on the test (i.e., positive stereotyping for girls). In the third school, the teacher gives gender-neutral information (control group).\nAfterwards the researchers compare the average math grades across the three groups. Because the schools may also differ in the student population, researchers also measured scholastic aptitude and use that as a covariate in the analysis.\nOpen the dataset stereotyping.sav.\nIn your own words, explain what a covariate is and give two examples of covariates that should/can be included in neuro-psychological research.\n\n\nAnswer\n\nCovariates are “nuissance” variables that we are not directly interested in, but that allow us to better estimate the effect of another variable of interest.\nThis is related to causal inference.\nWe must thus justify our covariates by reference to their putative causal role with relation to our predictor and outcome.\nA covariate that causes both our predictor and our outcome is a confounder, and should be controlled for. This sometimes happens in “natural experiments”, where the factor is not randomly assigned to participants.\nA covariate that causes our outcome, but is unrelated to our predictor, can be controlled for to reduce error variance in the outcome and increase statistical power of the effect of interest. This typically happens in randomized controlled experiments, but it may also happen in natural experiments.\nAs a counter-example: A variable that is caused by our predictor and our outcome is a collider, and should never be controlled for! This will bias the effect of our predictor.\n\nMention two often-used covariates in research.\n\n\nAnswer\n\nGender and Age are two covariates that are often used in research.\n\nLet’s start analyzing the math scores using an ANOVA, thus ignoring any covariates for now.\n\nCompute the means across the three groups (Analyze à Compare means à Means.\nSelect MATH for the dependent list and STEREO as the independent.\nPaste and run the syntax.\nInspect the table that displays the mean differences between the groups.\n\nWhat is the first impression of stereotyping that we have from the mean differences?\n\n\nAnswer\n\nThe table shows that no stereotyping results in the lowest mean score on the math test. Positive stereotyping results in the highest mean score, and negative stereotyping is in between.\n\nRun an ANOVA: (Analyze -&gt; general linear model -&gt; univariate). Select MATH as the dependent variable and STEREO as fixed factor.\nWrite down the null and alternative hypothesis of the ANOVA, then check your answer.\n\n\nAnswer\n\nH0: \\(\\mu1 = \\mu2 = \\mu3\\) H1: not \\(\\mu1 = \\mu2 = \\mu3\\)\n\nTrue or false: The effect of stereotyping is significant (use \\(\\alpha\\)=0.05). \nTRUE\nFALSE\n\n\nAnswer\n\nYes, the F-test for the effect of Stereotyping on Math performance is significant, F(2,27) = 5.614, p = .009. Hence, we reject H0.\nWe have convincing evidence that, also at the population level, the means differ.\nIn other words, we have convincing evidence that the mean differences in math performance are not the result of sampling fluctuations but reflect true differences due to the manipulation (i.e. stereotyping).\n\nHow large is the R-square? \nHow do you interpretat this value of the R-square?\n\n\nAnswer\n\nThe R2 is 0.294.\nThis means that 29.4% of the variance in Math performance is explained by the Stereotyping.\n\nThe R-square should be equal to:\n\nThe ratio of the sum of squares for STEREO to the (corrected) total sum of squares.The ratio of the mean square for STEREO to the error mean square.The ratio of the sum of squares for STEREO to the error sum of squares.The ratio of the mean square for STEREO to the (corrected) total mean square.\n\nTest whether there is an effect of stereotyping (regardless of whether it is positive or negative stereotyping).\nTrue or false: The effect of stereotyping (regardless of whether it is positive or negative stereotyping) is significant. \nTRUE\nFALSE\nReport Levene’s test, significance of the contrast that you tested, and an interpretation of the difference between the two means.\n\n\nAnswer\n\nThe Levene’s test is not significant (p = .805), so there is no evidence for violation of the assumption of homoscedasticity.\n\nTest the mean math score for each experimental group against the control group; that is, you have to test two planned contrasts.\nAre the means different when tested at an experiment-wise alpha of .05 and using a Bonferroni corrected alpha per test? Substantiate your answer.\n\n\nAnswer\n\nThe contrast is significant at \\(\\alpha\\) = .05.\nThese results suggest that stereotyping (positive/negative) results in better math performance than non-stereotyping.\n\n\n21.5.1 ANCOVA\nThe next step is to add scholastic aptitude as a covariate in our analysis.\nIn the lecture, we considered two situations in which ANCOVA is used; one in which the covariate was not related to the grouping variable, and one in which the covariate is associated.\nWhich situation do we have in this study? To answer the question, you need to ask for some additional statistics in SPSS.\n\n\nAnswer\n\nWe can check if the covariate is associated with the experimental factor by inspecting the means of the experimental groups on the covariate, or by running a one-way ANOVA.\nThe covariate is associated with the grouping variable, and thus mean differences in math between the three groups may be confounded by mean differences in scholastic aptitude between the three groups.\n\n\n21.5.1.1 Assumption or not?\nSome texts state that ANCOVA has an additional assumption, beyond those of multiple regression. This “extra assumption” is the assumption of “homogeneity in regression slopes”.\nThe assumption of homogeneity in regression slopes states that the within-group effect of the covariate is the same across groups. That is, the covariate does not interact with the grouping variable.\nFor example, in this assignment, the assumption would imply that the effect of scholastic aptitude is independent of the experimental condition.\nBut: ANCOVA is just a special name for multiple regression with a categorical predictor of interest and control variables that we’re not interested in (“nuissance variables”). If ANCOVA is multiple regression, then how can it have differen assumptions than multiple regression?\nThe answer is that ANCOVA does not have different assumptions, but if we were to allow for interaction between the factor and covariate, we would simply no longer call the resulting model an ANCOVA.\nSo instead of saying “ANCOVA has an assumption of homogeneity in regression slopes”, we could say: “it is conventional to call a model ANCOVA if it contains a factor and some control variables, but no interaction”. Of course we can add interaction terms in such a model - but then we just call it multiple regression with an interaction, not ANCOVA.\nOmitting an interaction between the factor and the covariate means that we force the within-group effect of the covariate to be the same across levels of the factor. In this study, that means that we do not allow the effect of scholastic aptitude to depend on the experimental condition.\nBefore we carry out the actual ANCOVA, we can check whether this model is correctly specified, or whether we are missing a significant interaction. We can check whether there is a significant interaction effect using the following syntax:\n    UNIANOVA SA BY STEREO WITH MATH\n      /METHOD=SSTYPE(3)\n      /INTERCEPT=INCLUDE\n      /PRINT=DESCRIPTIVE PARAMETER HOMOGENEITY\n      /CRITERIA=ALPHA(.05)\n      /DESIGN=STEREO MATH MATH*STEREO.\nCopy and run the syntax. Go to the table “Tests of Between-Subjects Effects”. Take a look at the row STEREO*MATH.\nTrue or false: There is a significant interaction effect. \nTRUE\nFALSE\nIf there is a significant interaction effect, what do we do?\n\n\nAnswer\n\nRemember we can check for a significant interaction effect for two reasons:\n\nOur hypothesis is about the interaction effect\nAs an assumption check for correct model specification (i.e., we’re interested in main effects, but we want to make sure that we’re not ignoring an interaction when we do). This is similar to checking for linearity.\n\nIn the first case, we should not be conducting ANCOVA at all; we should start with multiple regression with interaction, because the interaction effect is our effect of interest.\nIn the second case, we can do two things: We can change our analysis based on the results of the “assumption check” - but such data-dependent decisions increase the risk of overfitting and Type I errors. Alternatively, we can just report the results of our planned ANCOVA analysis, but mention in the discussion that we observed a significant interaction, which means that the model may have been misspecified. We can also report results from both models, and see if/how the conclusions change if we allow for interaction (this is called a sensitivity analysis).\n\n\n21.5.1.2 Back to ANCOVA\nLet’s run an ANCOVA, proceed as follows:\n\nNavigate to Analyze &gt; General linear model ? Univariate\nSelect MATH as the dependent variable, STEREO as fixed factor, and SA as the covariate.\nAlso, via OPTIONS ask for the Parameter estimates. Paste and run the syntax.\n\nConsider the Tests of Between Subjects Effects table (henceforth referred to as the “ANCOVA table”) and the FF-test for the grouping factor (STEREO).\nWhat’s the p-value for the overall test of model fit? \nWhat conclusions can be drawn from the F-test?\n\n\nAnswer\n\nThe F-test is a test of the effect of Stereotyping on Math performance controlled for Scholastic aptitude. Conceptually, it tests whether differences in the adjusted means in Math performance are significant. Adjusted means are the means we would expect if the group had an average level of Scholastic aptitude.\nIn other words, it tests the differences in the hypothetical situation we would have had three groups that had exactly the same level of Scholastic aptitude.\nThe F-test is not significant, F(2,26) = 2.333, p = .117, which means that, controlled for Scholastic aptitude, we don’t have convincing evidence that Stereotyping had an effect on Match performance.\n\nCompare the results ANOVA and ANCOVA. What important difference do we see and how would you explain those?\n\n\nAnswer\n\nThe ANOVA suggested a significant effect, whereas once controlled for Scholastic aptitude (ANCOVA) the effect was no longer significant.\nThus, the mean differences between the experimental groups we saw before were indeed confounded with differences in Scholastic aptitude!\n\nConsult the table parameter estimates.\nWhat is the regression slope for scholastic aptitude? \nExplain the meaning of estimated parameter for scholastic aptitude.\n\n\nAnswer\n\nThe parameter estimate for Scholastic aptitude is 0.470. The effect is significant when using \\(\\alpha\\) = .05.\nIt is the pooled within-group regression effect of Scholastic aptitude on Math performance, controlled for Stereotyping.\nThus, if Scholastic aptitude increases by one unit, the predicted Math score increases by .470 units, while controlling for Stereotyping.\n\nBased on the Parameters Estimates and the group-specific means, compute the adjusted group means (for each of the groups!) on MATH for an average scholastic aptitude.\nWhat is the adjusted group mean for the group that received the Negative Stereotype manipulation? \n\\(\\bar{Y}_k^{adj} = \\bar{Y}_k -b_w(\\bar{X}_k- \\bar{X})\\)\nTo use the formula, you need to know the group means on MATH (you computed them before), you have to know the group means and overall mean SA (you can compute them via means), and the regression effect which is given in the table with parameter estimates.\nCheck your adjusted means against this answer model:\n\n\nAnswer\n\n\n\nRerun the ANCOVA. Now via OPTIONS also ask for the estimated means. You do so by selecting stereo in the list of Display Means for (at the top of the menu). Look in the table Estimated Marginal Means and verify your answer to the previous question.\nWrite down in your own words – and as precise as possible – the meaning of adjusted means.\n\n\nAnswer\n\nThe estimated marginal means (i.e., the adjusted means) are the group means if all groups would’ve had an average of 6.20 on the covariate.\n\nFinally, we want to look at several effect size estimates.\nHow much of the variance in Math do SA and STEREO explain? \nControlled for SA, how much of the remaining variance in Math does STEREO explain? Use the formula mentioned in the lecture slides to calculate the partial \\(\\eta^2\\): \nVerify your answer by running the ANCOVA again. Now, in options, select the box Estimates if effect size.\nTrue or false: SPSS reports the same partial η2. \nTRUE\nFALSE\nCould you summarize your findings of the ANCOVA in a few brief sentences?\nMention the significance tests, (un)adjusted means and the effect size estimates.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>GLM: ANCOVA</span>"
    ]
  },
  {
    "objectID": "rmanova.html",
    "href": "rmanova.html",
    "title": "22  GLM: Repeated Measures ANOVA",
    "section": "",
    "text": "22.0.1 Two Repeated Measurements\nRepeated Measures ANOVA is used to analyze data collected in within-participants designs, where the same outcome measure is collected from the same individuals multiple times.\nA study design in which the same participants are assessed repeatedly is called a Within-Participants Design. Within-participants designs have distinct advantages in comparison to between-participants designs. In these designs, participants serve as their own control, eliminating variability due to individual differences from the error term. This intrinsic control enhances statistical power and efficiency. These designs are used, for example, in longitudinal studies, test-retest designs, diary studies, and repeated physiological assessments.\nWhile within-participants designs offer significant advantages, they also present challenges that require careful consideration. Order effects, where the sequence of experimental conditions influences results, are a common concern. Differential order effects, where the influence of order varies across different sequences, can further complicate data interpretation. An example of a differential order effect is wen the effect of a drug administered before a placebo condition persists into the placebo phase of the experiment. To mitigate order effects, researchers often employ the Latin square design. This experimental design ensures each condition appears once in every position within the order, thus minimizing the influence of sequence on outcomes. By controlling for order effects, researchers enhance the internal validity of their experiments.\nBeyond order effects, within-participants designs are also affected by learning- and historical effects. A learning effect occurs when participants’ increasing familiarity with questionnaires affects their subsequent responding. Historical effects occurr when external events happen during the study, and influence participants’ responses. Finally, the effect of time is often confounded with the effect of experimental conditions.\nThe paired samples t-test is suitable for scenarios where participants are measured before and after an intervention. This technique simply analyzes the difference score between pretest and posttest scores.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>GLM: Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "rmanova.html#lecture",
    "href": "rmanova.html#lecture",
    "title": "22  GLM: Repeated Measures ANOVA",
    "section": "\n22.1 Lecture",
    "text": "22.1 Lecture",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>GLM: Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "rmanova.html#formative-test",
    "href": "rmanova.html#formative-test",
    "title": "22  GLM: Repeated Measures ANOVA",
    "section": "\n22.2 Formative Test",
    "text": "22.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nIn Repeated Measures ANOVA, what type of experimental design involves the same participants being exposed to multiple conditions?\n\nWithin-participants designBetween-participants designCross-sectional designLongitudinal design\n\nQuestion 2\nWhich of these is NOT a methodological concern in within-participants designs?\n\nInteraction effectsOrder effectsHistorical effectsLearning effects\n\nQuestion 3\nHow does the Latin square design address order effects in experiments?\n\nRandomly assigns participants to different conditionsManipulates the order of conditions for experimental purposesSelects a single condition for all participantsControls for order effects by ensuring each condition appears once in every position\n\nQuestion 4\nWhat is the primary advantage of within-participants designs in comparison to between-participants designs?\n\nElimination of variability due to individual differences from the error termLarger sample sizesGreater external validityLower cost\n\nQuestion 5\nWhat statistical technique is commonly used to analyze data collected in within-participants designs with two repeated measurements?\n\nChi-square testPaired samples t-testAnalysis of varianceIndependent samples t-test\n\nQuestion 6\nWhat assumption of the general linear model is violated when analyzing data with repeated measurements?\n\nNormal distribution of errorsIndependence of errorsHomogeneity of variancesLinearity of relationships\n\nQuestion 7\nWhat is the purpose of using a multivariate approach in analyzing data with more than two repeated measurements?\n\nTo calculate mean differences between measurementsTo ignore the repeated measurementsTo assess the order effects in the dataTo perform a test that is robust to violations of sphericity\n\nQuestion 8\nWhat is the key assumption in the multivariate approach for analyzing data with repeated measurements?\n\nIndependence assumptionNormality assumptionSphericity assumptionHomoscedasticity assumption\n\nQuestion 9\nIn mixed design ANOVA, what type of factors are considered?\n\nOnly within-participants factorsBoth within-participants and between-participants factorsOnly between-participants factorsCategorical and continuous factors\n\nQuestion 10\nWhat does the term ‘sphericity’ refer to in the context of repeated measures ANOVA?\n\nThe distribution of the residualsEqual variances and correlations of differences scores of all pairs of repeated measurementsThe shape of the distribution of the outcomesThe distribution of the predictor variables\n\n\n\n\nShow explanations\n\nQuestion 1\nIn a within-participants design, the same participants are exposed to different conditions, allowing for the comparison of outcomes within the same individuals.\nQuestion 2\nOrder effects refer to the potential impact of the sequence in which conditions are presented on the observed outcomes. Learning effects imply that participants respond to a questionnaire differently when they already know the questions. Historical effects mean that something external happens while you are running the experiment. Interaction effects are a statistical term.\nQuestion 3\nThe Latin square design helps control for order effects by ensuring that each condition appears in each position within the order an equal number of times.\nQuestion 4\nWithin-participants designs allow each participant to serve as their own control, effectively removing variability due to individual differences from the error term. The cost is often indeed lower, but that’s not the primary advantage.\nQuestion 5\nThe paired samples t-test is used to analyze the differences between two related measurements, such as pretest and posttest scores.\nQuestion 6\nRepeated measurements within the same individuals violate the assumption of independence of errors, as observations from the same participant are likely to be correlated.\nQuestion 7\nA multivariate approach is robust to the assumption of sphericity because it considers the interrelationships between different repeated measurements, treating them as correlated outcomes.\nQuestion 8\nThe sphericity assumption assumes that the variances and correlations among all pairs of repeated measurements are equal, which is essential for accurate results.\nQuestion 9\nMixed design ANOVA involves the consideration of both within-participants and between-participants factors to understand the interactions between these factors on the outcomes.\nQuestion 10\nSphericity refers to the assumption that the variances and correlations among all difference scores between pairs of repeated measurements are equal, which is crucial for accurate analysis.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>GLM: Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "rmanova.html#tutorial",
    "href": "rmanova.html#tutorial",
    "title": "22  GLM: Repeated Measures ANOVA",
    "section": "\n22.3 Tutorial",
    "text": "22.3 Tutorial\n\n22.3.1 Repeated Measures ANOVA\nIn this tutorial, we will explore how to perform a repeated-measures ANOVA using SPSS to assess the effect of repeated measurements of depression symptoms in a sample of military veterans. The primary objective is to determine whether there are significant changes in depression symptom scores across multiple time points.\nLoad the dataset called depression.sav containing depression symptom scores at different time points for each participant.\n\nClick on “Analyze” in the top menu and select “General Linear Model” and then “Repeated Measures.”\n\n\n22.3.1.1 Defining the Within-Subjects Factor\n\nIn the “Repeated Measures” dialog box, name your within-subjects factor as “time.”\nSpecify the number of levels as 4 (since there are four repeated measurements).\nClick the “Add” button.\n\n22.3.1.2 Defining Within-Subjects Variables\n\nClick on the “Define” button to configure within-subjects variables.\nIn the “Repeated Measures” dialog box, move the variables corresponding to each time point (e.g., scl1, scl2, scl3, scl4) to the “Within-Subjects Variables” box while maintaining their correct order.\n\nConfiguring Options\n\nClick the “Options” button.\nCheck the boxes for “Descriptive statistics” and “Estimate of effect size.”\nClick “Continue.”\n\nRunning the Test\n\nClick “OK” to run the repeated-measures ANOVA.\nThe result will appear in the Output Viewer.\n\nInterpreting the Result\nDescriptive Statistics\nThe descriptive statistics provide insight into the direction of any potential effect. The means comparison shows the average depression symptom scores at different time points.\nTrue or false: There is an increase in symptoms over time. \nTRUE\nFALSE\nAssumption of Sphericity\nSPSS tests assumption of sphericity using Mauchly’s test of sphericity.\nTrue or false: In this analysis, the assumption of sphericity is met. \nTRUE\nFALSE\nTrue or false: According to the Huyn-Feldt estimate of epsilon, the deviation from sphericity is small. \nTRUE\nFALSE\nLet’s assume sphericity for now. Choose the appropriate test and correction based on this assumption.\nWhat is the appropriate F-value for the chosen test? \nWhat is the appropriate df for the chosen test? \n\n22.3.2 Pairwise Comparisons\nExamine the table of pairwise comparisons.\nWhich difference is smallest? \nT2 v T3\nT2 v T4\nT1 v T2\nT3 v T4\nIf you were to use Bonferroni correction to control for multiple comparisons, you would divide the experiment-wise alpha level by the number of comparisons. How many comparisons are you making here? \nReport your results. Make sure to reference both the RM-ANOVA test, and post hoc comparisons with Bonferroni correction. Then, check your answer.\n\n\nAnswer\n\n“A repeated-measures ANOVA revealed a significant effect of time on depression symptom scores, F(3, 2931) = 7.29, p &lt; .001. For post hoc pairwise comparisons, we applied a Bonferroni correction. Since there are 6 comparisons between 4 time points, we established the alpha level as .05/6 = .008. Using this alpha level, we found that the mean depression symptom score increased significantly from T1 to T3 (Mean difference = .29, p = .003), and from T1 to T4 (Mean difference = .41, p &lt; .001). These results suggest that depression symptoms increased significantly over time for the military veteran sample.”",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>GLM: Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "reliability_validity.html",
    "href": "reliability_validity.html",
    "title": "23  Reliability and Validity",
    "section": "",
    "text": "23.0.1 Classical Test Theory\nQuestionnaires are widely used in the social sciences to measure a variety of constructs, including self-reported behavior, beliefs, knowledge, opinions, values, attitudes, and attributes. Crucially, oftentimes the same construct is measured with multiple questions. Researchers may want to know whether these questions do a good job at measuring the construct of interest. This is the topic we will explore this week.\nRecall that a construct is an abstract feature of interest within a population, such as intelligence, perseverance, or education. Some constructs can be either observed (directly measured); other constructs are latent (measured indirectly through observed indicators, which can be questions).\nExamples of observed constructs are height and weight. Latent constructs cannot be directly measured and require observed indicators to capture their underlying meaning. Latent constructs are commonly used in social sciences to measure attitudes, beliefs, opinions, and other complex attributes. The items, more often than not, are questions in a questionnaire - although they could also be tests, performance assessments, behavioral observations, multiple choice questions, puzzles, et cetera.\nClassical Test Theory posits that observed test scores are a function of the true score on the latent construct, plus measurement error. Different sources of measurement error can influence observed scores, including instrument properties and individual factors.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reliability and Validity</span>"
    ]
  },
  {
    "objectID": "reliability_validity.html#lecture",
    "href": "reliability_validity.html#lecture",
    "title": "23  Reliability and Validity",
    "section": "\n23.1 Lecture",
    "text": "23.1 Lecture",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reliability and Validity</span>"
    ]
  },
  {
    "objectID": "reliability_validity.html#formative-test",
    "href": "reliability_validity.html#formative-test",
    "title": "23  Reliability and Validity",
    "section": "\n23.2 Formative Test",
    "text": "23.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat does reliability refer to in psychometrics?\n\nThe extent to which an instrument measures what it is intended to measure.The average covariance between items and the average variance of items.The clarity and readability of questionnaire items.Consistency or stability of test scores over time or across measurement occasions.\n\nQuestion 2\nWhich type of reliability assesses consistency across repeated administrations of the same test?\n\nContent reliabilityInter-rater reliabilityTest-retest reliabilityInternal consistency\n\nQuestion 3\nWhat is the primary purpose of content validity?\n\nTo ensure that the instrument adequately covers all aspects of the construct being measured.To examine the correlation between individual items and the total scale score.To assess whether items in the instrument are clearly related to the construct of interest.To determine the association between the instrument and external measures.\n\nQuestion 4\nWhich type of validity assesses whether an instrument is associated with relevant outcomes or indicators of the construct?\n\nContent validityCriterion validityFace validityExternal consistency\n\nQuestion 5\nWhat is the key focus of face validity?\n\nThe apparent relevance and clarity of the instrument's items.The relationship between items within a test.The association between repeated assessments of the test.The stability of test scores over time.\n\nQuestion 6\nWhich measure of internal consistency is based on the average covariance between items and the average variance of items?\n\nCronbach's alphaMcDonald's OmegaVarianceTest-retest correlation\n\nQuestion 7\nWhat is the relationship between reliability and validity?\n\nReliability is a necessary condition for validity.You can't have reliability without validity, and vice versa.Validity is a necessary condition for reliability.Reliability and validity are independent.\n\nQuestion 8\nIf an item is contra-indicative, which statement is likely true about the item-total correlation for that item?\n\nIt is negativeIt is positiveIt is near-zeroIt is small (e.g., below .30)\n\nQuestion 9\nWhat is true about alpha-if-item-deleted?\n\nIt is a diagnostic tool that can help you identify items that don't work well with the rest of a questionnaire.It is a decision criterion to eliminate items from a questionnaire.It is a diagnostic tool to help you identify items that are not valid.It is a diagnostic tool that can help you identify unreliable items.\n\n\n\n\nShow explanations\n\nQuestion 1\nReliability refers to the consistency or stability of test scores over time or across different measurement occasions.\nQuestion 2\nTest-retest reliability assesses consistency across repeated administrations of the same test over time.\nQuestion 3\nContent validity involves ensuring that the instrument comprehensively covers all aspects of the construct being measured.\nQuestion 4\nCriterion validity assesses whether an instrument is associated with outcomes or indicators of the construct it is designed to measure.\nQuestion 5\nFace validity focuses on the apparent relevance and clarity of the instrument’s items.\nQuestion 6\nCronbach’s alpha estimates the internal consistency of a scale by considering the average covariance between items and the average variance of items.\nQuestion 7\nReliability is a necessary condition for validity, meaning that an instrument must be consistent and stable to accurately measure the intended construct.\nQuestion 8\nContra-indicative items measure the opposite of what the scale measures, so they should correlate negatively with the scale.\nQuestion 9\nAlpha-if-item-deleted tells you what Cronbach’s alpha of a questionnaire would be if you left out that item. You should not follow this blindly, but you can use it to identify items that don’t work well with the rest of the scale. This says nothing about the reliability of individual items, nor their validity.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reliability and Validity</span>"
    ]
  },
  {
    "objectID": "reliability_validity.html#tutorial",
    "href": "reliability_validity.html#tutorial",
    "title": "23  Reliability and Validity",
    "section": "\n23.3 Tutorial",
    "text": "23.3 Tutorial\n\n23.3.1 Norm Violating Behaviors\nIn this assignment we are going to take a look at scale that measures whether people engage in norm violating behaviors.\nThe scale consists of the following items:\n\nJoyriding\nTaking soft drugs\nAccepting a bribe\nThrowing away litter\nDriving under influence of alcohol\nSmoking in public places\nSpeeding over limit\nEuthanasia\n\nOpen the datafile evs.sav.\nIn SPSS, navigate to Analyze –&gt; Scale –&gt; Reliability Analysis.\nSelect the seven items that are in this scale.\nThen go to statistics and select the options “Item”, “Scale” and “Scale if item deleted”. Click on continue.\nNow paste and run the syntax.\nWhat is the value of Cronbach’s Alpha? \nFinish the following sentence.\nThis Cronbach’s Alpha is \nPoor\nQuestionable\nAdequate\nGood\nCronbach’s Alpha is an estimate of the scale reliability.\nDescribe in your own words what scale reliability entails.\n\n\nAnswer\n\nThe reliability of a scale shows the internal consistency of the answers on all items on a scale.\n\nLook at the “Item-Total Statistics” table.\nWhat does the last column “Cronbach’s alpha if Item Deleted” tell you?\n\n\nAnswer\n\nThe values in the column “Cronbach’s Alpha if item deleted” shows the Cronbach’s Alpha of the scale if one of the items would be deleted. In other words, it shows what the impact on the reliability of the scale would be if a certain item would be excluded from the scale.\n\nIf you would were examining the psychometric properties of this scale, which items would you consider to give cause for concern?\n\n\nAnswer\n\nWe look at the Cronbach’s Alpha if item deleted to see the internal consistency of a scale without that item.\nGiven the Cronbach’s Alpha of our original scale (.693), the table shows the only deleting the item “Euthenasia” would result in a higher Cronbach’s Alpha.\nWe might thus question whether this item belongs in the scale or not.\n\n\n23.3.2 Machiavellianism\nIn this assignment we will look into the latent concept Machiavellianism.\nThe personality trait of Machiavellianism is part of what’s called the “dark triad”, a personality type that is characterized by deceitfulness, cynicism, and an absence of morality and empathy.\nOpen the datafile shortmach2.sav. The data file contains data on Machiavellianism and some other variables.\nThe included Machiavellianism scale consists of the following 20 items. These items are scored on a 1-5 Likert scale ranging from “Disagree” to “Agree”.\nQ1. Never tell anyone the real reason you did something unless it is useful to do so.\nQ2. The best way to handle people is to tell them what they want to hear.\nQ3. One should take action only when sure it is morally right.\nQ4. Most people are basically good and kind.\nQ5. It is safest to assume that all people have a vicious streak, and it will come out when they are given a chance.\nQ6. Honesty is the best policy in all cases.\nQ7. There is no excuse for lying to someone else.\nQ8. Generally speaking, people won’t work hard unless they’re forced to do so.\nQ9. All in all, it is better to be humble and honest than to be important and dishonest.\nQ10. When you ask someone to do something for you, it is best to give the real reasons for wanting it rather than giving reasons which carry more weight.\nQ11. Most people who get ahead in the world lead clean, moral lives.\nQ12. Anyone who completely trusts anyone else is asking for trouble.\nQ13. The biggest difference between most criminals and other people is that the criminals are stupid enough to get caught.\nQ14. Most people are brave.\nQ15. It is wise to flatter important people.\nQ16. It is possible to be good in all respects.\nQ17. P.T. Barnum was wrong when he said that there’s a sucker born every minute.\nQ18. It is hard to get ahead without cutting corners here and there.\nQ19. People suffering from incurable diseases should have the choice of being put painlessly to death.\nQ20. Most people forget more easily the death of their parents than the loss of their property.\nItems can be indicative or contra-indicative of a certain trait.\nWhich items are contra-indicative to the trait Machiavellianism?\n\n\nAnswer\n\nIn this assignment, the following questions are contra-indicative: Q3, Q4, Q6, Q7, Q9, Q10, Q11, Q14, Q16, and Q17.\nIf an item is contra indicative, low scores on these items indicate a lot of Machiavellianist traits, whereas high scores indicate not so much of an endorsement of Machiavellianist traits in one’s personality.\n\nWe need to recode contraindicative items before we carry out reliability analysis.\nHowever, just to see what happens, let’s first carry out a reliability analysis with all original (i.e. not recoded) variables.\nTake the following steps:\nAnalyze –&gt; Scale –&gt; Reliability Analysis\nSelect the 20 items (Q1A - Q20A)\nClick on Statistics\nUnder Descriptives, select Items, Scale, and Scale if item deleted\nUnder Inter-Item, select Correlations\nPaste and run the syntax\nWhat is the estimated reliability? \nTrue or false: This scale can be considered reliable. \nTRUE\nFALSE\nThis low reliability might be a result of the fact that we have not recoded our contra-indicative items yet. We can also see this in the inter-item correlation table; many of the items are negatively correlated! Let’s rectify this.\nWe have to recode all contraindicative items. Use syntax to do so, and check your answer below.\n\n\nAnswer\n\nCOMPUTE Q3r = 6-Q3A.\nEXECUTE.\n\nFor the second person in the dataset, the original score on Q4A was  and the score on the recoded variable Q4r was .\nBased on this comparison, do you think you successfully recoded the variable?\nWe will now run the reliability analysis including the recoded items. Chnge your syntax, or re-run the analysis via the visual interface.\nWhat is the value of Cronbach’s Alpha? \nThis Cronbach’s Alpha is \nPoor\nAdequate\nGood\nQuestionable\nCheck the corrected item total correlations.\nWhich of these items has the smallest association with other items in the scale? (Type its name) \nExplain why you think it makes sense (or not) that this item is correlated the least with the rest of the scale.\n\n\nAnswer\n\nThis item-total correlation of Q19 is .255.\nItem Q19 reads: “People suffering from incurable diseases should have the choice of being put painlessly to death”.\nOne could argue that a high score on this item should relate to a low score on the scale for agreeing with this item nowadays might actually show empathy with those suffering.\n\nInspect Cronbach’s alpha if item deleted. If you had to remove one item based on this Cronbach’s alpha if item deleted, which item would it be? (Type its original name) \n\n\nExplanation\n\nBased on the Cronbach’s Alpha if item deleted, we would delete the item that would result in the greatest increase in Cronbach’s Alpha if it would be deleted.\nWe find the highest Cronbach’s Alpha if item deleted for the item Q17r (.889).\nTherefore, based on the Cronbach’s Alpha if item deleted, we would delete item Q17r.\n\nTaking everything into consideration, would you remove any items from the Machiavellianism scale?\n\n\nExplanation\n\nTaking the statistical output into consideration, we might want to consider removing item Q17r or item Q19.\nWe do so for two reasons:\n\nThey both correlate less than .3 with the other items on the scale, and\nhave a Cronbach’s Alpha if item deleted higher than .887 (the current Cronbach’s Alpha of the scale).\n\nPlease note that we should always take into consideration theoretical reasons as well when deciding to delete an item from a scale. In this assignment, however, we base our conclusions on statistical reasons only.\nYou might want to consider removing item Q17 or Q19. They both correlate less than .3 with the rest of the scale, and have an Alpha if item removed higher than .887.\n\nWe always remove items one by one. We start with the worst item. After removing a bad item from a scale, the item-statistics will change a little. It might be that the item-statistics improve, and that there is no need to remove the second item.\nHowever, since the differences are not that big and the scale reliability is pretty high, we will keep both items in our scale for the remainder of this assignment.\nOnce we finished reliability analysis, we can use the scale in other analyses.\nIn order to do that, we need to arrive at a total scale score. So, we need one score for each person in the dataset that tells what their score is on the personality trait Machiavellianism.\nThere are several methods of obtaining such a total score, but one straightforward and easy way is to calculate the sum score.\nNavigate to Transform -&gt; Compute Variable.\nGive a new name to the sum score in the box Target Variable, such as Mach.\nIn the box Numeric Expression, enter all variables and add together (i.e., Q1A + Q2A + Q3r …. + Q20A). Ensure that you use the recoded variables for the contra-indicative items!\nPaste and run the syntax.\nWe will now use our newly developed scale in a regression analysis! In this analysis we will try to explain Machiavellianism based on the variables Gender (0=men; 1=women), Age and Voted (0=Voted in past election, 1= Not voted in past election).\nNavigate to Analyze –&gt; Regression –&gt; Linear\nEnter the sumscore Mach as dependent variable\nEnter Gender, Age, and Voted as independent variables\nPaste and run the syntax\nInspect the Model Summary table. How much of the variance in Machiavellianism do the variables Gender, Age and Voted explain? \nTrue or false: Gender, Age and Voting together explain a significant amount of variance in the variable Machiavellianism. \nTRUE\nFALSE\nInspect the table Coefficients and take a look at the partial effects. Which of the variables does not have a significant partial effect on Machiavellianism? \nGender\nAge\nVoting\nControlled for Age and Voting, which group (men or women) scores higher on the Machiavellianism scale? \nMen\nWomen\n\n\nExplanation\n\nGiven that in the variable Gender men are coded as 0 and women are coded as 1, and that the regressions coefficient for Gender is negative (-7.440), we should conclude that men score higher on Machiavellianism than women, controlled for Age and Voting.\n\nFinish the following sentence.\nControlled for Gender and Voting, if we would increase one year in age, the predicted score on the Machiavellianism scale would \nIncrease with 0.243 units.\nDecrease with 0.243 units.\nDecrease with 0.294 units.\nIncrease with 0.294 units.\n\n23.3.3 Solidarity\nIn this assignment, you will evaluate the reliability of a scale measuring solidarity.\nDownload and open the following data file: solidarity.sav.\nYou’ve practiced with reliability analysis in the previous assignments. Now you can use that knowledge to evaluate the reliability of the solidarity scale more independently.\nInclude all eleven items that are part of the scale (v266 to v276).\nInspect the output of the reliability analysis.\nTrue or false: You have to recode questions. \nTRUE\nFALSE\n\n\nExplanation\n\nWe check if we should recode any items by looking at how they are phrased. To do so, we don’t necessarily need to look at the output.\nWe could, however, also use the inter-item correlations displayed in the output to check if we should recode items. If any of the inter-item correlations are negative, this should be in indication for contra-indicative items.\n\nWhat is the reliability of the scale? \nIf you had to remove one item from the scale based on Cronbach’s Alpha if item deleted, which one would you pick? Type the name: \nCan you think of other reasons for removing this item?\n\n\nExplanation\n\nComparing the content of item Q80E to the content of the other questions in the scale, we can conclude that the content of this item is off-topic. This is a theoretical reason for excluding item Q80E from the scale.\n\nWhich item is most typical for the scale? Type its name: \n\n\nExplanation\n\nWe can tell from the higher item-total correlation, that tells us that this item has the strongest correlation with all other variables on the scale.\nAlso, the reliability of the scale would decrease most if this item would be deleted from the scale.\n\nLast, construct sum scores on the scale for all individuals (via Transform -&gt; Compute). Make sure you do not include the one item we discussed previously!\nOnce you created the sum score, have SPSS show the mean for this total score.\nWhat is the mean value of the sum score?",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reliability and Validity</span>"
    ]
  },
  {
    "objectID": "data_reduction.html",
    "href": "data_reduction.html",
    "title": "24  Dimension Reduction",
    "section": "",
    "text": "24.1 Principal Components Analysis (PCA)\nIn the previous section, we explored how multiple items can be used to measure a single underlying construct. Today, we will delve into three powerful techniques for reducing multiple items to a smaller number of variables: Principal Components Analysis (PCA), Exploratory Factor Analysis (EFA), and a little bit of Confirmatory Factor Analysis (CFA). Our focus will be on PCA and EFA, as they are particularly useful for understanding underlying structures in social science research. This section will primarily discuss Principal Components Analysis (PCA) and Exploratory Factor Analysis (EFA). These techniques help us explore relationships among items and identify latent constructs that explain the observed patterns in data. They serve as effective tools for dimensionality reduction, enabling us to summarize complex datasets with a smaller set of variables. While we do introduce CFA and reflect on its relationship to EFA, a more in-depth discussion of CFA is beyond the scope of this course.\nThroughout this lecture, assume that we have k items and n participants. Let’s now dive into the details of different data reduction methods.\nPCA is a data rotation technique designed to transform original items into uncorrelated components. These components represent linear combinations of the original items. The primary goal of PCA is dimension reduction, where a small number of components are used to explain most of the variance in the items. This allows us to represent the variance in the items more efficiently. For instance, if ten items measure extraversion, and one component explains most of the variance, we can retain that one component and discard the remaining nine.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "data_reduction.html#exploratory-factor-analysis-efa",
    "href": "data_reduction.html#exploratory-factor-analysis-efa",
    "title": "24  Dimension Reduction",
    "section": "\n24.2 Exploratory Factor Analysis (EFA)",
    "text": "24.2 Exploratory Factor Analysis (EFA)\nUnlike PCA, EFA is a latent variable method that assumes that latent variables (factors) cause people’s responses to the items. For example, extraversion may cause individuals to respond positively to questions about partying and socializing. EFA models the item covariance matrix as a function of a fixed number of factors. It is called “exploratory” because all items are allowed to load on (contribute to) all factors, without a predefined structure. In practice, well-constructed questionnaires will exhibit high loadings of items on one factor and low loadings on others.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "data_reduction.html#confirmatory-factor-analysis-cfa",
    "href": "data_reduction.html#confirmatory-factor-analysis-cfa",
    "title": "24  Dimension Reduction",
    "section": "\n24.3 Confirmatory Factor Analysis (CFA)",
    "text": "24.3 Confirmatory Factor Analysis (CFA)\nConfirmatory Factor Analysis (CFA) tests a theory about the specific associations between latent variables and observed indicators. Unlike Principal Components Analysis (PCA) and Exploratory Factor Analysis (EFA), which are exploratory, CFA is a confirmatory approach that tests how well a hypothesized measurement model fits the data. In CFA, researchers specify a theoretical model that defines the relationships between observed variables and latent constructs (factors). These latent constructs are not directly measured but are assumed to explain the correlations among the observed variables. The primary goal of CFA is to evaluate whether the data support the hypothesized model. By doing so, researchers can determine if their theoretical model fits the observed data well, providing evidence for the validity of the underlying construct and the measurement instrument. CFA is part of a family of statistical modeling techniques known as “Structural Equation Modeling” (SEM).\n\n24.3.1 Comparing Method\nPurpose:\n\nPCA: Dimensionality reduction.\nEFA: Exploration of relationships among items and identification of latent constructs.\nCFA: Testing a predefined theory about which items relate to specific latent constructs.\n\nAssumption:\n\nPCA: Does not assume latent variables; dropping components assumes they are irrelevant or represent error variance.\nEFA: Assumes all items are caused by a smaller number of latent variables (factors).\nCFA: Assumes specific items are caused by specific latent variables.\n\nInterpretation:\n\nPCA: Components are mathematical constructs with no further meaning.\nEFA: Factors represent theoretical latent constructs.\nCFA: Factors represent known theoretical latent constructs.\n\n24.3.2 Principal Components Analysis\nPCA is a data rotation technique that aligns the largest amount of variance with the first component, the second-largest variance with the second component, and so on. These components are uncorrelated by definition, and they serve as linear combinations of the original items. The primary use of PCA is dimension reduction by retaining only components that explain a significant amount of variance, thus providing a lower-dimensional representation of the data.\nWe can understand PCA in different ways. Firstly, as rotation of the data. PCA rotates the data so that the first component best reproduces the correlation matrix, and each subsequent component improves the reproduction. Secondly, we can understand PCA as a way to summarize k items using fewer than k components, without significant information loss (lossy compression of data).\nSelecting the Number of Components:\nVarious strategies exist to determine the number of components to retain, including Kaiser’s criterion (Eigenvalue &gt; 1), Cattell’s scree plot (inflection point), and Horn’s Parallel Analysis (comparison with random data’s Eigenvalues). Additionally, theoretical knowledge about the underlying data can guide the choice of components.\nInterpreting PCA Loadings:\nInterpreting PCA loadings can be challenging, especially in cases where multiple components are correlated. Orthogonal rotation, such as Varimax, can be employed to simplify the pattern of loadings and improve interpretability. However, it is essential to remember that rotated loadings should not be directly interpreted as correlations between items and factors as in PCA.\n\n24.3.3 Exploratory Factor Analysis\nEFA is a model-based approach that assumes the existence of latent variables that cause item responses. It is suitable when you expect clusters of items to be correlated (multicollinear) and seeks to explain correlations between items. EFA assumes that unexplained variance in the items can be attributed to measurement error. This aligns with test theory, where it is assumed that observed items measure latent constructs with error. EFA is particularly suitable when there is a theoretical basis for assuming the existence of latent variables, such as when developing a new questionnaire that has not been validated yet. However, if a theoretical model already exists, Confirmatory Factor Analysis (CFA) may be more appropriate.\nTo conduct EFA, we estimate the unknown factor loadings. Two common estimation methods are Principal Axis Factoring (PAF) and Maximum Likelihood (ML). PAF is a default method in SPSS and is based on an iterative procedure involving matrix algebra. It provides a solution even when the model is complex or the data are non-normal. On the other hand, ML is the same estimator used for CFA and works well when the data are multivariate normal. However, ML may not perform well when the model is overly complex (which is not necessarily a bad thing). ML estimation also allows for a test of model fit, which is useful for evaluating the appropriateness of the chosen model.\nFactor loadings represent the correlations between each item and the extracted factors. They indicate the strength and direction of the relationship between the observed item and the underlying factor. Factor loadings range from -1 to +1, with values closer to 1 indicating a stronger relationship. In our example, we can see the factor loadings in a factor matrix, where each row corresponds to an item and each column corresponds to a factor. The factor loadings help us identify which items load more strongly on specific factors.\nWe can compute Eigenvalues in EFA just as in PCA by taking the column sums of the squared loadings and indicate the amount of variance explained by each factor. Eigenvalues are always smaller than the initial eigenvalues obtained in Principal Component Analysis (PCA) because some variance is now attributed to error variance. Consequently, the sum of the Eigenvalues is also less than the number of indicators, and some Eigenvalues may even be negative.\nSimilarly, communalities in EFA are always &lt; 1 because EFA assumes the existence of error variance.\n\n24.3.3.1 Selecting the Number of Factors\nDetermining the appropriate number of factors to extract is a critical step in EFA. Researchers often use eigenvalues as a cue to determine the number of factors to extract, similar to the Kaiser’s criterion and Scree plot used in PCA - but note that in EFA, this can be misleading as Eigenvalues now depend on the number of extracted factors. Also, by default, SPSS applies Kaiser’s criterion and the Scree plot to PCA Eigenvalues, even if you request EFA!\nAn alternative criterion for determining the number of factors is using theoretical knowledge to guide the decision. For example, if emotions are believed to break down into positive and negative emotions, we may choose to extract two factors. Additionally, the chi-square test can be used to evaluate the appropriateness of different factor solutions and assist in selecting the best-fitting model. To directly compare models, one can compute the Bayesian Information Criterion (BIC) - a relative model fit index designed for comparing models, which balances model fit and complexity. It is computed from the chi square as follows:\n\\[\nBIC = \\chi^2 - df ∗ log(n)\n\\]\n\n24.3.4 EFA Assumption Checks\nBefore conducting exploratory factor analysis (EFA), it is good practice to perform several assumption checks to ensure the validity and appropriateness of the analysis. One critical aspect to consider is multicollinearity. While factor analysis aims to identify clusters of items that are correlated, excessive multicollinearity can lead to issues. This occurs when multiple items are perfectly linearly dependent, meaning that one item’s score can be exactly reproduced using other variables. In such cases, it becomes difficult to discern the unique contribution of collinear items to the underlying factor model. To detect multicollinearity, researchers can examine the determinant, a value between 0 and 1. It has been argued that the determinant should be greater than 0.00001, which indicates multicollinearity is not too high.\nAnother assumption check for EFA is the proportion of common variance among items. The Kaiser-Meyer-Olkin (KMO) statistic provides an estimate of this proportion. A higher KMO value indicates that more of the variance among items can be explained by common factors, making the data more suitable for factor analysis. Researchers can interpret the KMO value as follows:\n\n\nValue\nInterpretation\n\n\n\n0.00 to 0.49\nunacceptable\n\n\n0.50 to 0.59\nmiserable\n\n\n0.60 to 0.69\nmediocre\n\n\n0.70 to 0.79\nmiddling\n\n\n0.80 to 0.89\nmeritorious\n\n\n0.90 to 1.00\nmarvelous\n\n\n\n24.3.5 Rotating Factor Loadings\nIn factor analysis, we aim to interpret the underlying structure of observed variables. The pattern of factor loadings is crucial in this process, helping us identify items that load highly on specific factors and potentially naming those factors based on high-loading indicators. In a perfect world, factor loadings would be clear and straightforward, with each item loading highly on only one factor. However, real-life factor loadings are not always so clear-cut, making interpretation more challenging.\nTo improve interpretability, we use rotation, which applies a linear transformation to the original factor loadings. Two main types of rotation are orthogonal and oblique rotation. Orthogonal rotation produces uncorrelated factors. The most common technique is VARIMAX rotation, which maximizes the variance of the squared loadings within each factor. Oblique rotation allows factors to correlate; the most common technique is oblimin rotation. In the social sciences, it is often sensible to allow factors to correlate (e.g., different personality dimensions are probably associated).\nOne-Factor EFA and One-Factor CFA:\nAlthough this course is not about confirmatory factor analysis, it is nevertheless useful to know that a one-factor EFA model is identical to a one-factor CFA model. In other words, if our theory implies a one-factor model, we can use exploratory factor analysis (EFA) with maximum likelihood (ML) estimation to test that model. While EFA aims to identify underlying factors without any preconceived hypotheses about their association with items, CFA tests a hypothesized model - in this case, that one factor explains all item scores. CFA with ML estimation produces a chi-square test that can be used to assess model fit. Note, however, that this test can be sensitive to sample size and may reject good models. Researchers can also use the Root Mean Square Error of Approximation (RMSEA) as an alternative model fit index, where values below 0.08 indicate good fit. RMSEA is calculated from the chi square as:\n\\[\nRMSEA = \\frac{\\sqrt{\\chi^2 - df}}{\\sqrt{(n - 1)*df}}\n\\]\nTreating a one-factor EFA as CFA also allows us to estimate latent variable reliability. Recall that Cronbach’s alpha assumes that all items are equally important. This means that it assumes that all factor loadings are the same. Factor analysis tests this assumption. Especially when factor loadings differ, it may be useful to compute latent variable reliability instead, using McDonald’s Omega (or composite reliability). It allows for different factor loadings, making it more appropriate for cases where items have varying contributions to the latent variable. The formula for McDonald’s Omega is:\n\\[\n\\omega = \\frac{SSL}{SSL+SSR} = \\frac{\\text{Sum of Squared Loadings}}{SSL + \\text{Sum of  Squared Residuals}}\n\\]\nCalculate SSL as: \\(SSL = (\\sum_{j=0}^k L_{1,k})^2\\) (first sum loadings, then square sum)\nCalculate SSR as: \\(SSR = 1-\\sum_{j=0}^k L_{1,k}^2\\) (first square loadings, then sum)\n\n24.3.6 Estimating Factor Scores\nIn many cases, researchers want to conduct further analyses using idividuals’ scores on components or latent variables. In previous sections, we learned about two common methods for obtaining scale scores from multiple items: sum scores and mean scores. In sum scores, we add up the responses from each item to create a total score for each individual. Similarly, in mean scores, we take the average of the responses from all items to obtain a score. In both cases, all items contribute equally to the final scale score. However, this approach assumes that all items are equally important, which might not always be the case. PCA and EFA both allow us to determine whether items are indeed equally important. We can also try to compute scale scores that take differences in item loadings into account.\nFor PCA, computing such scores is straightforward; these are simply given by multiplying the loadings for one component with the observed item scores. Since this is not a latent variable technique, there is only one possible solution to this calculation. To compute a PCA score for a specific individual, we multiply their standardized item scores by the corresponding factor loadings and then sum the results. For instance, if an individual has standardized item scores of 1, 3, and 2 on items with factor loadings of 0.85, 0.80, and 0.14, respectively, their PCA score would be calculated as \\((0.85 * 1 + 0.80 * 3 + 0.14 * 2) / (0.85^2 + 0.80^2 + 0.14^2) = 2.44\\). This score represents the individual’s relative level on the component.\nEstimating latent variable scores in exploratory factor analysis (EFA) is more complex compared to PCA. Unlike PCA, which provides unique factor scores for each individual, EFA does not uniquely determined factor scores. An infinite number of latent variable datasets is consistent with the same EFA model. To estimate factor scores, researchers use methods like the regression method and the Bartlett method. The regression method involves ordinary least squares estimates and aims to maximize the multiple correlation between factor scores and common factors. However, these estimates are biased and the estimated factor scores correlate with one another and with the different latent variables. The Bartlett method produces factor scores that only correlate with their own latent variable but still correlate with estimated scores for other factors. Both methods thus have shortcomings. Some (see references below) have argued that it might be preferable to simply use mean scores instead of factor scores. In cases where factor loadings are approximately equal, this is probably fine.\nFurther reading:\nEveritt, B. S., & Howell, D. C. (2005). Encyclopedia of Statistics in Behavioral Science. DOI:10.1002/0470013192.bsa726 DiStefano, C., Zhu, M., & Mindrila, D. (2009). Understanding and Using Factor Scores: Considerations for the Applied Researcher. DOI:10.7275/da8t-4g52",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "data_reduction.html#lecture",
    "href": "data_reduction.html#lecture",
    "title": "24  Dimension Reduction",
    "section": "\n24.4 Lecture",
    "text": "24.4 Lecture",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "data_reduction.html#formative-test",
    "href": "data_reduction.html#formative-test",
    "title": "24  Dimension Reduction",
    "section": "\n24.5 Formative Test",
    "text": "24.5 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat is the primary goal of Principal Components Analysis (PCA)?\n\nModel confirmationFactor extractionDimension reductionData classification\n\nQuestion 2\nIn Exploratory Factor Analysis (EFA), what is the role of latent variables?\n\nThey confirm pre-defined structuresThey represent linear combinations of original itemsThey cause people's responses to the itemsThey are directly measured\n\nQuestion 3\nWhat distinguishes Confirmatory Factor Analysis (CFA) from PCA and EFA?\n\nIt assumes that factors cause item responsesIt is an exploratory methodIt tests a hypothesized measurement modelIt is used for dimension reduction\n\nQuestion 4\nHow are components ordered in PCA?\n\nRandomly assigns variance to componentsLargest variance with the first component, second-largest with the second, and so onAligns smallest variance with the first componentEqually distributes variance across all components\n\nQuestion 5\nWhat is one way to understand PCA?\n\nAs a technique for classification of individualsAs a process for data duplicationAs a method for data expansionAs a method of lossy compression of data\n\nQuestion 6\nWhat is the purpose of using orthogonal rotation, like Varimax, in PCA?\n\nTo directly interpret loadings as correlationsTo simplify the pattern of loadings and improve interpretabilityTo increase the correlation between items and factorsTo reduce the number of components retained\n\nQuestion 7\nWhat distinguishes Principal Axis Factoring (PAF) from Maximum Likelihood (ML) estimation?\n\nML is only suitable for CFAPAF provides a solution even in complex models or non-normal dataPAF allows for a test of model fitML always results in higher factor loadings\n\nQuestion 8\nHow are Eigenvalues computed differently in EFA compared to PCA?\n\nEigenvalues are larger in EFAEigenvalues are the same in both EFA and PCAEigenvalues are smaller in EFA as some variance is attributed to error varianceEFA does not compute Eigenvalues\n\nQuestion 9\nWhat indicates a problem with multicollinearity in Exploratory Factor Analysis (EFA)?\n\nA determinant value lower than 0.00001A determinant value equal to 1No presence of multicollinearityA high Kaiser-Meyer-Olkin (KMO) statistic\n\nQuestion 10\nWhich of these is a method for estimating latent variable scores in EFA?\n\nUsing mean scores of all itemsRegression methodSum scoresAssigning equal weights to all items\n\n\n\n\nShow explanations\n\nQuestion 1\nThe primary goal of PCA is dimension reduction, where a small number of components are used to explain most of the variance in the items.\nQuestion 2\nIn EFA, latent variables (factors) are assumed to cause people’s responses to the items, unlike PCA where components represent linear combinations of original items.\nQuestion 3\nCFA is a confirmatory approach that tests how well a hypothesized measurement model fits the data, unlike PCA and EFA, which are exploratory.\nQuestion 4\nPCA aligns the data such that the largest amount of variance is with the first component, and each subsequent component accounts for the next largest variance.\nQuestion 5\nPCA can be understood as a way to summarize k items using fewer than k components, which can be seen as a form of lossy compression of data.\nQuestion 6\nOrthogonal rotation, such as Varimax, is employed in PCA to simplify the pattern of loadings, making them easier to interpret.\nQuestion 7\nPAF is an iterative method suitable for complex models or non-normal data, whereas ML, used for both EFA and CFA, is better for data that are multivariate normal.\nQuestion 8\nIn EFA, Eigenvalues are always smaller than in PCA because some variance is attributed to error variance, leading to Eigenvalues that are less than the number of indicators and sometimes negative.\nQuestion 9\nIn EFA, a determinant value lower than 0.00001 indicates excessive multicollinearity, making it difficult to discern the unique contribution of collinear items to the factor model.\nQuestion 10\nIn EFA, factor scores can be estimated using methods like the regression method or the Bartlett method, each with its own advantages and shortcomings.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "data_reduction.html#tutorial",
    "href": "data_reduction.html#tutorial",
    "title": "24  Dimension Reduction",
    "section": "\n24.6 Tutorial",
    "text": "24.6 Tutorial\n\n24.6.1 PCA\nOpen the data file: emotions.sav.\nThe data file consists of data from the International College Survey 2001 (Diener and colleagues, 2001). In this survey, data on emotions was collected for 41 countries. The data you’ll analyze in this assignment is about norms for experiencing/expressing 12 emotions in Belgium.\n\n\nLet’s look at the data. The first two columns contain the number of the participant and the nation, so you don’t need to include them in the analysis.\nTrue or false: There are missing data. \nTRUE\nFALSE\nSuppose we are only interested in reducing the number of dimensions of the data, which method would you use? \nConfirmatory Factor Analysis\nPrincipal Component Analysis\nExplanatory Factor Analysis\nPath Analysis\nNavigate to Analyze → Dimension Reduction → Factor in SPSS\nIn the tab Extraction: choose the correct method.\nAlso enable the option Scree plot and specify which variables need to be included in the analysis.\nCheck the Options tab. Can you determine what method is used to deal with missing data?\n\nAll cases with missing values are removed prior to analysisNo action is takenAll correlations are computed based on available data for that pair of variablesAll missing values are removed prior to analysis\n\nPaste the syntax and run the analysis.\nTake a look at the output.\nWhat number of component have an Eigenvalue greater than 1 (Kaiser’s criterion)? \nHow many components does the scree plot suggest?\n\nRedo the analysis with the number of components you need to retain according to the scree plot.\nYou can specify the number of components in the Extraction menu of the Factor Analysis window.\nClick Fixed number of factors and enter the number of components (2).\nRun the analysis and look at the loadings in the Component matrix.\nTrue or false: This solution is easy to interpret. \nTRUE\nFALSE\nThe two principal components seem to correspond with positive emotions (appropriate and valued), and negative emotions (inappropriate and not valued), but there is not enough simple structure (too many variables have a high loading on both components).\nTo aid interpretation, you could rotate the solution. Which type of rotation is most appropriate here? \northogonal\noblique\n\n\nAnswer\n\nIt is unlikely that positive and negative emotions are uncorrelated! An oblique rotation seems by far the most sensible choice.\n\nRegardless of your previous answer, redo the analysis and choose Direct Oblimin in the Rotation menu.\nTake a look at the component loadings in the Pattern matrix.\nWhich component would you label Positive Emotions? Number.. \nCompare the component loadings in the Pattern Matrix with the loadings in the Component Matrix.\nWe now observe that the loadings resemble a simple structure more closely than before the rotation: the low loadings are lower and the high loadings are higher.\nNote: Due to the oblique rotation, the loadings are no longer equal to item-component correlations.\nWhat is the correlation between the two rotated components? \nRedo the Principal Component Analysis again one last time to save the component scores in the data set. Open Scores in the Factor Analysis window, check the Save as variables checkbox. Have a look at these component scores (now added to your data set): these are the scores for each person on the two components.\nAlternatively, add this syntax:\n  /SAVE REG(ALL)\nWhat is the component score for the first person on the first component? \nTake a look at the table Total Variance Explained.\nHow much of the variance do the two components together account for? %\nWhat proportion of the variance in the item stress is accounted for by the two components? \nWhich item has the highest unicity? \nCheerful\nAnger\nPride\nHappy\n\n24.6.2 Exploratory Factor Analysis\nWe will move on to work with Exploratory Factor Analysis.\nFor this second assignment you will perform an Exploratory Factor Analysis (EFA) in SPSS on a set of 18 items. These items measure Tolerance and are part of the European Value Survey (EVS).\nDiscuss with your group when we decide to use Exploratory Factor Analysis and when we decide to use Principal Component Analysis.\n\n\nExplanation\n\nPCA is a data reduction technique. We use it when we want to summarize information in the items.\nEFA is used to identify latent variables underlying the measured items. EFA is typically used when a questionnaire has not been validated yet. When we use EFA, we usually do not know exactly which item belongs to which dimension (although we might have an idea based on our theory).\n\nDiscuss with your group: When do we use Confirmatory Factor Analysis?\n\n\nExplanation\n\nCFA is used when we DO know which items belong to which dimension. With CFA we can then check whether the model that we have in mind corresponds with what we see in the data.\n\n\n\n\nOpen the file evs.sav in SPSS.\n\nSelect Factor via Analyze -&gt; Dimension Reduction.\nWhich extraction method should we use if we want a test of model fit? \nMaximum Likelihood\nPrincipal Components Analysis\nPrincipal Axis Factoring\nUnweighted Least Squares\nDrag all items of the tolerance scale (i.e., V225 - V2242) into the ‘items’ window. Go to Descriptives and select the options “Coefficients”, “Determinant”, and “KMO and Bartlett’s test of sphericity”. Then, go to extraction and select “unrotated factor solution” and “scree plot”. Paste and run the syntax.\nWhat is the Determinant? \nTrue or false: The determinant indicates that multicollinearity might be a problem for these data. \nTRUE\nFALSE\nThe factorability, as determined by the KMO index, is \nMiddling\nMarvelous\nMediocre\nHow many factors would you want to select based on the scree plot? \nHow many factors would you want to select based on Kaiser’s criterion? \nWhat are the limitations of using these criteria?\n\n\nAnswer\n\nBoth are based on eigenvalues computed for PCA, but you are performing EFA now.\nAlthough you can also compute eigenvalues for EFA, SPSS doesn’t use those for the scree plot and Kaiser’s criterion - and moreover, eigenvalues for EFA depend on the number of extracted factors, which defeats the purpose of using them to determine how many factors to extract.\nFurthermore, EFA is a theory-driven technique; it makes sense to use theory to determine how many factors to retain.\n\nAssume that we’re extracting two factors for now. Re-do your analysis with the appropriate number of factors.\nIn the tab Extraction: choose the number of factors you want to extract.\nIn the tab Rotation: Tick the box Direct Oblimin.\nIn the tab Options: The interpretation of the pattern matrix is easier if you suppress all coefficients in that table that are small (e.g., values &lt; 0.30). To do so, click on options and ask SPSS to suppress the small coefficients.\nIn the tab Descriptives: Ask for the reproduced matrix.\nPaste and run the syntax.\nWhen we interpret the output of the factor analysis, we inspect 4 tables: the pattern matrix, the communalities, the factor correlation matrix, and the reproduced correlation matrix.\nWe will start with the pattern matrix.\nInspect the factor loadings in the pattern matrix.\nWhich item has the highest absolute factor loading on Factor 2? Type the variable label from the table: \nDecide for yourself: are the two factors clearly interpretable? Then check your answer.\n\n\nAnswer\n\nThe solution almost follows a simple structure where each item loads on one factor. Only for the item Having casual sex do we see high factor loadings on both factors.\n\nInspect the communalities table.\nHow much of the variance in the item “suicide” do the factors explain? \nCheck the correlations between the three factors.\nHow substantial is the correlation between the factors? \nweak\nmoderate\nlarge\nInspect the residual correlations.\nWhich residual correlation is most concerning?\n\nBetween driving under the influence and claiming state benefits.Between Cheating on tax and Paying cashBetween taking soft drugs and joyriding.Between speeding over the limit and smoking in public places.\n\nTake a look at the pattern matrix again.\nCan you think of a meaningful label for each of the factors? (Take into consideration whether the loadings are positive or negative). Then check your answer.\n\n\nAnswer\n\nThere appears to be a distinction between legal and religious issues.\n\n\n24.6.3 Exploratory Factor Analysis II\nOpen the dataset called student_questionnaire.sav.\nIt contains data on moral judgment in a variety of domains of social life (variables whose names start with MACJ). Note that you need the variable MACJ13_imputed, not MACJ13.\n\n24.6.3.1 Model Selection using the BIC\nWhen conducting EFA with ML estimation, we obtain a chi-square test of model fit that allows us to compute the BIC, a comparative fit index that can help us choose the number of factors that best balances model fit and complexity.\nRun an EFA analysis for 1-3 and 7-9 factors. Using syntax can help you do this easily - just copy-paste the basic syntax below four times and change the number of classes:\nFACTOR\n  /VARIABLES MACJ1 MACJ2 MACJ3 MACJ4 MACJ5 MACJ6 MACJ7 MACJ8 MACJ9 MACJ10 MACJ11 MACJ12 MACJ13_imputed\n    MACJ14 MACJ15 MACJ16 MACJ17 MACJ18 MACJ19 MACJ20 MACJ21\n  /MISSING LISTWISE \n  /CRITERIA FACTORS(1) ITERATE(100)\n  /EXTRACTION ML\n  /ROTATION NOROTATE.\nOpen a spreadsheet in Excel or Google Sheets, and copy-paste the chi-square values and degrees of freedom into the first two columns. Obtain the number of (valid) observations using whatever procedure you want (for example, Descriptives).\nAssuming that you have used the first two columns, paste the following formula into the fourth column. Replace “n” with the number of valid observations. Drag the formula down to copy it the all cells in its column:\n= A1 - B1 * LOG(n)\nWhat is the BIC for 3 factors? \nBased on the BIC, out of the set of models compared, which number of factors would you choose? \nTrue or false: This finding corresponds to the conclusion you would draw from the Scree plot. \nTRUE\nFALSE\nTrue or false: This finding corresponds to the conclusion you would draw from Kaiser’s criterion. \nTRUE\nFALSE\nTrue or false: KMO suggests that there is insufficient common variance for factor analysis. \nTRUE\nFALSE\nTrue or false: The determinant suggests a potential problem with multicollinearity. This might be because there are so many similar items. \nTRUE\nFALSE\nIf I told you that the theory specified 7 factors, how many factors would you prefer? Explain why, then check your answer.\n\n\nAnswer\n\nThe BIC for 7 factors is almost identical to the one for 8 factors. If theory dictates 7 factors, you might prefer to stick with 7, as the evidence for 8 factors is not overwhelmingly stronger.\n\n\n24.6.3.2 Latent Variable Reliability\nRegardless of your previous answer, perform EFA with one factor. Recall that this is equivalent to performing CFA with one factor.\nCronbach’s alpha assumes that all items have equal factor loadings. Examine the factor loadings matrix.\nTrue or false: it looks like the factor loadings are indeed all equivalent. \nTRUE\nFALSE\nCompute Cronbach’s alpha for these items, and report the value: \nCopy-paste the factor loadings into a spreadsheet.\nUse the spreadsheet function =SUM() to sum the loadings, then square the sum to get the SSL, \\(SSL = (\\sum_{j=0}^k L_{1,k})^2\\)\nCreate a new column with the squared factor loadings. Use the function =A1^2 (assuming that cell A1 contains your first factor loading). Then sum these squared loadings to get the SSR, \\(SSR = 1-\\sum_{j=0}^k L_{1,k}^2\\).\nFinally, calculate McDonald’s Omega:\n\\[\n\\omega = \\frac{SSL}{SSL+SSR}\n\\]\nReport McDonald’s Omega: \nNote that McDonald’s Omega is larger than Cronbach’s alpha. This is a rule; Cronbach’s alpha underestimates reliability compared to McDonald’s omega, and the underestimation becomes worse as the assumption of equal factor loadings is more violated.\n\n24.6.3.3 Model Fit\nFinally, calculate the RMSEA model fit index for this one-factor model. The cutoff for acceptable fit is RMSEA &lt; .08.\n\\[\nRMSEA = \\frac{\\sqrt{\\chi^2 - df}}{\\sqrt{(n - 1)*df}}\n\\]\nTrue or false: The one-factor model has acceptable fit. \nTRUE\nFALSE",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "confint.html",
    "href": "confint.html",
    "title": "25  BE2 - Confidence Intervals",
    "section": "",
    "text": "25.1 Lecture\nThere is no lecture for this topic, but you can re-watch part of the lecture on the sampling distribution to refresh your knowledge about confidence intervals!",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>BE2 - Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confint.html#formative-test",
    "href": "confint.html#formative-test",
    "title": "25  BE2 - Confidence Intervals",
    "section": "\n25.2 Formative Test",
    "text": "25.2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\n\nQuestion 1\nWhat does a confidence interval represent in statistics?\n\nThe exact value of the population parameter.A range of values within which the true population parameter is likely to fall.The average value of the sample data.The highest value in the sample data.\n\nQuestion 2\nHow is the confidence level of a confidence interval chosen?\n\nBy using standard values such as 95% or 99%.The researcher determines the desired level of confidence in capturing the true parameter.By the size of the sample data.By the range of values in the sample data.\n\nQuestion 3\nWhat does a wider confidence interval indicate?\n\nSmaller sample size.A narrower range of values in the sample data.Higher confidence in the accuracy of the estimate.Greater uncertainty or less precision in estimating the population parameter.\n\nQuestion 4\nIn general, as the sample size increases, what happens to the width of the confidence interval?\n\nIt remains unchanged.It increases.It decreases.It becomes equal to the population parameter.\n\nQuestion 5\nWhat is the relationship between the width of a confidence interval and the level of confidence?\n\nWidth and confidence level are equal.Inverse relationship: Higher confidence leads to a wider interval.Direct relationship: Higher confidence leads to a narrower interval.No relationship: Width and confidence level are independent.\n\nQuestion 6\nWhat is the purpose of a confidence interval in hypothesis testing?\n\nOne can reject a null hypothesis whose hypothesized value lies outside of a X% confidence interval, with a p-value smaller than = X/100.One can reject a null hypothesis whose hypothesized value lies outside of a X% confidence interval, with a p-value of 1-(X/100).One can reject a null hypothesis whose hypothesized value lies outside of a X% confidence interval, with a p-value smaller than = 1-(X/100).One can reject an alternative hypothesis whose hypothesized value lies outside of a X% confidence interval, with a p-value smaller than = 1-(X/100).\n\nQuestion 7\nA researcher collected data from a sample of 80 participants. The sample mean is 72, and the standard deviation is 2.5. What is the 95% confidence interval?\n\n[71.72, 72.28][69.50, 74.50][67.10, 76.90][72.45, 72.55]\n\nQuestion 8\nWhat is the correct statement about a 95% confidence interval?\n\nThere is a 95% probability that the population value is larger than the lower bound and smaller than the upper bound of the confidence interval.95% of all confidence intervals contain the population value.The 95% confidence interval is a procedure with 95% probability of providing an interval that contains the population value.The 95% confidence interval contains the population value with 95% probability.\n\n\n\n\nShow explanations\n\nQuestion 1\nA confidence interval represents a range of values within which the true population parameter is likely to fall.\nQuestion 2\nThe confidence level of a confidence interval is chosen by the researcher to determine the desired level of confidence in capturing the true parameter.\nQuestion 3\nA wider confidence interval indicates greater uncertainty or less precision in estimating the population parameter.\nQuestion 4\nAs the sample size increases, the width of the confidence interval decreases, indicating increased precision in estimating the parameter.\nQuestion 5\nThe relationship between the width of a confidence interval and the level of confidence is inverse: higher confidence leads to a wider interval.\nQuestion 6\nThe purpose of a confidence interval in hypothesis testing is to assess the range of values where the population parameter might lie and make decisions about hypotheses.\nQuestion 7\nThe standard error of the sample mean is calculated by dividing the standard deviation (2.5) by the square root of the sample size; add +/- 1.96* the standard error to the mean.\nQuestion 8\nThe probability is in the procedure of calculating a confidence interval. We cannot make any probability statements about the population value, or about the results of specific confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>BE2 - Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confint.html#tutorial",
    "href": "confint.html#tutorial",
    "title": "25  BE2 - Confidence Intervals",
    "section": "\n25.3 Tutorial",
    "text": "25.3 Tutorial\n\n25.3.1 Confidence Intervals\nIn this assignment we will work on some questions regarding the confidence interval. We focus on the confidence interval around the mean, but everything you learn can also be applied to confidence intervals around the mean difference or regression coefficients.\nFinish the following sentence. The confidence interval around the mean is constructed around the \nPopulation mean (under H0)\nSample mean\nIn the population the variable IQ is normally distributed with \\(IQ \\sim N(\\mu=100, \\sigma=15\\).\nImagine that we drew 2000 samples from the population. For each of the samples we would calculate a 90% confidence interval around the sample mean. If you had to make a guess, how many intervals would you expect to contain the value 100? \nImagine we drew a sample from the population and we calculated the 95% confidence interval around the sample mean for a particular variable. The lower bound of the confidence interval is equal to 85 and the upper bound to 95.\nWhich of the following statements is correct?\n\nThere is a 95% probability that the population mean lies between 85 and 95.There is a 95% probability that a confidence interval calculated based on a sample from this population contains the true population value.There is a 95% probability that if we would draw a new sample for the same population, the true population value lies between 85 and 95.The probability that the population mean lies between 85 and 95 is 95%.\n\nConfidence intervals are interpreted in terms of long-run probability. IF we could draw a huge number of samples from the population, 95% of those samples would provide a confidence interval that contains the population mean.\nWe can never know whether one specific confidence interval contains the population value, however.\nSo we can NEVER draw a conclusion like “there is a 95% probability that the population mean lies between 85 and 95”.\nRecall the first lecture, in which I explained the idea of a “random experiment”. Think of a 95% confidence interval as a random experiment with a 95% probability of containing the population value. One specific confidence interval is not a random experiment. Whether the population mean lies within the interval is not a matter of probability. It either does or it does not. We just don’t know which of these is true.\nImagine a population with variable X, where \\(X \\sim N(\\mu 50, \\sigma = 10)\\)\nAssume a confidence level of 95% for all intervals.\nYou plan to draw a sample with \\(n=20\\) and compute a 95% confidence interval. What’s the probability that this interval will contain 50? %\nYour colleague has already drawn a sample of \\(n=20\\). What’s the probability that their confidence interval includes 50? \n0%\nCan’t say\n100%\n95%\nIf you would draw 20 samples, how many samples would you expect the confidence interval to contain the value 50? \nTrue or false: if you draw 100 samples, 95 of them will provide a confidence interval that contains the population value. \nTRUE\nFALSE\n\n\nExplanation\n\nThis is false because the phrase “will provide” is not a probability statements, but a deterministic one.\n“The number of 95% confidence intervals out of 100 samples that contain the population value” is a random experiment. We expect an outcome of 95, but if we conduct this random experiment, the observed outcome may differ a little, e.g. 93, 94, 97 times are all fine.\n\nAll else being equal, what would you expect to happen to the confidence intervals of smaller samples? \nThey become narrower\nThey stay the same\nThey become wider\n\n\nExplanation\n\nBy increasing the sample size, our estimate becomes more precise. This will lead to more narrow confidence intervals.\nMathematically it also makes sense, because the confidence interval is based on the standard error. Remember that the formula for the standard error is \\(SE = \\frac{\\sigma}{n}\\) . A smaller sample size leads to a smaller standard error, which leads to a narrower interval.\nNote that this does affect the probability of confidence intervals containing \\(\\mu\\).\n\nIf the standard deviation increases (and everything else stays the same) the confidence interval will \nbecome narrower\nbecome wider\nstay the same.\nIf we change the confidence level to 90%, the interval will \nbecome narrower\nstay the same\nbecome wider and \nfewer\nthe same number of\nmore intervals will contain \\(\\mu\\).",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>BE2 - Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Appendix A — Data for Portfolio",
    "section": "",
    "text": "A.1 SS: Values and Beliefs about Individuals and Collectives\nYou will need to use an appropriate data source for your portfolio assignments. Although you are welcome to use any data that you consider to be suitable for making the assignments - including data sets that you have previously collected, or open access data sources - we want to make sure that everybody has a backup option that meets the course requirements. Below, we introduce three data sources that have been customized for the three major tracks. You can follow your own interests in selecting one of these data sources.\nNOTE: Make sure to enter your correct group number when you generate the data; you must use a file that is unique to your group.\nTo generate a synthetic data set based on the sources described below, visit this link.\nThis synthetic dataset was inspired by Wave 7 of the World Values Survey (Haerpfer et al., 2022).\nThe World Values Survey (WVS) is a global research project that explores people’s values and beliefs and what social and political impact these have. Among topics covered are support for democracy, tolerance of ethnic minorities, support for gender equality, the role of religion and changing levels of religiosity, the impact of globalization, attitudes toward the environment, work, family, politics, national identity, culture, diversity, insecurity, and subjective well-being. This data source is used by governments, scholars, and international organizations like the United Nations.\nExamples of research questions:\nData documentation: https://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp\nReference: Haerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano J., M. Lagos, P. Norris, E. Ponarin & B. Puranen (eds.). 2022. World Values Survey: Round Seven - Country-Pooled Datafile Version 5.0. Madrid, Spain & Vienna, Austria: JD Systems Institute & WVSA Secretariat. doi:10.14281/18241.20.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data for Portfolio</span>"
    ]
  },
  {
    "objectID": "data.html#ss-values-and-beliefs-about-individuals-and-collectives",
    "href": "data.html#ss-values-and-beliefs-about-individuals-and-collectives",
    "title": "Appendix A — Data for Portfolio",
    "section": "",
    "text": "What proportion of participants considers work to be very important in life? (Q5)\nWhat proportion of participants score more extreme than 9/10 on a left-right political ideology scale? (Q240)\nIs trust in the government significantly higher than the neutral middle of the scale (3)? (Q292O)\nDoes participants’ age predict the attitude that children should take care of their parents? (Q38 and Q262)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data for Portfolio</span>"
    ]
  },
  {
    "objectID": "data.html#cn-behavioral-and-neural-correlates-of-empathy-in-adolescents",
    "href": "data.html#cn-behavioral-and-neural-correlates-of-empathy-in-adolescents",
    "title": "Appendix A — Data for Portfolio",
    "section": "A.2 CN: Behavioral and Neural Correlates of Empathy in Adolescents",
    "text": "A.2 CN: Behavioral and Neural Correlates of Empathy in Adolescents\nThis synthetic dataset was inspired by a study by Overgaauw and colleagues (2014).\nAdolescence is characterized by significant changes in how individuals perceive and interact with others, both cognitively and emotionally. Empathy is a crucial element in appropriately responding to the emotions and actions of others. It is often described as the capacity to understand and share the emotional experiences of others, enabling us to comprehend and anticipate their intentions. Children who possess higher levels of empathy demonstrate greater emotional regulation and engage in more prosocial behavior towards others. This experimental study presented adolescents with either positive or negative social situations, and asked them to focus either on person A or person B in those situations (in negative situations, person A was the perpetrator and person B was the victim). They then measured how many coins participants were willing to give to the focal person. Empathy was measured using a scale with three sub-dimensions of empathy (Contagion, Understanding, and Support), and brain activation in several regions of interest was measured.\nReference: Overgaauw, S., Güroğlu, B., Rieffe, C., & Crone, E. A. (2014). Developmental Neuroscience, 36 (3-4). Behavior and Neural Correlates of Empathy in Adolescents. https://doi.org/10.1159/000363318",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data for Portfolio</span>"
    ]
  },
  {
    "objectID": "data.html#be-sustainable-food-choices",
    "href": "data.html#be-sustainable-food-choices",
    "title": "Appendix A — Data for Portfolio",
    "section": "A.3 BE: Sustainable Food Choices",
    "text": "A.3 BE: Sustainable Food Choices\nThis synthetic dataset was inspired by a study by De Boer and colleagues (2007).\nSustainability goals may require people in Western countries to reduce their meat consumption. This study investigated which values motivate sustainable food choices related to meat consumption. The researchers surveyed 1530 Dutch consumers and found that various human values were related to different food choice motives. Universalism, in particular, had a unique impact on food choices that favored reduced meat, or free-range meat consumption. This study provided insight into the way values, motives and attitudes influencing sustainable food choices and shape individuals’ dietary decisions.\nReference: Joop de Boer; Carolien T. Hoogland; Jan J. Boersema (2007). Towards more sustainable food choices: Value priorities and motivational orientations. Food Quality and Preference, 18(7), 0–996. doi:10.1016/j.foodqual.2007.04.002.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data for Portfolio</span>"
    ]
  },
  {
    "objectID": "probability_tables.html",
    "href": "probability_tables.html",
    "title": "Appendix B — Z-table",
    "section": "",
    "text": "B.1 t-table\nTable gives the right-tail probability corresponding to a Z-value of the value in the Z-column plus the value in the column name, which indicates the second digit of the Z-score.\nTable gives the t-value corresponding to the right-tail probability indicated by the columns.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Z-table</span>"
    ]
  },
  {
    "objectID": "formula_sheet2.html",
    "href": "formula_sheet2.html",
    "title": "Appendix C — Formula sheet",
    "section": "",
    "text": "C.1 General Part\nMean: \\(\\bar{X} = \\frac{\\Sigma_{i=1}^nx_i}{N}\\)\nVariance: \\(S^2_x = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}{n-1}\\)\nStandardized values (Z-values): \\(Z = \\frac{X-\\mu}{\\sigma}\\)\nZ-statistic in one sample Z-test: \\(Z = \\frac{\\bar{x}-\\mu_x}{\\sigma_{x}}\\)\nStandard error of the mean: \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\)\nCohen’s d: \\(\\frac{\\bar{X}_1-\\bar{X}_2}{s_{pooled}}\\)\n\\(s^2_{pooled} = \\frac{(n_1-1)*s_1^2 + (n_2-1)*s_2^2}{n_1+n_2-2}\\)\n\\(s_{pooled} = \\sqrt{s^2_{pooled}}\\)\nF-statistic in one-way ANOVA: \\(F (df_b, df_w) = \\frac{(SS_b/df_b)}{(SS_w/df_w)} = \\frac{MS_b}{(MS_w}\\)\nSimple regression model: \\(Y' = b_0+b_1X\\)\nMultiple regression model: \\(Y' = b_0+b_1X_1 + b_2X_2\\)\nExplained variance: \\(R^2 = \\frac{s^2_{y'}}{s^2_y}\\)\nt-statistic in a one sample t-test: \\(t = \\frac{\\bar{X}-\\mu_{H0}}{se_x}\\), where \\(se_x = \\frac{s_x}{\\sqrt{n}}\\), \\(df = n - 1\\)\nt-statistic in an independent samples t-test: \\(t = \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)_{H0}}{se_{x_1-x_2}}\\)\n\\(se_{x_1-x_2} = \\sqrt{s^2_{pooled}(\\frac{1}{n_1}+\\frac{1}{n_2})}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Formula sheet</span>"
    ]
  },
  {
    "objectID": "formula_sheet2.html#business-and-economics",
    "href": "formula_sheet2.html#business-and-economics",
    "title": "Appendix C — Formula sheet",
    "section": "C.2 Business and economics",
    "text": "C.2 Business and economics\nLogistic function: \\(P(Y=1|X) = \\frac{e^{(b_0 + b_1X)}}{1 + e^{(b_0 + b_1X)}}\\)\nFrom probability to odds: \\(\\text{odds} = \\frac{P}{1 - P}\\)\nFrom odds to probability: \\(P = \\frac{\\text{odds}}{1 + \\text{odds}}\\)\nFrom odds to logit: \\(\\text{logit} = \\ln(\\text{odds})\\)\nFrom probability to logit: \\(\\text{logit} = \\ln\\left(\\frac{P}{1 - P}\\right)\\)\nFrom logit to odds: \\(\\text{odds} = e^{\\text{logit}}\\)\nFrom logit to probability: \\(P = \\frac{e^{\\text{logit}}}{1 + e^{\\text{logit}}}\\)\nWald test statistic: \\(W = (\\frac{b}{se_b})^2\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Formula sheet</span>"
    ]
  },
  {
    "objectID": "formula_sheet2.html#cognitive-neuroscience",
    "href": "formula_sheet2.html#cognitive-neuroscience",
    "title": "Appendix C — Formula sheet",
    "section": "C.3 Cognitive neuroscience",
    "text": "C.3 Cognitive neuroscience\nNumber of Possible Pairwise Comparisons: \\(k \\times \\frac{(k - 1)}{2}\\)\nFactorial ANOVA Linear Model: \\(Y_{jkl} = \\mu_Y + \\alpha_k + \\beta_l + \\alpha\\beta_{kl} + \\epsilon_{jkl}\\)\nEta-squared for Factor A: \\(\\eta_A^2 = \\frac{SS_A}{SS_{total}}\\)\nPartial eta-squared for Factor A: \\(\\eta_{partial.A}^2 = \\frac{SS_A}{SS_A + SS_w}\\)\nAdjusted Mean: \\(\\bar{Y}_{i(adj)} = \\bar{Y}_i - b_w(\\bar{X}_i - \\bar{X})\\)\nt-Statistic in Paired Samples t-Test: \\(t = \\frac{\\bar{d}}{\\frac{s_{\\bar{d}}}{\\sqrt{n}}}, \\quad \\text{df} = n - 1\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Formula sheet</span>"
    ]
  },
  {
    "objectID": "formula_sheet2.html#social-sciences",
    "href": "formula_sheet2.html#social-sciences",
    "title": "Appendix C — Formula sheet",
    "section": "C.4 Social Sciences",
    "text": "C.4 Social Sciences\nReliability: \\(r_{xx'} = \\frac{\\text{var}(T)}{\\text{var}(X)} = \\frac{\\text{var}(T)}{\\text{var}(T) + \\text{var}(E)}\\)\nEigenvalue of Component 1 for 6 Items: \\(\\lambda_1 = a_{11}^2 + a_{21}^2 + a_{31}^2 + a_{41}^2 + a_{51}^2 + a_{61}^2\\)\nThe proportion of Variance Accounted For by component 1 (when there are J items) is: \\(\\text{Proportion VAF} = \\frac{\\lambda_1}{\\text{TotalVar}} = \\frac{\\lambda_1}{J}\\)\nComponent loadings for component 1 and item j are represented as: \\(a_{j1} = r_{X_jC_1}\\)\nCommunality for 2 Components: \\(h_{j2} = r_{XjC1}^2 + r_{XjC2}^2 = a_{j1}^2 + a_{j2}^2\\)\nUnicity for 2 Components: \\(b_{j2} = 1 - h_{j2}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Formula sheet</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix D — References",
    "section": "",
    "text": "DeBruine, L. M., & Lakens, D. (n.d.). Methods Book\nTemplate. Retrieved June 4, 2025, from https://debruine.github.io/booktem/\n\n\nHalpern, J. Y. (2015, May 1). A Modification of the\nHalpern-Pearl Definition of Causality. https://doi.org/10.48550/arXiv.1505.00162\n\n\nHoijtink, H., Bruin, J. de, Duken, S. B., Flores, J., Frankenhuis, W.,\n& Lissa, C. J. van. (2023). The Open Empirical\nCycle for Hypothesis Evaluation in\nPsychology. https://doi.org/10.31234/osf.io/wsxbh\n\n\nMorabia, A. (2013). Hume, Mill, Hill, and the\nSui Generis Epidemiologic Approach to Causal\nInference. American Journal of Epidemiology,\n178(10), 1526–1532. https://doi.org/10.1093/aje/kwt223\n\n\nPearl, J. (2009). Causal inference in statistics: An\noverview. Statistics Surveys, 3, 96–146. https://doi.org/10.1214/09-SS057\n\n\nPeikert, A., Ernst, M. S., & Brandmaier, A. M. (2023). Why does\npreregistration increase the persuasiveness of evidence? A\nBayesian rationalization [Preprint]. https://osf.io/cs8wb.\nhttps://doi.org/10.31234/osf.io/cs8wb\n\n\nVan Lissa, C. J. (2022a). Developmental data science: How\nmachine learning can advance theory formation in Developmental\nPsychology. Infant and Child Development,\n32(6), 1–12. https://doi.org/10.1002/icd.2370\n\n\nVan Lissa, C. J. (2022b). Complementing preregistered confirmatory\nanalyses with rigorous, reproducible exploration using machine learning.\nReligion, Brain & Behavior, 0(0), 1–5. https://doi.org/10.1080/2153599X.2022.2070254",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>References</span>"
    ]
  }
]