# Correlation: Measuring Relationships

```{r, include = FALSE}
source("R/booktem_setup.R")
source("R/my_setup.R")
```

## Correlation – Describing Relationships Between Variables {#sec-correlation}

Correlation is a statistical technique used to **quantify the degree to which two variables move together**. It provides a concise summary of the association between pairs of scores across individuals. For example, a researcher might retrieve each student’s high school GPA (a measure of academic performance) and pair it with their family’s annual income. The goal is to determine whether higher grades tend to correspond with higher income. In correlational studies, each individual contributes two measurements, commonly referred to as *X* and *Y* forming the foundation for analysis.

To explore this relationship visually, researchers often rely on scatter plots. In a scatter plot, *X* values appear along the horizontal axis and *Y* values along the vertical. Each point on the plot corresponds to one participant’s pair of scores. These plots allow immediate detection of linear trends, and outliers—patterns that may remain obscured when examining data in purely numerical or tabular form.

At its core, correlation analysis is concerned with two key features of the relationship between variables: **direction** and **strength**. The direction may be positive—indicating that increases in *X* tend to be accompanied by increases in *Y*—or negative—suggesting that higher *X* values predict lower *Y* values. The **strength** of this association is expressed by the magnitude of the correlation coefficient. An *r* value close to ±1 indicates a near-perfect linear relationship, whereas an *r* near 0 signals little to no linear trend. Since Pearson’s *r* specifically quantifies linear associations, it is not suited for capturing non-linear patterns, even if a strong relationship exists in another form.

Before arriving at the correlation coefficient, statisticians often begin with **covariance**—a preliminary measure of how two variables vary together. Covariance reflects direction: it is positive when high values of *X* accompany high values of *Y*, and negative when they move in opposite directions. However, its numerical value is not directly interpretable because it is tied to the units of the measurement. A covariance expressed in centimeters and kilograms will differ from one computed in meters and pounds, even if the underlying association remains unchanged. As a result, covariance cannot meaningfully convey the *strength* of a relationship—only whether the variables tend to move in the same or opposite directions.

To address this limitation, the **Pearson correlation coefficient (*r*)** is computed by standardizing covariance. Specifically, the covariance is divided by the product of the standard deviations of *X* and *Y*, which removes the influence of measurement units from the calculation. This standardization not only allows correlations to be expressed on a common –1 to +1 scale, but also enables meaningful interpretation of both **direction** and **strength**. Because the resulting value is unit-free, it can be compared across variables measured on different scales and across studies.

Because the Pearson correlation coefficient expresses direction and strength independently of measurement units, it serves as a valuable tool for evaluating theoretical expectations. In psychology, medicine, and related fields, many theories propose that particular variables should exhibit a specific form of association. For example, a developmental framework might anticipate a positive correlation between parents’ IQs and their child’s IQ, whereas a clinical model could predict a negative association between mindfulness and cortisol levels. Observing the expected correlation offers empirical support for the theory; the absence or reversal of the relationship may call for theoretical refinement or reconsideration.

Despite their utility, correlation coefficients must be interpreted with care. First and most importantly, **correlation does not imply causation**. A strong association between two variables does not mean that one causes the other. For example, ice-cream sales and drowning deaths may correlate positively—not because one causes the other, but because both increase with hot weather. Establishing causality requires experimental control, not statistical association. This topic will be explored in more depth in **Week 7**.

Secondly, a **restricted range** of scores can obscure or distort relationships. If only a narrow segment of the variable is sampled, such as examining IQ and creativity within a highly selective university, correlations may appear weaker than they truly are. 

Third, **outliers** can have a disproportionate impact. A single extreme case can artificially inflate or deflate *r*, potentially leading to misleading conclusions. This is dramatically illustrated by **Anscombe’s Quartet**—a set of four datasets that share nearly identical statistical properties, including the same correlation coefficient, yet differ markedly in their underlying structure. In one case, a single outlier drives the correlation; in another, the relationship is non-linear. Despite similar *r* values, each scatter plot tells a different story. For this reason, **visual inspection of scatter plots** is essential before interpreting correlation coefficients.

![Anscombe’s Quartet](Anscombe's_quartet_3.svg.png)

## Summary

In summary, **covariance** offers an initial metric for gauging whether two variables tend to vary in the same or opposite direction. However, because its magnitude depends on the measurement units of the variables involved, it cannot be directly interpreted in terms of strength. The **Pearson correlation coefficient (*r*)** addresses this limitation by standardizing the covariance, yielding a unit-free statistic bounded between –1 and +1. This standardized measure expresses both the **direction** and **strength** of a linear relationship, enabling meaningful comparisons across contexts. Nevertheless, interpreting correlations requires caution—particularly with respect to restricted sampling ranges, the influence of outliers, and the fundamental distinction between correlation and causation.

# Formative Test

This short quiz checks your grasp of **Chapter 3 – Covariance & Correlation**.  
Work through it after you’ve studied the lecture slides (and before the next live session) so we can focus on anything that still feels uncertain. Each incorrect answer reveals a hint that sends you back to the exact slide or numerical example you need.

```{r}
#| results = "asis"
#| echo    = FALSE
#| cache   = FALSE
add_mcs("ch3_formative_corrected.csv")
```

# Tutorial

```{r, include = FALSE}
source("R/booktem_setup.R")
source("R/my_setup.R")
```

# 5 Tutorial

## 5.1 Correlation – Practice Dataset (`LAS_SocSc_DataLab2.sav`)

Open **`LAS_SocSc_DataLab2.sav`** (find it in the **`data`** folder you downloaded earlier).  
The file contains six variables (`X1` … `X6`). You’ll inspect three bivariate relationships.

### 5.1.1 Plot the pairs

Generate three simple scatter-plots:

1. `Graphs` › `Legacy Dialogs` › `Scatter/Dot` ► **Simple Scatter**  
2. Pairings & axis order  
   * `X1` (X-axis) vs `X2` (Y-axis)  
   * `X3` (X-axis) vs `X4` (Y-axis)  
   * `X5` (X-axis) vs `X6` (Y-axis)  
3. *Paste* and *Run* each syntax block.

Describe **linearity**, **direction**, and **strength** for each plot.

::: {.webex-check .webex-box}

“The relationship between `X1` and `X2` is positive.” `r torf(TRUE)`

“The relationship between `X5` and `X6` is positive.” `r torf(FALSE)`

“The relationship between `X1` and `X2` is linear.” `r torf(TRUE)`

“The relationship between `X3` and `X4` is linear.” `r torf(FALSE)`

Strength of `X1`–`X2`:  
`r mcq(c("answer" = "moderate", "weak", "zero", "strong")[sample.int(4)])`

Strength of `X3`–`X4`:  
`r mcq(c("moderate", "weak", "zero","answer" = "strong")[sample.int(4)])`

Strength of `X5`–`X6`:  
`r mcq(c("answer" = "moderate", "weak", "zero","strong")[sample.int(4)])`

:::

**Example of a cyclical (non-linear) pair?**

`r hide("Show answer")`
Any cyclical process, e.g., “time of day vs. tide height” or “sun elevation vs. day length”.
`r unhide()`

### 5.1.2 Correlation coefficients

Even when the pattern is non-linear it’s useful to see why Pearson *r* can mislead.

*Analyze* › *Correlate* › *Bivariate*  
Select all six variables → **OK**.

::: {.webex-check .webex-box}

`X1`–`X2` correlation: `r fitb(.50, num = TRUE, tol = .01)`

`X2`–`X6` correlation: `r fitb(.06, num = TRUE, tol = .01)`

`X3`–`X4` correlation: `r fitb(-.80, num = TRUE, tol = .01)`

Can we interpret `X3`–`X4`’s *r* at face value?  
`r mcq(c("answer" = "No, assumption of linearity violated",
         "No, assumption of association violated",
         "No, assumption of normality violated",
         "Yes, otherwise SPSS would give an error")[sample.int(4)])`

Interpret `X5`–`X6`:  
`r mcq(c("answer" = "Moderate negative",
         "Moderate positive",
         "Weak positive",
         "Weak negative")[sample.int(4)])`

:::

*Take-away:* Pearson’s *r* is good at detecting linear patterns (like X1–X2), but it may be close to zero even when the variables have a strong curved pattern (like X3–X4).

---

## 5.2 Correlation – Work Dataset (`Work.sav`)

Having practiced on simulated data, let’s now apply the same workflow to a real dataset related to the workplace.

> **File location:** `data/Work.sav`

### 5.2.1 Why inspect the plot first?

Before trusting Pearson *r* we check for

* an **approximately linear** pattern, and  
* **extreme values** that could distort the statistic.

::: {.webex-check .webex-box}

Select the **two** key reasons:

`r mcq(c("To check if the relationship is linear"  = TRUE,
         "To check if there are any extreme values" = TRUE,
         "To check if the relationship is positive" = FALSE,
         "To check if the variables are normally distributed" = FALSE,
         "To check if the relationship is strong enough" = FALSE))`

:::

### 5.2.2 Create the scatter-plot

`Graphs` › `Legacy Dialogs` › `Scatter/Dot` → **Simple Scatter**

* X-axis =`scmental` (Mental Pressure)  
* Y-axis =`scemoti`  (Emotional Pressure)

*Paste* and *Run*.

::: {.webex-check .webex-box}

Is the cloud roughly **linear**? `r torf(TRUE)`

Any obvious **outliers**? `r torf(FALSE)`

Visual strength:  
`r mcq(c("Moderately strong and positive" = TRUE,
         "Moderately weak and positive"  = FALSE,
         "Moderately strong and negative" = FALSE,
         "Moderately weak and negative"  = FALSE)[sample.int(4)])`

:::

### 5.2.3 Compute Pearson *r*

`Analyze` › `Correlate` › `Bivariate` → (`scmental`, `scemoti`) → **OK**

::: {.webex-check .webex-box}

Correlation (3 decimals): `r fitb(.54, num = TRUE, tol = .01)`

Significance (α = .05)

*Relationship is:*  
`r mcq(c("significant" = TRUE, "not significant" = FALSE))`

*p-value:*  
`r mcq(c("< .05" = TRUE, "≥ .05" = FALSE))`

:::

### 5.2.4 Interpretation

::: {.webex-check .webex-box}

Population inference:

`r mcq(c("We can conclude that there is a relationship between mental and emotional pressure in the population." = TRUE,
         "We can conclude that there is no relationship between mental and emotional pressure in the population." = FALSE,
         "We cannot draw a conclusion on whether or not there is a relationship." = FALSE)[sample.int(3)])`

:::

*Take-away:* Mental and emotional pressure show a **moderately strong, significant positive relationship**—employees who feel more mentally pressured also tend to feel more emotionally pressured.

---
