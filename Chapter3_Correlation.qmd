# Correlation: Measuring Relationships

```{r, include = FALSE}
source("R/booktem_setup.R")
source("R/my_setup.R")
```

Correlation is a statistical technique used to **measure and describe the relationship between two variables**.  
Typically, the two variables are observed as they occur naturally—there is **no manipulation** or experimental control.  
For instance, a researcher might:

* Retrieve each student’s high‑school GPA from records (academic performance)  
* Survey that student’s family to record annual income  

The researcher then examines whether a relationship exists between grades and family income.  
> **Key point:** Each individual provides **two scores**—one for each variable—usually labelled *X* and *Y*.

## Visualising Pairs of Scores

Pairs of scores can be displayed in a **table** or on a **scatter plot**.

* **Scatter plot:**  
  *Horizontal axis* = X values  
  *Vertical axis* = Y values  
  Each point represents one individual.

A scatter plot immediately reveals patterns—straight‑line trends, curves, clusters, or outliers.

## What a Correlation Describes

A single correlation number summarises **three characteristics** of the X–Y relationship.

### 1  Direction  
* **Positive (+)** – variables change together: when the value of **X** increases from one individual to another, the value of **Y** tends to increase as well; likewise, when **X** decreases, **Y** decreases.  
* **Negative (–)** – variables move oppositely: as **X** increases, **Y** decreases (and vice-versa).

### 2  Form  
* Most common: **linear** – points cluster around a straight line.  
* Other forms exist; special correlations can capture curves or monotonic trends.

### 3  Strength (Consistency)  
* **Perfect** relationship → points lie exactly on a line → |r| = 1.00  
* **No** relationship → points scattered randomly → r = 0  
* Values between 0 and 1 measure how tightly points hug the line.

Direction (sign) and strength (magnitude) are **independent**.  
*r* = +0.80 and *r* = –0.80 are equally strong but in opposite directions.  
A valid correlation **cannot** exceed +1.00 or –1.00.

## Covariance – the Raw “Move-Together” Score

Before we standardise anything, statisticians look at **covariance**—a raw gauge of how two variables shift in tandem.

* **Direction only**  
  *Positive covariance* tells us the variables rise and fall together; *negative covariance* says they move in opposite directions.  
  Crucially, the *size* of the number does **not** tell you “how strong” the relationship is, because…

* **Units matter**  
  Covariance is expressed in the combined units of the two variables (e.g., cm × kg). Change either scale (switch centimetres to metres, dollars to euros) and the covariance value changes—even though the underlying relationship is identical.

* **Why we still need correlation**  
  To talk meaningfully about *strength*, we divide covariance by the spreads of X and Y.  
  That standardisation removes units and caps the result between –1 and +1, giving us
  **Pearson’s *r***—a number whose magnitude *and* sign are comparable across studies.

> Think of covariance as the raw “move-together” handshake; correlation turns that handshake
> into a unit-free score, letting us discuss both **direction and strength** on a common scale.


## The Pearson Correlation

The **Pearson correlation (r)** quantifies the **degree and direction of a linear relationship** between two variables.

## Where and Why Correlations Are Used

1. **Prediction**  
   When two variables are correlated, knowing a person’s score on **X** helps you *predict* their likely score on **Y**. College admissions are a classic case: universities correlate applicants’ SAT scores with first-year GPA. A strong positive correlation means an admissions officer can forecast academic performance with quantifiable accuracy; the stronger the correlation, the smaller the typical prediction error (a concept formalised later in regression).

2. **Validity – Does a test measure what it claims?**  
   A new questionnaire that purports to assess *anxiety* should correlate highly with established anxiety inventories, moderately with related constructs such as neuroticism, and only weakly with unrelated traits like extraversion. These patterns—**convergent** and **discriminant** validity—show that the test taps the intended construct rather than something else.

3. **Reliability – How stable or consistent is a measure?**  
   Administer an intelligence test on Monday and again two weeks later: a large test-retest correlation (e.g., *r* ≈ 0.90) indicates score stability and hence reliability. Lower correlations suggest random measurement error or situational drift. The same logic underlies “split-half” and “internal consistency” checks, where correlations among test items show whether they all tap the same construct.

4. **Theory Verification**  
   Many psychological or biomedical theories predict that two variables should relate in a particular direction. A developmental theory, for example, may predict a positive correlation between parents’ IQs and their child’s IQ; a clinical hypothesis may expect a negative correlation between mindfulness and stress hormones. Demonstrating the predicted correlation supports the theory, whereas an absent or opposite correlation prompts revision or rejection of the theoretical account.

## Interpreting Correlations – Four Cautions

1. **Correlation ≠ Causation**  
   A correlation simply indicates that **X** and **Y** move together—it says **nothing** about *why* they do so.  
   *Example:* Ice-cream sales and drowning deaths correlate positively. Hot weather is the lurking third factor driving both. Concluding that buying ice-cream causes drowning would be a causal fallacy. Only a controlled experiment, where one variable is manipulated and others held constant, can establish cause-and-effect.

2. **Restricted Range**  
   A correlation computed from a narrow slice of data can understate—or occasionally overstate—the true relationship.  
   *Example:* If you correlate IQ with creativity using only university students (IQ range perhaps 110–130), *r* may be near zero even though a broader sample (IQ 70–150) shows a clear positive trend. **Never generalise beyond the range of scores actually sampled.**

3. **Outliers**  
   One or two extreme data points can dominate the correlation calculation because *r* considers every pair of scores.  
   *Example:* In a scatter plot of five ordinary points that show no trend, adding a single extreme point far out on both axes can swing *r* from ≈ 0 to +0.85 (or −0.85) and completely change the apparent conclusion. **Always inspect a scatter plot** before trusting the numerical value.

4. **Proportion of Variability (r²)**  
   The raw correlation coefficient can feel abstract. Squaring it yields **r²**, the *coefficient of determination*—the proportion of variability in **Y** that can be predicted from **X**.  
   *Example:* *r* = 0.50 → *r²* = 0.25 → 25 % of the variation in **Y** is associated with differences in **X**; the other 75 % is due to factors not captured by the correlation. Reporting *r²* helps readers judge practical significance, not just statistical strength.

## Summary

* Correlation summarises **direction, form, and strength** of relationships.  
* Pearson’s *r* captures **linear** patterns.  
* **Applications**: prediction, validity, reliability, theory tests.  
* Interpret with care: consider causality, range, outliers, and r².  

Correlation is a powerful descriptive tool—but recognising its limits is crucial before drawing conclusions or making predictions.

# Formative Test

This short quiz checks your grasp of **Chapter 3 – Covariance & Correlation**.  
Work through it after you’ve studied the lecture slides (and before the next live session) so we can focus on anything that still feels uncertain. Each incorrect answer reveals a hint that sends you back to the exact slide or numerical example you need.

```{r}
#| results = "asis"
#| echo    = FALSE
#| cache   = FALSE
add_mcs("ch3_formative.csv")
```

# Tutorial

```{r, include = FALSE}
source("R/booktem_setup.R")
source("R/my_setup.R")
```

## Correlation

For the upcoming few questions we need the data file `LAS_SocSc_DataLab2.sav`.
Open the file in SPSS. You will see that the file contains data for six variables, named X1 through X6. We will inspect the associations between pairs of variables (so called bivariate relationships).

First, generate a scatter plot for X1 and X2. Proceed as follows: Graphs > Legacy dialogs > Scatter/dot. Then ask for a Simple scatter. Put X1 on the X-Axis and X2 on the Y-Axis. Describe the association. Take into account whether the relationship follows a straight line (i.e., linearity), is positive or negative (i.e., direction), and whether the relationship seems to be weak, moderate or strong (i.e., strength).

Second, generate a scatter plot for X3 and X4. Make sure that X3 is shown on the X-axis and X4 on the Y-axis. Describe the association in terms of linearity, direction and strength.

Third, generate a scatter plot for X5 and X6. Describe the association in terms of linearity, direction and strength.

::: {.webex-check .webex-box}

Is the relationship between X1-X2 positive? `r torf(TRUE)`

Is the relationship between X5-X6 positive? `r torf(FALSE)`

Is the relationship between X1-X2 linear? `r torf(TRUE)`

Is the relationship between X3-X4 linear? `r torf(FALSE)`

Give an indication of the strength of the relationship between X1-X2: `r mcq(c("answer" = "moderate", "weak", "zero", "strong")[sample.int(4)])`

Give an indication of the strength of the relationship between X3-X4: `r mcq(c("moderate", "weak", "zero","answer" = "strong")[sample.int(4)])`

Give an indication of the strength of the relationship between X5-X6: `r mcq(c("answer" = "moderate", "weak", "zero","strong")[sample.int(4)])`

:::


Consider the relationship between X3 and X4, can you think of an example of two variables that would be associated in this way?

`r hide("Show answer")`

Any cyclical process;

* Time in the day and how far the water reaches up the beach (ebb and flow)
* Location of the sun in the sky
 
`r unhide()`

### Correlation Coefficient

In this step we will look at the correlation coefficient as numerical description of linear association.

Notice that in the previous step we found a non-linear association. The correlation coefficient would not be a valid measure to describe such an association, but nevertheless it is instructive to see why caution should be exercised in drawing conclusions about association from the correlation coefficient alone.

We will use SPSS to compute the correlation coefficient.

Analyze > Correlate > Bivariate
Select X1, X2, ... X6 as the variables
Click OK

Consult the table Correlations in the output.

There are several values in the table, but we are looking for the Pearson Correlation. The other numbers are the so called significance level, a concept we discuss soon, and the sample size.

## Quiz

::: {.webex-check .webex-box}

What is the correlation coefficient for the variables X1 and X2? `r fitb(.50, num = TRUE, tol = .01)`

What is the correlation coefficient for the variables X2 and X6? `r fitb(.06, num = TRUE, tol = .01)`

What is the correlation coefficient for the variables X3 and X4? `r fitb(-.80, num = TRUE, tol = .01)`

Can we interpret this correlation coefficient? `r mcq(c("answer" = "No, assumption of linearity violated", "No, assumption of association violated", "No, assumption of normality violated", "Yes, otherwise SPSS would give an error")[sample.int(4)])`

Interpret the correlation between X5 and X6? `r mcq(c("answer" = "Moderate negative", "Moderate positive", "Weak positive", "Weak negative")[sample.int(4)])`

:::
