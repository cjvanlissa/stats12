# Correlation: Measuring Relationships

```{r, include = FALSE}
source("R/booktem_setup.R")
source("R/my_setup.R")
```

Correlation is a statistical technique used to **measure and describe the relationship between two variables**.  
Typically, the two variables are observed as they occur naturally—there is **no manipulation** or experimental control.  
For instance, a researcher might:

* Retrieve each student’s high‑school GPA from records (academic performance)  
* Survey that student’s family to record annual income  

The researcher then examines whether a relationship exists between grades and family income.  
> **Key point:** Each individual provides **two scores**—one for each variable—usually labelled *X* and *Y*.

## Visualising Pairs of Scores

Pairs of scores can be displayed in a **table** or on a **scatter plot**.

* **Scatter plot:**  
  *Horizontal axis* = X values  
  *Vertical axis* = Y values  
  Each point represents one individual.

A scatter plot immediately reveals patterns—straight‑line trends, curves, clusters, or outliers.

## What a Correlation Describes

A single correlation number summarises **three characteristics** of the X–Y relationship.

### 1  Direction  
* **Positive (+)** – variables change together: when the value of **X** increases from one individual to another, the value of **Y** tends to increase as well; likewise, when **X** decreases, **Y** decreases.  
* **Negative (–)** – variables move oppositely: as **X** increases, **Y** decreases (and vice-versa).

### 2  Form  
* Most common: **linear** – points cluster around a straight line.  
* Other forms exist; special correlations can capture curves or monotonic trends.

### 3  Strength (Consistency)  
* **Perfect** relationship → points lie exactly on a line → |r| = 1.00  
* **No** relationship → points scattered randomly → r = 0  
* Values between 0 and 1 measure how tightly points hug the line.

Direction (sign) and strength (magnitude) are **independent**.  
*r* = +0.80 and *r* = –0.80 are equally strong but in opposite directions.  
A valid correlation **cannot** exceed +1.00 or –1.00.

## Covariance – the Raw “Move-Together” Score

Before we standardise anything, statisticians look at **covariance**—a raw gauge of how two variables shift in tandem.

* **Direction only**  
  *Positive covariance* tells us the variables rise and fall together; *negative covariance* says they move in opposite directions.  
  Crucially, the *size* of the number does **not** tell you “how strong” the relationship is, because…

* **Units matter**  
  Covariance is expressed in the combined units of the two variables (e.g., cm × kg). Change either scale (switch centimetres to metres, dollars to euros) and the covariance value changes—even though the underlying relationship is identical.

* **Why we still need correlation**  
  To talk meaningfully about *strength*, we divide covariance by the spreads of X and Y.  
  That standardisation removes units and caps the result between –1 and +1, giving us
  **Pearson’s *r***—a number whose magnitude *and* sign are comparable across studies.

> Think of covariance as the raw “move-together” handshake; correlation turns that handshake
> into a unit-free score, letting us discuss both **direction and strength** on a common scale.


## The Pearson Correlation

The **Pearson correlation (r)** quantifies the **degree and direction of a linear relationship** between two variables.

## Where and Why Correlations Are Used

1. **Prediction**  
   When two variables are correlated, knowing a person’s score on **X** helps you *predict* their likely score on **Y**. College admissions are a classic case: universities correlate applicants’ SAT scores with first-year GPA. A strong positive correlation means an admissions officer can forecast academic performance with quantifiable accuracy; the stronger the correlation, the smaller the typical prediction error (a concept formalised later in regression).

2. **Validity – Does a test measure what it claims?**  
   A new questionnaire that purports to assess *anxiety* should correlate highly with established anxiety inventories, moderately with related constructs such as neuroticism, and only weakly with unrelated traits like extraversion. These patterns—**convergent** and **discriminant** validity—show that the test taps the intended construct rather than something else.

3. **Reliability – How stable or consistent is a measure?**  
   Administer an intelligence test on Monday and again two weeks later: a large test-retest correlation (e.g., *r* ≈ 0.90) indicates score stability and hence reliability. Lower correlations suggest random measurement error or situational drift. The same logic underlies “split-half” and “internal consistency” checks, where correlations among test items show whether they all tap the same construct.

4. **Theory Verification**  
   Many psychological or biomedical theories predict that two variables should relate in a particular direction. A developmental theory, for example, may predict a positive correlation between parents’ IQs and their child’s IQ; a clinical hypothesis may expect a negative correlation between mindfulness and stress hormones. Demonstrating the predicted correlation supports the theory, whereas an absent or opposite correlation prompts revision or rejection of the theoretical account.

## Interpreting Correlations – Four Cautions

1. **Correlation ≠ Causation**  
   A correlation simply indicates that **X** and **Y** move together—it says **nothing** about *why* they do so.  
   *Example:* Ice-cream sales and drowning deaths correlate positively. Hot weather is the lurking third factor driving both. Concluding that buying ice-cream causes drowning would be a causal fallacy. Only a controlled experiment, where one variable is manipulated and others held constant, can establish cause-and-effect.

2. **Restricted Range**  
   A correlation computed from a narrow slice of data can understate—or occasionally overstate—the true relationship.  
   *Example:* If you correlate IQ with creativity using only university students (IQ range perhaps 110–130), *r* may be near zero even though a broader sample (IQ 70–150) shows a clear positive trend. **Never generalise beyond the range of scores actually sampled.**

3. **Outliers**  
   One or two extreme data points can dominate the correlation calculation because *r* considers every pair of scores.  
   *Example:* In a scatter plot of five ordinary points that show no trend, adding a single extreme point far out on both axes can swing *r* from ≈ 0 to +0.85 (or −0.85) and completely change the apparent conclusion. **Always inspect a scatter plot** before trusting the numerical value.

4. **Proportion of Variability (r²)**  
   The raw correlation coefficient can feel abstract. Squaring it yields **r²**, the *coefficient of determination*—the proportion of variability in **Y** that can be predicted from **X**.  
   *Example:* *r* = 0.50 → *r²* = 0.25 → 25 % of the variation in **Y** is associated with differences in **X**; the other 75 % is due to factors not captured by the correlation. Reporting *r²* helps readers judge practical significance, not just statistical strength.

## Summary

* **Covariance** is the raw “move-together” score—its sign shows *direction*, but its size depends on the measurement units.  
* **Correlation (Pearson’s *r*)** is **standardised covariance**, giving a unit-free measure of linear *direction* **and** *strength* on a –1 … +1 scale.  
* **Key applications:** prediction, test validity, test reliability, and theory checks.  
* Always interpret with care—look at causality, restricted range, outliers, and remember to translate *r* into practical meaning via *r²*.

Covariance and correlation are powerful descriptive tools—knowing both their possibilities **and** their limits is essential before drawing conclusions or making predictions.

# Formative Test

This short quiz checks your grasp of **Chapter 3 – Covariance & Correlation**.  
Work through it after you’ve studied the lecture slides (and before the next live session) so we can focus on anything that still feels uncertain. Each incorrect answer reveals a hint that sends you back to the exact slide or numerical example you need.

```{r}
#| results = "asis"
#| echo    = FALSE
#| cache   = FALSE
add_mcs("ch3_formative.csv")
```

# Tutorial

```{r, include = FALSE}
source("R/booktem_setup.R")
source("R/my_setup.R")
```

# 5 Tutorial

## 5.1 Correlation – Practice Dataset (`LAS_SocSc_DataLab2.sav`)

Open **`LAS_SocSc_DataLab2.sav`** (find it in the **`data`** folder you downloaded earlier).  
The file contains six variables (`X1` … `X6`). You’ll inspect three bivariate relationships.

### 5.1.1 Plot the pairs

Generate three simple scatter-plots:

1. `Graphs` › `Legacy Dialogs` › `Scatter/Dot` ► **Simple Scatter**  
2. Pairings & axis order  
   * `X1` (X-axis) vs `X2` (Y-axis)  
   * `X3` (X-axis) vs `X4` (Y-axis)  
   * `X5` (X-axis) vs `X6` (Y-axis)  
3. *Paste* and *Run* each syntax block.

Describe **linearity**, **direction**, and **strength** for each plot.

::: {.webex-check .webex-box}

Is the relationship between `X1`–`X2` positive? `r torf(TRUE)`

Is the relationship between `X5`–`X6` positive? `r torf(FALSE)`

Is the relationship between `X1`–`X2` linear? `r torf(TRUE)`

Is the relationship between `X3`–`X4` linear? `r torf(FALSE)`

Strength of `X1`–`X2`:  
`r mcq(c("answer" = "moderate", "weak", "zero", "strong")[sample.int(4)])`

Strength of `X3`–`X4`:  
`r mcq(c("moderate", "weak", "zero","answer" = "strong")[sample.int(4)])`

Strength of `X5`–`X6`:  
`r mcq(c("answer" = "moderate", "weak", "zero","strong")[sample.int(4)])`

:::

**Example of a cyclical (non-linear) pair?**

`r hide("Show answer")`
Any cyclical process, e.g., “time of day vs. tide height” or “sun elevation vs. day length”.
`r unhide()`

### 5.1.2 Correlation coefficients

Even when the pattern is non-linear it’s useful to see why Pearson *r* can mislead.

*Analyze* › *Correlate* › *Bivariate*  
Select all six variables → **OK**.

::: {.webex-check .webex-box}

`X1`–`X2` correlation: `r fitb(.50, num = TRUE, tol = .01)`

`X2`–`X6` correlation: `r fitb(.06, num = TRUE, tol = .01)`

`X3`–`X4` correlation: `r fitb(-.80, num = TRUE, tol = .01)`

Can we interpret `X3`–`X4`’s *r* at face value?  
`r mcq(c("answer" = "No, assumption of linearity violated",
         "No, assumption of association violated",
         "No, assumption of normality violated",
         "Yes, otherwise SPSS would give an error")[sample.int(4)])`

Interpret `X5`–`X6`:  
`r mcq(c("answer" = "Moderate negative",
         "Moderate positive",
         "Weak positive",
         "Weak negative")[sample.int(4)])`

:::

*Take-away:* linear *r* works well for straight clouds (X1–X2) but can mask strong curved links (X3–X4).

---

## 5.2 Correlation – Work Dataset (`Work.sav`)

Having practised on simulated data, let’s repeat the workflow on a real workplace dataset.

> **File location:** `data/Work.sav`

### 5.2.1 Why inspect the plot first?

Before trusting Pearson *r* we check for

* an **approximately linear** pattern, and  
* **extreme values** that could distort the statistic.

::: {.webex-check .webex-box}

Select the **two** key reasons:

`r mcq(c("To check if the relationship is linear"  = TRUE,
         "To check if there are any extreme values" = TRUE,
         "To check if the relationship is positive" = FALSE,
         "To check if the variables are normally distributed" = FALSE,
         "To check if the relationship is strong enough" = FALSE))`

:::

### 5.2.2 Create the scatter-plot

`Graphs` › `Legacy Dialogs` › `Scatter/Dot` → **Simple Scatter**

* X-axis =`scmental` (Mental Pressure)  
* Y-axis =`scemoti`  (Emotional Pressure)

*Paste* and *Run*.

::: {.webex-check .webex-box}

Is the cloud roughly **linear**? `r torf(TRUE)`

Any obvious **outliers**? `r torf(FALSE)`

Visual strength:  
`r mcq(c("Moderately strong and positive" = TRUE,
         "Moderately weak and positive"  = FALSE,
         "Moderately strong and negative" = FALSE,
         "Moderately weak and negative"  = FALSE)[sample.int(4)])`

:::

### 5.2.3 Compute Pearson *r*

`Analyze` › `Correlate` › `Bivariate` → (`scmental`, `scemoti`) → **OK**

::: {.webex-check .webex-box}

Correlation (3 decimals): `r fitb(.54, num = TRUE, tol = .01)`

Significance (α = .05)

*Relationship is:*  
`r mcq(c("significant" = TRUE, "not significant" = FALSE))`

*p-value:*  
`r mcq(c("< .05" = TRUE, "≥ .05" = FALSE))`

:::

### 5.2.4 Interpretation

::: {.webex-check .webex-box}

Population inference:

`r mcq(c("We can conclude that there is a relationship between mental and emotional pressure in the population." = TRUE,
         "We can conclude that there is no relationship between mental and emotional pressure in the population." = FALSE,
         "We cannot draw a conclusion on whether or not there is a relationship." = FALSE)[sample.int(3)])`

:::

*Take-away:* Mental and emotional pressure show a **moderately strong, significant positive relationship**—employees who feel more mentally pressured also tend to feel more emotionally pressured.

---
